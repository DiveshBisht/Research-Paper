ISSUE_ID,COMMENT,
LUCENE-7616,"As an additional nasty point, sometimes this method is called from places like BooleanQueryNodeBuilder, where it goes into an error message to show the user. So this error message also shows the wrong syntax, but it also isn't immediately clear how a QueryNodeBuilder would know what syntax was used to create the QueryNode it has been passed.",
LUCENE-7629,"I suspect it might be a JVM issue with nothing that we can do to fix it, but couldn't find any other JIRAs reporting the same thing so it made sense to create one.",
LUCENE-7630,"Hi, could you create a Pull Request and add the link here? About your branch: I would not use cloneAttributes() because thats slow for this simple case. cloneAttributes() only helps if you want to modify the attributes in the AttributeSource that was created, but is not useful for simple save/restore use cases. For your case, you should simple use captureState(), save the state object and then call restorestate() instead of clearAttributes(). After restoring you can adapt term text and positions/offsets. In addition when you clone or capture state, the call to clearAttributes() is useless and also slows down. When restoring states, everything is restored, so the additional clearing before is not needed.",
LUCENE-7630,I commited the suggested improvements and made a pull request https://github.com/apache/lucene-solr/pull/138. The NGramTokenFilter probably has the same issue. I can port the fix to that class when everything is correct.,
LUCENE-7630,The NGramTokenFilter probably has the same issue. I can port the fix to that class when everything is correct.Please do! You can update the current PR. Otheriwise PR looks fine.,
LUCENE-7630,done,
LUCENE-7630,"Thanks, I will merge and commit this after some testing!",
LUCENE-7630,Thanks Nathan!,
LUCENE-7631,Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future. We can deal with fixing existing rawtypes or a few other classes of warning in the future. David Smiley - you seemed interested in this conversation last time it came up.,
LUCENE-7631,Thanks for filing this issue!  Are all the changes in this patch necessary to get the build to pass?  So to clarify... no code (outside what the patch touches) needs adjustments?,
LUCENE-7631,"Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer. Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task. I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.",
LUCENE-7631,"Thanks Mike! I am for using this patch. Robert's suggestion was to enable ""all"" warnings, but IMHO this is a bad idea, because if somebody compiles with a later Java version, the build may suddenly fail (because a later version of the compiler added a new warning type). I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed! The only downside of this patch is that we no longer get any warnings displayed that are currently disabled (rawtypes, unchecked). So we should fix them asap (in a separate issue). BTW: Maybe we can enable rawtypes and unchecked errors earlier in Lucene and leave them disabled in Solr. As far as I remember we already have a separate warning setting for Solr. This may be the reason why Solr does not show any problems!?",
LUCENE-7631,Yes on Solr your change is not enabled: https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30 We should also review Solr (maybe in a separate issue).,
LUCENE-7631,"I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed! Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add. I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?",
LUCENE-7631,"Hi, IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed. What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).",
LUCENE-7635,Here is a patch.,
LUCENE-7635,"i'm not very familiar with Kuromoji but i believe the lines you 're deleting in this patch are intended to catch comments at the end of a line â€“ not just the begining, ie…",
LUCENE-7635,"It was done long time ago. I don't remember original intention... But it doesn't seem to be common for dictionaries to have comments at the end of line (synonym, stop). Wonder if it's better to follow the same rule.",
LUCENE-7646,"Note that you can use {noformat} around code blocks here to preserve indendentation. Can you post the NullPointerException? Was this already fixed by LUCENE-7576?  Though your ""silly"" regexp (..) should have been a NORMAL case I think? I turned your code fragment into a test case (will attach patch) and it doesn't hit an NPE ....",
LUCENE-7646,"Patch w/ test case, but it does not show the NPE...",
LUCENE-7646,"Mike, thanks for the comments. I didn't notice that when I posted the code like that, it changed the regex from",
LUCENE-7646,I've just tried with 6.4 and it does indeed seem to have been fixed by LUCENE-7576. I'd not realised I was using the API incorrectly. Thanks!,
LUCENE-7646,"Phew, thanks for bringing closure Tom Mortimer!",
LUCENE-7647,"Here is a patch. On the writing side, things are easy since there is a single instance that is used from a single thread and for a short amount of time, so I just made the compressor implement Closeable. However things are a bit more complicated on the reading side because of clones and the fact that we do not close them all. So to keep things simple, I just changed the codec to create Inflater instances on demand.",
LUCENE-7647,What does it do to performance to create a deflater instance every time? This seems very inefficient.,
LUCENE-7647,s/deflater/inflater of course,
LUCENE-7647,"For fetching the top hits I think it is fine anyway, if there is an issue I suspect it would be more with merging. I can try to run luceneutil with this change next week. Do you have ideas to make it more efficient maybe?",
LUCENE-7647,"I think its first important to understand how it impacts performance, including worst cases. That means merging with deletes and lots of results and stuff too: not just best-cases like top hits only. Alternative solutions are possible depending on the impact: e.g. pool managed by the top Decompressor and passed via clone(), and decompress could simply release back to the pool. This is kind of a standard pattern, but of course it adds complexity. We should avoid it if its really not necessary.",
LUCENE-7647,"I ran a merge that cantains 1M documents from the wikipedia benchmark including deleted docs, in order to test the worst case. Here is what the info stream reports about stored fields before/after the change. I think this is either noise a an acceptable slow down. That makes sense since we always decompress about 16K of data. Initialization of the Inflater is likely much less costly than decompressing that amount of data.",
LUCENE-7647,Thanks for running the benchmark!,
LUCENE-7650,Java file for reproducing the error.,
LUCENE-7650,"since Complexphraseqp parses query twice, you need escape it twice. That is.",
LUCENE-7650,"Thanks! That helped. Not an issue anymore for me. I think it would make sense to add to ComplexPhraseQueryParser's javadoc the following comment (or something alike). Special care has to be given when escaping: because some parts of the query might be parsed multiple times, these parts have to be escaped as often.",
LUCENE-7651,"I set to ""critical"", because this also breaks our latest release 6.4, which can't be build from source with Java 8 update 121",
LUCENE-7651,I will try to figure out if moving the Javascript to a separate file helps here. In HTML 5 inline Javascript in HTML files is a no-go.,
LUCENE-7651,"I checked it out, it disallows any <script>, also externally linked ones in the -bottom parameter of Javadocs. This is amajor pain. As adding this special allow-javascript parameter is brekaing builds with older versions and its hard to detect the exact build version in Ant, we have 2 options. 	Remove pretti. 	Inject the prettify code somehow else into the docs (e.g. by copying all script files together and append them to the javadoc-generated JS file after javadoc succeeds).",
LUCENE-7651,"This implements first option. I checked our sources. We allready append the CSS of prettify to Javadocs's main CSS files. As Java 8 contains also a Javadocs-generated scripts.js file, we could do the same here. I am working on that.",
LUCENE-7651,This would be my proposal.,
LUCENE-7651,Sorry last patch had a missing pair of brackets. This fixes it any also works locally. Stupid Javascript! KRRRR!,
LUCENE-7651,"Attached is a patch that also updates Prettify to latest version. The used one produced Javascript errors in Chrome, so I updated. I also removed the useless additional language plugin files.",
LUCENE-7651,"Final updates, I think this is committable. Any suggestions?",
LUCENE-7651,Add correct license header to CSS file.,
LUCENE-7651,"I just checked. The ""hack"" also works with IBM J9, so I see no problem doing this. I was afraid that maybe IBM does not have script.js in the Javadoc output. Nevertheless, this is not risky like the previous hack. If the Javadocs generator no longer produces script.js or stylesheet.css the Code Prettify would just no longer work, but not break Javadocs or build.",
LUCENE-7651,I sent the following message to OpenJDK: http://mail.openjdk.java.net/pipermail/javadoc-dev/2017-January/000281.html,
LUCENE-7651,"I committed this to master and branch_6x. I will now disable all 6.4 branch builds. If we will have a further bugfix release on this branch, we should backport this. Please reopen in that case.",
LUCENE-7651,"Fixed for now, please reopen for every bugfix release on any branch that was not yet updated to use this.",
LUCENE-7651,"BTW, the release notes of Java 8u121 was updated to mention this change: http://www.oracle.com/technetwork/java/javase/8u121-relnotes-3315208.html. It still breaks our previous releases, but we can tell people that its caused by Oracle, not us.",
LUCENE-7651,"I backported this change to Lucene 5.5.4, because otherwise we cannot run the build with Java 8 anymore. The problem is the following: Java 7's Javadocs do not add a ""script.js"" file to the Javadocs output, so this fix injects the javadocs, but because a ""script.js"" is nowhere referenced in the HTML files, prettyprint is not loaded. We have the following possibilities: 	revert this backport commit, but it prevents smoketester from suceeding (as it checks also Java 8). It also makes it impossible to build the 5.5.4 release with Java 8 	add more hacks to inject a script tag into the head element, but that's really complicated as you have to do it in every HTML file! 	ignore the fact that Javadocs do not code-prettyprint correctly anymore in Java 7. The Javadocs are fine, just the code exaples are no longer syntax highlighted. I'd go for the third item. Any comments? If we go this route, I will add a comment to the Changelog that prettyprinting Javadocs is no longer working, if docs are build with Java 7.",
LUCENE-7651,"We don't have access to recent paid only Java 7 JDKs, but as the Javadocs fix was declared a security issue, I assume that it is also applied to Java 7, so without this fix Java 7 paid updates will fail the build, too.",
LUCENE-7652,"Can you also read what the value of LRUQueryCache.ramBytesUsed is? LRUQueryCache has references to current open IndexReaders due to close listeners. I believe you are analysing a heap dump and recursively adding everything that is referenced by your LRUQueryCache while it does not make sense ot take open IndexReaders into account. The keys of LRUQueryCache.cache are instances of SegmentCoreReaders, and I've checked many of the keys, the only reference to them is LRUQueryCache.cache, given LRUQueryCache.cache is an IdentityHashMap, that means you can't even get to them outside of the cache because you can't get a key that's equivalent to one of these in the cache. This indicates that you are leaking index readers. When there are no readers that reference a segment anymore, that segment is closed, which triggers the eviction of all associated entries in the query cache. You should review your usage of Lucene to make sure that there is a call to close() for every index reader that you acquire.",
LUCENE-7652,"Hi Adrien, ramBytesUsed is only ~500KB. Thanks for the info regarding leaking index readers, I'll try to track those down.",
LUCENE-7652,Another possibility is that you are affected by this bug: https://issues.apache.org/jira/browse/LUCENE-7657.,
LUCENE-7652,Closing now as this is either an IndexReader leak from the application side or a duplicate of LUCENE-7657.,
LUCENE-7652,"I have found our application was indeed leaking, we basically have something like. reader was closed after used but dir was never closed, therefore causing this leak. I have not yet verified whether we are impacted by LUCENE-7657.",
LUCENE-7657,"Here is a patch that makes sure the queries returned by Weight.getQuery() do not reference a TermContext. An alternative way to fix the issue would be to remove the reference from TermContext on the reader context, but it is not straightforward. Opinions welcome.",
LUCENE-7657,"I was doing more testing and this patch does not work. We only unset the context in createWeight, so wrapper queries like BooleanQuery or ToParentBlockJoinQuery would still have a reference to the index reader if the wrap a TermQuery that has a TermContext. So I guess the only way would be to remove the reference from TermContext to the index readers.",
LUCENE-7657,Here is a patch that removes the reader context reference from TermContext.,
LUCENE-7657,Clever solution to factor out this Object identity; I wonder if we should fix IndexReader.getXXXCacheKey() similarly ... later!,
LUCENE-7662,Michael McCandless - what do you think of this patch?,
LUCENE-7662,"Thanks Mike Drob; I think this patch looks good, except it makes some tests angry. Maybe we just need to relax that base test case to accept the new CorruptIndexExcpeption as well, and look to its cause to check the exception message? Also, I think it'd be a bit better to use our expectThrows method in the test case, wrapped around the one line where you try to open an index reader, instead of the @Test(expected = ...), which would pass if CorruptIndexException was hit anywhere in that test case?",
LUCENE-7662,"Those are good suggestions, I'll get them into the next version of this patch.  Looking at the code in MockDirectoryWrapper, some of the ""a random IOException"" stuff looks really hackish, especially where we are checking for string messages to match. I'm uncomfortable with how brittle some of that is. We already have FakeIOException available and I think it would be good to use that instead in several places. Do you think we should handle that here, or I can file a new issue for it.",
LUCENE-7662,Updated patch with some test clean up.,
LUCENE-7662,"Thanks Mike Drob, the new patch looks great, and +1 to do that test cleanup here.  I'll push soon!",
LUCENE-7662,Hmm something is still angry.,
LUCENE-7662,"Thanks. That is frustrating. I ran it 10 times and somehow never hit that or a similar seed. When the test uses the compound format, since there is no .doc file to remove, the index doesn't get corrupted and correctly never throws the exception. I couldn't figure out how to disable compound format from the test, so instead we can attempt to delete the doc or the .cfe file. I also made a change to check that we do delete something, otherwise the index would never be corrupt here. Since I can't imagine all possible future index file layouts, this seems prudent.",
LUCENE-7662,Thanks Mike Drob! That is frustrating. I ran it 10 times and somehow never hit that or a similar seed. The joys of randomized tests!  I'll review and push soon...,
LUCENE-7662,Thank you Mike Drob!,
LUCENE-7668,Simple patch; I had to also fix CannedTokenStream to set the canned token type for the test case.,
LUCENE-7668,"Why don't you capture all attributes with captureState and restore them instead of clearAttributes? After that you can change token text and offsets/position. There is a TODO about this in the source code. The problem mentioned here also affects payloads or other attributes like flags, keyword, or the japanese ones. I fixed the same issue in NGramFilters a week ago.",
LUCENE-7668,"Ahh in fact it's already doing the captureState/restoreState here, and so in fact there is no bug!  I wrote the test first, and it was only failing because CannedTokenStream was failing to carry over the type I had asked for. Here's a new patch, just adding the test case, and removing dead code from WDGF. Thanks Uwe Schindler.",
LUCENE-7668,"Hah, thanks for bringing to closure!  Thanks for removing the dead TypeAttribute! Maybe we should fix CannedTokenStream to do what its comment says (captureState/restoreState).",
LUCENE-7668,"Maybe we should fix CannedTokenStream to do what its comment says (captureState/restoreState). I would love to, but I'm not quite sure how to do it.  CannedTokenStream gets an array of Token in ... how can I do a restoreState from a Token?",
LUCENE-7668,"There is one possibility: As CannedTokenStream is a source and owns the attributes, it could use the Token Attributefactory (see the final field on deprecated Token class) in its ctor. Because of this one could use copyTo of Token to copy it into the Token behind all attributes. But as CannedTokenStream only supports the attributes of Token, we could also just ensure all of those are copied. So look which attributes are implemented and use those! Not sure what's the better idea.",
LUCENE-7668,"OK I tried your first idea Uwe Schindler, using Token's attribute factory for CannedTokenStream and then casting the offset attribute to Token and using copyTo.  It seems to work, as crazy as it looks!",
LUCENE-7668,"Looks good! Thanks for adding the comment about the crazyness - I see no better way to do this (without casting)! It is also good that you left the clearAttributes(), because without it, the ""extra"" attributes maybe added by TokenFilters, would not be cleared and then the Asserting* tests would fail.",
LUCENE-7668,Thank you Uwe Schindler!,
LUCENE-7670,"Patch that uses the SearcherManager(Directory,...) ctor instead of the SM(IndexWriter,...) ctor in the case of opening a suggester over an already built index. Committing shortly.",
LUCENE-7674,This particular error means that there is a problem in the way your index is structured since you had at least one segment that did not have a parent doc as a last document. This is wrong because block joins work on blocks of documents that contain 0-n children followed by one parent so the last document is necessarily a parent document.,
LUCENE-7674,Thanks Adrien Grand!  I'm trying to figure out if this an issue on my side (very possible) or if it's a Solr or Lucene issue. All my indexing goes through Solr (via SolrJ) and as far as I can tell I'm not attempting to index any child documents without a corresponding parent document.  I'm not even sure if Solr or SolrJ would allow me to do that. Does it make sense that optimizing the index would cause the problem to go away? I think I was able to snag a copy of the index that was causing problems before the optimized version was able to replicate.  Any suggestions/pointers for trying to track down whatever docs are problematic?  Will running CheckIndex on it tell me anything useful?,
LUCENE-7674,"Tim Underwood, it usually happens when uniqueKey is duplicated, it causes deleting former parent doc. It can be verified with org.apache.lucene.search.join.CheckJoinIndex, although it doesn't have main() method. Adrien Grand, what if will invoke CheckJoinIndex logic lazily somewhere in? It won't cost much as well it should be lazy, but provides more predictable behaviour for users.",
LUCENE-7674,"Thanks Mikhail Khludnev!  Running CheckJoinIndex on my bad index (assuming I got my parentsFilter right) says java.lang.IllegalStateException: Parent doc 3324040 of segment _vfo(6.3.0):C28035360/10475131:delGen=86 is deleted but has a live child document 3323449 Running CheckJoinIndex on the optimized version of the index doesn't complain. So... that leaves me wondering where the bug is.  I am frequently (via Solr) re-indexing parent/child documents that duplicate existing documents based on my unique key field but my understanding is that Solr should automatically delete the old parent and child documents for me.  Maybe thats a bad assumption. It looks like maybe I'm running into one or more of these issues: SOLR-5211, SOLR-5772, SOLR-6096, SOLR-6596, SOLR-6700 Sounds like I should probably just make sure I explicitly delete any old parent/child documents that I'm replacing to be on the safe side.",
LUCENE-7674,I also noticed that I have some deleteByQuery calls that target parents documents but not their children (my assumption being that Solr or Lucene would also delete the corresponding child documents).  Perhaps that is what is causing the orphan child documents.  I'll be sure to explicitly delete those also.,
LUCENE-7674,"LUCENE-7674.patch introduces CheckingQueryBitSetProducer which checks parent segment's bitset before caching and switches {!parent} {!child} to use it. It laid well, beside of, and it's interesting! BJQParserTest.testGrandChildren(). When we have three levels: parent, child, grand-child and searching for children (2nd level), it requires to include all ascendant levels (parent) in bitset. This, will break existing queries for those who run more than two level blocks. But such explicitly strict behavior solves problems for those who tires to retrieve intermediate levels by [child] then, I remember a couple of such threads in the list. What do you think?",
LUCENE-7674,"Tim Underwood, you've got everything right! Thanks for gathering those pet peeves in the list. Here is one more, SOLR-7606 - it's my favorite ones. I need to tackle them sooner or later.",
LUCENE-7674,"Adrien Grand , Uwe Schindler, what's your opinion about CheckingQueryBitsetProducer and restricting multilevel blocks?",
LUCENE-7674,"It feels wrong to me that we enforce these rules at search time, while they should be enforced at index time. I think the true fix to all these block join issues would be to make Solr know queries that describe the parent and child spaces rather than expect users to provide them at search time. Then once it knows that, it could reject update/delete operations that would break the block structure, fail queries that use a parent query that is not one of the expected ones, maybe add a FILTER clause to the child query to restrict it to the child space in case some fields are used at multiple levels, etc.",
LUCENE-7674,"I agree with Adrien. The current block join support in Solr is a desaster, because it was released to early. Just nuke the broken APIs and create a new one, so Solr internally knows from schema/mapping how to block join and also prevent misformed updates. This is also worth a backwards compatibility break! Doing expensive runtime checks on every query just to keep a broken API/implementation is not a good idea. Break hard and come with a better API, the users will still be more happy, trust me. I know so many users who f*ck up the block joins, as Solr does not enforce it correctly. Do the following: 	remove Solr ID fields from child documents (why do we have them? This also makes updates to child documents impossible) 	always hide child documents on ""normal"" queries and return them only with the parent document (like Elasticsearch does) 	automatically add block join queries if fields of the child documents are part of the query 	add some extra queries to specifically search on childs and return childs only (hiding parents, of course) 	if somebody updates a parent document, delete also all childs and create a new block 	hide the block join filter. Solr should have an internal marker field to support block join, which is never exposed",
LUCENE-7674,"Oh.. I've got your point, guys. Thanks. I'd probably raise gsoc ticket and try to scratch backlog.",
LUCENE-7674,+1 to Adrien Uwe's remarks.  It was released too early.,
LUCENE-7674,"Ok. I started to scratch the spec at SOLR-10144. Everybody are welcome. Meanwhile, I tried to reproduce this exact failure to come up with more informative message. But it seems like it's impossible - recently redesigned BlockJoinQuery ignores children behind the last parent in segment.",
LUCENE-7676,looks good. i think the @Test can be dropped from the test case.,
LUCENE-7676,+1 too,
LUCENE-7676,Thank you both for the reviews.,
LUCENE-7679,"Here is a patch that re-organises how MemoryIndex builds its internal field Info structures.  If an IndexableField is passed to addField(), we re-use as many of its FieldType settings as possible. I needed to make FieldInfo.setDocValuesType() public, but that brings it into line with .setPointDimensions() so I don't think it's too bad a change?",
LUCENE-7679,Thanks Martijn,
LUCENE-7682,"I think I know why some of this is going on - in NearSpansOrdered stretchToOrder handles figuring out the effective position length it needs to search over and advances each spans to the relevant distance for a match. The second span is advanced just enough so the first instance of 'feed' matches (which satisfies the query), and matchEnd is set to that ""feed"" occurrence's end position (and matchWidth updated as well), and it stops after that, so NearSpansOrdered effectively does not see that last occurrence of feed when twoPhaseCurrentDocMatches() is called (from getTermToSpans in PhraseHelper).  This first end position of the first ""feed"" occurrence is what's used instead of the last end position within the slop.",
LUCENE-7682,Are you saying then that there seems to be a bug in NearSpansOrdered (and not any highlighter)?  Presumably the original Highlighter WSTE would be affected as well?  Can we test this?  If we can get to the bottom of this ASAP then we have a chance of getting a fix into v6.4.2.,
LUCENE-7682,"For queries requiring t1 near t2 with enough slop, t1 t1 t2 matches twice, but t1 t2 t2 matches only once. This behaviour was introduced with the lazy iteration, see: https://issues.apache.org/jira/browse/LUCENE-6537?focusedCommentId=14579537&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14579537. This is also a problem for LUCENE-7580 where matching term occurrences are scored: there the second occurrence of t2 will not influence the score because it is never reported as a match. LUCENE-7398 is probably also of interest here. To improve highlighting and scoring, we will probably have to rethink how matches of span queries are reported. One way could be to report all occurrences in the matching window, and forward all the sub-spans to after the matching window. Would that be feasible?",
LUCENE-7682,Should this be marked as a bug for a module other than the highlighter then since it also affects scoring?,
LUCENE-7682,"Michael, don't worry about how you filed it; this happens often â€“ report the bug and then dig deeper and see it's indirectly caused by something else.  Here, it's not clear is LUCENE-7398 or LUCENE-7580 will fix it; I've been following these issues; we'll see.   Paul Elschot I'm glad you noticed this issue; this is in your area of interest",
LUCENE-7683,I think it would make sense to make Scorer.getWeight final and FilterScorer delegate getChildren.,
LUCENE-7685,Here is a patch.,
LUCENE-7685,"What on earth did this comment mean? This makes rewritten query equal the original, so that user does not have to .rewrite() their query before searching. Why would a user have to (in the past) manually rewrite their query before searching .",
LUCENE-7687,This reads to me as the underlying issue of whether ComplexPhrase query parser uses underlying (automatically-constructed) MultiTerm analysis chain (and takes into account multiterm-ready ascii folding filter) or does its own expansion using pure tokens.,
LUCENE-7690,I'll dig; looks related to LUCENE-7662.,
LUCENE-7690,Thanks Steve Rowe!,
LUCENE-7695,"This would have been better discussed on the mailing list first, I feel. I suspect what might be happening here is that one of the terms is hitting synonym expansion and perhaps that is not supported. This is strengthened by the fact that the words in the exception do not match the word you gave triggering it. So, I would check the type definition, synonym file it uses and the synonyms in there. If I am right, the bigger question then is whether ComplexPhraseQueryParser is expected to support synonyms. If yes, then that would be the actual issue here.",
LUCENE-7695,"CPQP transforms only certain queries to spans. So, the failure is obvious and patches are welcome.",
LUCENE-7695,"Hello Alexandre Rafalovitch, The terms i used in the examples do not have synonyms defined, actually, the synonym file is so far still empty. About the words not matching, you are right, i copy/pasted another exception, i was looking for words and word combinations that do and do not cause trouble. Apologies for the confusion. Thanks, Markus",
LUCENE-7695,"I cannot seem to import stuff from Lucene's analysis module into a unit test that's in Lucene's queryparser module, doesn't work in org.apache.lucene.queryparser.complexPhrase.TestComplexPhraseQuery. Any ideas on how to test it?",
LUCENE-7695,you can try to approach org.apache.lucene.analysis.MockSynonymAnalyzer in TestComplexPhraseQuery,
LUCENE-7695,"Patch for master. I cannot get the unit tests to run (ant keeps hanging here) so i applied a crude fix and tested it via Solr and it works. When processing the SynonymQuery i actually have no idea what it should do with more than 1 term, i think it should rewrite itself again but i am really not sure. Or should it create SpanTermQuery for each term and wrap those in a SpanOrQuery and add that to the list of allSpanClauses?",
LUCENE-7695,"Here's a patch where each Term in the SynonymQuery is wrapped as SpanTermQuery in a SpanOrQuery, which is then added to the allSpanClauses array. If there is just one term in the SynonymQuery, it is added as a SpanTermQuery directly. This seems more appropriate, but don't take my word for it.",
LUCENE-7695,"It seems the top level query can also be a SynonymQuery, at least via Solr. Updated patch to take care of that as well but it seem i broke something as well. It is now no longer possible to embed FuzzyQuery. Won't work anymore. But working with multiple terms on the same position does work now, e.g. KeepWordFilter with stemmed terms. I need to go, but will take a peek later.",
LUCENE-7695,"New patch, all SynonymQuery's are turned into a SpanOrQuery now and it works, as it seems. Haven't got a clue yet why this doesn't work, but have it wrapped in a boolean query does.",
LUCENE-7695,what about LUCENE-7695.patch ?,
LUCENE-7695,"Hello Mikhail Khludnev, your patch works nicely!",
LUCENE-7695,Removed fix/version 6.4.2. Thanks Mikhail!,
LUCENE-7698,This seems to be a regression in Solr 6.4.0. At least a quick test shows correct results in 6.3.0.,
LUCENE-7698,Looks to me like LUCENE-7603 broke this.,
LUCENE-7698,"Hmm, no good, sorry about this ... thank you for reporting this Ere Maijala; I'll try to make a Lucene test case showing this.",
LUCENE-7698,"OK I see what's happening: this filter (CommonGramsQueryFilter) deletes the unigram tokens, but keeps posLength=2 on the bigram tokens, which makes a disconnected graph, and then the query parser does the wrong thing. I think the right fix is for it to set posLength to 1 when it drops unigram tokens .. I'll work on a patch.",
LUCENE-7698,"OK here's a patch fixing CommonGraphsQueryFilter to not create a disconnected graph.  Ere Maijala could you please try this and see if it fixes your use case?  Thanks. I also added an experimental option to QueryBuilder (base class for query parsers) to disable graph handling, as a safety for other tokenizer components that may create disconnected graphs.",
LUCENE-7698,"Michael McCandless, thanks for the fix. An initial check indicates that the patch fixes my use case. I ran the tests in branch_6x. The patch didn't quite apply cleanly to branch_6_4 and after applying manually a test didn't compile.",
LUCENE-7698,OK thanks for confirming Ere Maijala; I'll fix that test on back port.,
LUCENE-7698,Thank you Ere Maijala!,
LUCENE-7704,"Hi Sebastian Yonekura Baeza, actually, this is by design: it is up to you to downcase the rules you add to the SynonymMap.Builder, and then that ignoreCase option will ignore the case of the incoming tokens during analysis. I'm sorry the javadocs were missing (so you would not have known this is by design!!), so I've copied over the javadocs from the old SynonymFilter, and I've fixed your test case to down-case the rules, and now it's passing, in the attached patch.",
LUCENE-7704,"OK I improved the javadocs explaining ignoreCase, and folded in your test case (thank you Sebastian Yonekura Baeza!) here: https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;a=commitdiff;h=3ad6e41910158a46025ff78330d78a31a7081887",
LUCENE-7704,"Oh, sorry I missed those docs, given that it was a deprecated class I didn't pay much attention to it. Indeed, without the javadocs the parameter ignoreCase was kind of misleading. Thank you Michael McCandless for the clarification!",
LUCENE-7708,"The CJKBigramFilter is working correctly because it sets the position length attribute only if outputUnigrams is set. So only the ShingleFilter is problematic since outputUnigrams is not check when position length is set. So for instance with shingles of size 2, the input ""foo bar baz"" would create two tokens  ""foo bar"" and ""bar baz"" with a pos len of 2 and an position increment in between which forms a disconnected graph. I'll work on a patch shortly.",
LUCENE-7708,"Here is one patch for the ShingleFilter. When outputUnigrams is set to false, position length for a shingle of size N is the number of position created by shingles of smaller size: (N - minShingleSize) + 1. Michael McCandless can you take a look ?",
LUCENE-7708,"agreed to the idea, but some tests are failing with the patch.",
LUCENE-7708,Thanks Steve ! I pushed a new patch that solves the tests failures.,
LUCENE-7708,"I'm beasting 1000 iterations of TestRandomChains with the patch, and run 110 found the following reproducing seed - maybe it's ShingleFilter's fault?  (I didn't investigate further). this seed fails on unpatched master, so the patch on this issue isn't to blame.  I created a separate issue: LUCENE-7711",
LUCENE-7708,"Agreed, thanks Jim Ferenczi!",
LUCENE-7708,"Agreed, LGTM, all lucene/analysis/common/ tests pass for me with the latest patch. Also, 1000 beasting iterations of TestRandomChains didn't trigger any failures with this patch (other than the unrelated one at LUCENE-7711).",
LUCENE-7708,Thanks Steve Rowe and Michael McCandless !,
LUCENE-7708,"There's no fix version here.  CHANGES.txt says it's in 6.5.0. (looking for possible causes of a shingle filter problem confirmed in Solr 6.3 and 6.4, this couldn't be the cause)",
LUCENE-7708,Shawn Heisey one shingle filter problem is fixed in LUCENE-7708 and appears in 6.3 when the support for graph analysis has been added to the QueryBuilder. The other shingle filter problem I can think of is when the number of paths is gigantic and produces an OOM. I opened LUCENE-7747 to fix this. Although I think that the workaround for now is to be disable graph query analysis when the analyzer contains a shingle filter that produces shingles of different size. The graph analysis in this case builds all possible path since each position has different side paths.,
LUCENE-7708,"Jim Ferenczi what we do after committing/all-done is ""Resolve"" the issue (not ""Close"").  That dialog box will give you the option to set the fix-version.  Later on during the release process, there should be a JIRA step that involves bulk-closing all issues resolved for this version.",
LUCENE-7708,Thanks David Smiley. I updated the status.,
LUCENE-7708,"Looks like 6.5.0 isn't a valid version yet.  Easy enough to add, but if I do so, would I be doing the right thing?",
LUCENE-7708,"Looks like 6.5.0 isn't a valid version yet. Easy enough to add, but if I do so, would I be doing the right thing? I see Jim already set the version to 6.5, but FYI Shawn Heisey, historically people have excluded the trailing "".0"" in minor release labels here on JIRA.",
LUCENE-7711,"Another reproducing failure on branch_7_0, from my Jenkins",
LUCENE-7715,"I  haven't tried the patch, but I don't see how it deals with the initial state that all sub spans have a start position of -1. In master currently, nextStartPosition iterates over the pq until the start position of the top element is greater than -1, but I don't see that logic in your patch. There is a startDocument method but if I am not mistaken it is only called from two-phase iteration, so if NearSpansUnordered are consumed directly, that would work, but not if they are nested within other spans? (I'm not very familiar with spans so I could easily miss something.)",
LUCENE-7715,"how it deals with the initial state that all sub spans have a start position of -1. There is no need for that, the intermediate data structure is a priority queue that is not a Spans itself. If the names of this priority queue (SpanTotalLengthEndPositionWindow) and its methods (startDocument/nextPosition) are misleading, they need to be improved. The core search tests and precommit pass.",
LUCENE-7715,"OK, I just applied the patch to understand how it works. It looks good to me, I'll merge it soon. Thanks Paul!",
LUCENE-7715,Thanks Adrien.,
LUCENE-7717,"I think that in org.apache.lucene.search.uhighlight.MultiTermHighlighting.extractAutomata() condition ""if (query instanceof AutomatonQuery)"" should be the last in ""if"" chain",
LUCENE-7717,Hello Dmitry. - I am attaching potential test case adapted from your code snippet (no pun intended) in the description. The test passes locally for me though. Could you perhaps try running it locally too and adapt/adjust it and with/without the MultiTermHighlighting change you mention? Thanks. - Christine,
LUCENE-7717,"At some point after MultiTermHighlighting.java was first written, PrefixQuery was altered to be a subclass of AutomatonQuery.  So PrefixQuery detection could simply be removed now, I think, since it's handled via AutomatonQuery condition. I'm working on debugging to see why this fails & a proper test.  (the test would go in TestUnifiedHighlighterMTQ by the way)",
LUCENE-7717,"Here's my take on it:  The UnifiedHighlighter (and PostingsHighlighter from which it derives) processes the MultiTermQueries (e.g. wildcards) in the query and creates multiple CharacterRunAutomaton intended to match the same things.  CharacterRunAutomaton takes a Automaton as input, and when it does it's processing, it matches the Character code points (integers from 0 to 0x10FFFF) against the integers in the Automaton.  However, this strategy assumes that the Automaton was constructed based on character code points.  But AutomatonQuery.getAutomaton is intended to match byte by byte (integers 0 to 255).  PrefixQuery.toAutomaton will get 2 bytes for the the ""Ñ"" in BytesRef form, and add 2 states.  This does not line up with the assumptions of CharacterRunAutomaton. A short term immediate ""fix"" is simply to put AutomatonQuery last in the if-else list as Dmitry indicated.  As such, PrefixQuery will work again.  This was broken by LUCENE-6367 (Lucene 5.1).  TermRangeQuery, which also now extends AutomatonQuery, will likewise work â€“ broken by LUCENE-5879 (Lucene 5.2).  Again, back when MultiTermHighlighting was first written, neither of those queries extended AutomatonQuery.  But there will be bugs for other types of AutomatonQuery (namely WildcardQuery and RegexpQuery) that have yet to be reported. Robert Muir or Michael McCandless I wonder if you have any thoughts on how to fix this.  An idea I have is to not use a CharacterRunAutomaton in the UnifiedHighlighter; use a ByteRunAutomaton instead.  Then, add ByteRunAutomaton.run(char[] ...etc) that converts each character to the equivalent UTF8 bytes to match.  Even with that, I wonder if this points to areas to improve the automata API so that people don't bump into this trap in the future.  For example, maybe have the Automata self-report if it's byte oriented, Unicode codepoint oriented, or something custom.  Then, RunAutomaton could throw an exception if there is a mis-match.  However that would be a runtime error; maybe the Automata could be typed. Any way, what I'd like to do is do a short term fix that addresses many common cases and the title of this issue. And then do a more thorough fix in a follow-on issue.  Ishan Chattopadhyaya do you think this could go into 6.4.2 or are you only looking for ""critical"" issues?  It's debatable what's critical and not.  This bug has been around since 5.1 so perhaps it isn't.",
LUCENE-7717,"Here's a patch. It fixed the MultiTermHighlighting class in both the postingshighlight package as well as uhighlight.  It adds a test method to TestUnifiedHighlighterMTQ.  I also beefed up the test for a related method testWhichMTQMatched to avoid potential inadvertent changes to the CharRunAutomata toString that people might depend on.  It appears there was no breakage in this case but until I added more query types, wether it did or didn't break wasn't apparent.",
LUCENE-7717,IntelliJ IDEA has clued me into this else-if having dead code paths for a long while now here and I'm kicking myself for putting it off â€“ LOL.,
LUCENE-7717,"David Smiley, I don't mind including this fix, if you think this is a low risk fix and should be included. Feel free to backport this one to the release branch. I'm anyway waiting for SOLR-10215 as of now.",
LUCENE-7717,Closing.  I'll create a linked follow-up bug issue for WildcardQuery (also applies to Regexp) where we can discuss how to deal with that â€“ the more overall fix.  I don't think that one should hold up a 6.4.2.  It'll likely result in removing the PrefixQuery and TermRangeQuery sections in MultiTermHighlighting.,
LUCENE-7717,"Hello Christine I saw your test. I think that 	""I"" is not suitable data for test, because StandardAnalyzer converts data to lower case (but PrefixQuery doesn't) 	why ""random""? Ð¡ould be better make text array String[] texts =  {""i"", ""Ñ"", }  and run for each? Best regards",
LUCENE-7718,"Ishan, can you commit this change to master and branch_6x?",
LUCENE-7719,"Wow, this is a great catch Dmitry Malinin!  Thank you for opening the precursor issue. AutomatonQuery.getAutomaton really must return a UTF8-oriented automaton because that matches how the terms are indexed into Lucene, and what the automaton will be intersected with, to run the query. We should fix the javadocs to say this. And it is sort of annoying that these differences are not strongly typed, but the Automaton class is really agnostic to what ints you are putting onto its transitions. But, yeah, for highlighting, we are operating in UTF16 space, and so I think we need some way to have the CharacterRunAutomaton interface on top of a UTF8 automaton?  Maybe we should abstract out a separate interface that MultiTermHighlighting would use?  It seems it only uses the run method, to test if a given term is accepted?  And then, as you suggested, we could easily convert the incoming char[] to UTF8 BytesRef and use the byteRunAutomaton.run on that.",
LUCENE-7719,"Here is a patch for consideration. First of all, I realized that this bug is very minor because only some AutomatonQuery.getAutomaton are binary (I thought they all were); others are char based.  The ones that are binary in Lucene are PrefixQuery and TermRangeQuery, both of which are special cased in MultiTermHighlighting.  So practically speaking I think the only users to see this bug would be anyone building a custom AutomatonQuery.  Nonetheless this should be cleaned up and actually tested. The patch: 	Adds a fairly thorough test with lots of randomized unicode, testing PrefixQuery, TermRangeQuery, WildcardQuery, and FuzzyQuery 	removes the special casing of AutomatonQuery subclasses in MultiTermHighlighting.  AQ is handled generically now. 	added AutomatonQuery.isAutomatonBinary() with a new field to match. 	if the AQ.getAutomaton is not binary, we follow a simple/obvious pat 	if the automaton is binary, then I produce a CharRunAutomaton implementing run() to navigate the Automaton char by char.  It makes a BytesRunAutomaton on the automaton.  I inlined the one and two byte UTF char logic from UnicodeUtil.UTF16toUTF8.  If the char needs more bytes, then I call out to UTF16toUTF8 and work off the generated byte array for the remaining chars.  I think this is more maintainable, albeit slower, than reproducing the logic. I added a perf TODO in MultiTermHighlighting.java to have CompiledAutomaton expose the ByteRunAutomaton so that we don't need to rebuild it here.  The construction cost seems less than trivial as it determinizes the automaton and does other work.  Is this a big deal?  It seems kinda sorta; it depends.",
LUCENE-7719,"Michael McCandless What do you think of the patch?  In particular, I wonder what you think of: 	AutomatonQuery.isAutomatonBinary().  This is a very simple/innocent addition.  It's a shame Automaton.isBinary (or something similar) doesn't exist. 	See my TODO last paragraph above.  Also note even if that were done, the CompiledAutomaton isn't exposed by AutomatonQuery any way; so we'd need an accessor.  Perhaps alternatively AutomatonQuery might expose both a CharRunAutomaton and ByteRunAutomaton (i.e. move some of the code in this patch to there)?  If that wouldn't potentially be useful to other users then nevermind. 	The approach to convert chars to bytes at each step",
LUCENE-7719,Ping Michael McCandless since you've been involved with AutomatonQuery and automata in general. If you're too busy then I think the change to AutomatonQuery is innocent enough so I'm comfortable committing the patch as-is. I'm not sure if this will make 7.0 or not but I don't think it matters â€“ no back-compat issue / API issue.,
LUCENE-7724,Beasting with the repro line (after s/test/test-nocompile/ [1]) on the tip of branch_6x failed 3/150 iterations on my box -> 2% failure rate. [1] ant compile-test ; (for a in {1..500} ; do ant test-nocompile <remainder of repro line> ; done) 2>&1 | tee ~/output.txt,
LUCENE-7726,"which does in fact seem to be invalid HTML ... aren't & always suppose to be encoded as & ... even in URLs? That's the problem. Easy to fix. Just commit a fix or open issue about it. I noticed it already about a month ago, but had no time to fix it.",
LUCENE-7726,"I'm suprised the java8 javadocs/linter don't warn about this. We don't have a full HTML validator involved. In addition, for HTML5, the entity escaping can be left out, if it is unambiguous. This mimics the behaviour most browsers out there always had (because most web devs out there did this wrong). So the produced HTML is valid (HTML5) and also leads to no problems in HTML4 browsers. But we should still fix it. The requirement to escape also attributes is a requirement of just Java 9's Javac (which is a bug from the HTML5 perspective, but a good thing, too).",
LUCENE-7726,"There is also another TODO (separate issue, I'll open one): pegdown (the markdown processor) is also incompatible to Java 9; but it reached its end of life. The developer points to a new replacement lib - I just have to rewrite the Java code a bit. This is on my ""mental TODO list...."" since build 154 of Java 9.",
LUCENE-7726,"Patch fixing this issue. You can still not build full documentation because of failing ""pegdown"" processor, but that's a separate issue.",
LUCENE-7726,Thanks Hoss Man for reminding me!,
LUCENE-7726,"FYI, here is the HTML5 definition of ""ambiguous ampersand"": https://www.w3.org/TR/html5/syntax.html#syntax-ambiguous-ampersand. In fact, Java 9's Javadocs and Javac parser do not use that definition, which may be seen as a bug, but in fact this definition is just horrible and was added to make sloppy web developers happy...",
LUCENE-7728,Another reproducing branch_6x failure from Policeman Jenkins https://jenkins.thetaphi.de/job/Lucene-Solr-6.x-MacOSX/737/,
LUCENE-7728,Another reprodcing master failure: https://elasticsearch-ci.elastic.co/job/apache+lucene-solr+master/9262/console,
LUCENE-7728,"Another reproducing master failure from my Jenkins (not sure of commit sha, since job history has expired, but the email notification arrived on July 1st)",
LUCENE-7731,Thanks chillon.m!,
LUCENE-7749,This is a bug indeed! Would you like to submit a patch? We are soon going to release 6.5.1 so I think it would be nice to have that fix in.,
LUCENE-7749,I went ahead and worked on a patch to have it for 6.5.1 in time.,
LUCENE-7749,"Adrien Grand , thank you for working the on fix. I dont have lucene development set up here, but the patch may still need one more fix to work correctly (looking at 6.5.0 with your patch). Actually LRUQueryCache#CachingWrapperWeight not delegating the scorerSupplier was one of the problem, but there is another problem related to BulkScorer, if you look at the last part of the stack trace. BooleanWeight.java:385 always get the score with randomAccess=false, which again will ignore any IndexOrDocValuesQuery improvements. Weight#bulkScorer should probably use the scorerSupplier method instead of the scorer method internally.",
LUCENE-7749,"Hi Martin, thanks for looking at the patch. It is expected that BulkScorer always gets a scorer with randomAccess=false since bulk scorers always need to return all matching documents. I don't think this is an issue. The stack trace that you shared happened because the query cache noticed that a BooleanQuery that is part of your query tree (it might be your top-level query) is being reused, and so it decides to build a cache entry for it (LRUQueryCache$CachingWrapperWeight.cache). Building a cache entry requires to consume all documents that match the query, so it makes sense that the scorer is created with randomAccess=false. Note that the fact that the boolean scorer is created with randomAccess=false does not necessarily mean that all scorers of sub clauses of the BooleanQuery will be created with randomAccess=false too: if the query is a conjunction then only the clauses that have the lowest cost will be created with randomAccess=false. As a consequence, the IndexOrDocValues optimization still applies if the range query is not the least costly clause.",
LUCENE-7749,"I see. Thank you for checking ! One last suggestion: Weight#scorer method seems quite prone to confusion with the Weight#scorerSupplier, and could lead to additionnal problems like it happened with the LRUQueryCache. Wouldnt it be safer to expose only Weight#scorerSupplier, and make the Weight#scorer method protected ?",
LUCENE-7749,"I agree it is a bit error-prone and removing the scorer method or hiding it would fix the issue. The way I have been approaching this issue is that it is a fairly new and experimental API so I'd like to keep it optional for now (ie. query impls do not have to implement this method). Now that this feature is released, hopefully we'll get feedback about how well it works, which will in-turn give us opportunities to improve this API and once we are more confident about it, we can think about removing the trap that you observed. For the record, this is my personal opinion and might not be shared by other people who work on this project.",
LUCENE-7749,"I agree, refactoring and api improvement should not be part of patch versions. Current code is good enough for the moment, and that could be improved as part of a more global refactoring in the future.",
LUCENE-7755,Here is a patch.,
LUCENE-7755,+1 good catch!,
LUCENE-7757,"I suspect this is a duplicate of LUCENE-7719 based on the non-ASCII text I see in your example?  In 6.4.3, prefix queries were fixed â€“ LUCENE-7717 but wildcard queries (not a simple prefix) were pushed off until someone has time to fix.",
LUCENE-7757,"Yes, most documents (if not all) in our index will have non-ascii characters  (although I'm not sure I can locate them in the example above). I have tried to index a document without non-ascii characters, but highlighting for that document is still empty. Are you saying that the presence of non-ascii characters in the index will mess highlighting up for all documents? Thanks, Bjarke",
LUCENE-7757,"I got to the bottom of this one; it's tricky.  I see two issues: 1. The UH's PhraseHelper uses WeightedSpanTermExtractor to convert the query to a SpanQuery.  WSTE has no knowledge of  ComplexPhraseQuery so it has some fallback logic.  PhraseHelper overrides isQueryUnsupported but it has a lingering TODO with a return true, thus any any query not known in advance is not going to be highlighted.  I think this should be modified to return false.  I did that locally and I also found it to then be necessary to override getLeafContext() to return a dummy context.  The PH can't produce a real leaf context (here) because this is the stage at which it is merely analyzing the query, no possible wildcard expansion is done (yet).  The query worked in the original Highlighter because there is no split phase. 2. ComplexPhraseQueryParser produces a special Query subclass ComplexPhraseQuery.  CPQ implements rewrite() that also calls rewrite() on the clauses.  It expects a real (not a dummy) leaf context.  So this works from a query execution standpoint, but I think it would be more friendly with the UH if CPQ didn't cascade the rewrite.  It's not a simple matter of commenting out the cascaded rewrite though... I will investigate further when I have more time.",
LUCENE-7758,"This behaviour is irrational. Well, this is not exactly true. This is a token filter, meaning it can be applied on top on any set of other token filters. Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters), which also have a protected isTokenChar method that can be overriden in case you want to eg. split on whitespaces.",
LUCENE-7758,"This behaviour is irrational. Well, this is not exactly true. This is a token filter, meaning it can be applied on top on any set of other token filters. Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters), which also have a protected isTokenChar method that can be overriden in case you want to eg. split on whitespaces.",
LUCENE-7758,"Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. But why one feature should break another? I don't use synonyms or something like that, but I have no possibility to use token filter with properly offsets. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters). Such workaround applicable only to cases when input text can be simple splitted on specified characters. In my case I want to use icu_tokenizer before edge_ngram for properly split by words. For example, imagine japan language.",
LUCENE-7758,"I agree with what you are saying, it would like to have the ability to modify offsets in the cases that it makes sense too. I just wanted to react to your comment that the current behaviour is irrational. Moreover, I would not be surprised that highlighting the entire token is a desired behaviour for some users. I haven't thought about it much but it feels to me that we would need a way to annotate token streams in order to know whether the content of the CharTermAttribute matches the original text between the offsets stored in the OffsetAttribute if we want to change offsets safely.",
LUCENE-7758,"I just wanted to react to your comment that the current behaviour is irrational. Ok, in other words: unexpected and may be confusing from the point of view of the average user. I would not be surprised that highlighting the entire token is a desired behaviour for some users. I think for such cases just should be implemented possibility to behaviour tuning, for example parameter like inherit_offsets: true | false.",
LUCENE-7758,"Moreover, I would not be surprised that highlighting the entire token is a desired behaviour for some users. This is correct. Modifying offsets inside a TokenFilter is not going to be correct for highlighting for the reasons you are mentioning. This is a general issue with all token filters that are splitting tokens: The ""famous"" example is WordDelimiterFilter. Assigning offsets is the responsibility of tokenizers. Tokenfilters should just look at tokens and modify them, but not split them or change their offsets. In addition, highlighting is not meant to produce ""exact"" explanations of every analysis step. It is more meant to allow highlighting whole tokens afterwards, so the user has an idea, which token was responsible for a hit.",
LUCENE-7758,"Assigning offsets is the responsibility of tokenizers. Tokenfilters should just look at tokens and modify them, but not split them or change their offsets. But tokenizer can be only one, so there is no way to get tokens different than produced by specific single tokenizer. No way to customize without writing your own tokenizers. It is possible to combine token filters, but not tokenizers. In addition, highlighting is not meant to produce ""exact"" explanations of every analysis step. It is more meant to allow highlighting whole tokens afterwards, so the user has an idea, which token was responsible for a hit. I think this should be decided by Lucene users, not by anyone else. When you project your index and search behaviour, only you can decide how it should work based on your project requirements.",
LUCENE-7759,"Sigh, +1.",
LUCENE-7760,"Agreed. From http://mail-archives.apache.org/mod_mbox/lucene-java-user/201611.mbox/%3c36FCFD77-D873-4757-9D16-E89016F169E6@gmail.com%3e, where I most recently responded to a user question about the situation - this should be useful as a seed for javadoc fixes",
LUCENE-7760,OK thanks for the history here Steve Rowe ... I will fixup the javadocs based on this.,
LUCENE-7760,"Patch, improving the javadocs and adding a couple test cases ...",
LUCENE-7760,"Agreed. Tests look good.  I like your simplification of my explanation (""[long tokens] are chopped up at [maxTokenLength] and emitted as multiple tokens""); a more precise description about rule matching, including the possibility of emitted tokens not being e",
LUCENE-7760,"Oh, I forgot to mention: UAX29URLEmailTokenizer has the same issue, and would benefit from the same javadoc fix (and tests).",
LUCENE-7760,Thanks Steve Rowe; I'll do the same for UAX29URLEmailTokenizer.,
LUCENE-7760,"Another iteration, also fixing UAX29URLEmailTokenizer.  I also carried over the max token length check from StandardAnalyzer.",
MTOMCAT-103,does the file smapi-1.0-war-exec.jar contains the war file smapi-1.0.war ? As it looks to not be here: how do you configure the mojo in your pom ? ,
MTOMCAT-103,"I have unziped the smapi1.0-war-exec.jar file and in it I see the smapi-1.0.war file. I think it is finding the war, other wise I would be seeing the zip file exception that I saw when running it from the wrong directory. Here is how I have the plugin configured in the pom.",
MTOMCAT-103,should be fixed now. issue with path started with /. If you still have issues add a comment with stack trace. Thanks. ,
MTOMCAT-103,Integrated in TomcatMavenPlugin #56 (See https://builds.apache.org/job/TomcatMavenPlugin/56/). do not record jar entry starting with / remove it.,
MTOMCAT-103,That fixed it for me.  Thanks so much for your fast resolution.  This is an amazing plug-in keep up the good work. ,
MTOMCAT-104,some fixes has been committed do you still have the issue ?,
MTOMCAT-104,Nope this appears to be fixed. ,
MTOMCAT-105,"It looks the war generated by your project is not generated. Does your project produce a war file ? To trigger exec-war-only goal in the package phase, your execution block must be in build/plugins/plugin rather than in pluginManagement section. ",
MTOMCAT-105,"Aha, i see ! Yes, it's just like you said, now the the goal is triggered when doing the package phase. Thank you ! This should be another question, but still i got error when trying to run it with java. Strange that it's looking in the naming, since i got the file in here (i check the file's existence after extracting the executable war). And yup, it works fine in tomcat7:run. Out of topic question : Do you have a mailing list or a group or forum where i can ask questions ? I feel bad about making issues without first discussing it. =)",
MTOMCAT-105,Can I have a sample application  to reproduce that ? ,
MTOMCAT-105,"Here is the simple app attachment, whose executable war runs wrongly, with the error of java.io.FileNotFoundException: jndi:/localhost/BasicSetup/WEB-INF/faces-config.xml ",
MTOMCAT-105,can you try with enabling naming.,
MTOMCAT-105,"Thanks for the fast response ! I added that in the plugin configuration, still the same FNFE. And then i tried experimenting the 'useNaming', setting both of the config to true and false, but still the same FNFE. Just a wild guess from googling around if this has something to do with the classloader classpath where it couldnt see the content of the war that has the WEB-INF/faces-config.xml ? ",
MTOMCAT-105,any way you provide a sample project to reproduce that ? BTW some changes has been applied could you retry if you still have the issues ? ,
MTOMCAT-105,please provide more infos. ,
MTOMCAT-106,"please attach a patch file and check ""Grant license to ASF for inclusion in ASF works (as per the Apache License Â§5)"" Thanks! ",
MTOMCAT-106,"Just attached a patch. For a prettified preview, go to: https://github.com/apache/tomcat-maven-plugin/pull/1/files ",
MTOMCAT-106,to prevent too much complicated xml configuration. I would prefer : ExtraDependency extend Dependency ,
MTOMCAT-106,"Me too! But due to my (great) lack of skills as a Java programmer, I was unable to obtain the desired set of classes... so I just sticked with what I found for warDependencies and hacked the code with a bit of copy & paste & rename. Maybe you can properly refactor the patch? ",
MTOMCAT-106,you are using mvn 2.x ? ,
MTOMCAT-106,"Yep, 2.2.1 ",
MTOMCAT-106,ok. fyi You won't have this issue with 3.x. BTW I have to fix that  ,
MTOMCAT-106,"Thank you. At the moment I have to keep the software I'm co-working on, at version 2.2.1 because it's quite complex and many components and features need to be re-thinked before we can upgrade to mvn 3. We will approach mvn 3 during 2012 for sure, but for the next release we really look forward for your fix. Thank you again for your great work and quick response",
MTOMCAT-106,patch not used. Implemented with a more easy solution. SNAPSHOT deployed to http://repository.apache.org/content/groups/snapshots-group/. reopen if you still have issue. ,
MTOMCAT-106,"attached patch not used, implements a more simple solution.",
MTOMCAT-106,"Thank you, works fine ",
MTOMCAT-108,"I've tested this patch on Mac OSX 10.6.8, Fedora Core 16, and Redhat Linux 5.4. It handles, httpPort not being set so only https starts up it adds 3 options to the startup -keyAlias -clientAuth. It checks for the 6 -Djavax.net.ssl properties for setting up key and trust stores. I also added PasswordUtil to allow the passwords to be obfuscated (a security requirement for some systems is no passwords in clear text ... I know obfuscation is like closing your front door and hoping know one uses the peep hole). I've confirmed function as best I can with and without http turned on and with and without clientAuth.  It appears to be working correctly. ",
MTOMCAT-108,@Brad looks a good patch. Note the maven plugin use the maven code formatting: http://maven.apache.org/developers/conventions/code.html ,
MTOMCAT-108,btw regarding this obfuscate stuff I wonder how it's secured as the deobfuscate method is open source  ,
MTOMCAT-108,"Oliver, 1. Sorry about the formatting I tried to follow what you had, as odd as it looked.  If you can't take it as is then I'll have to fix it on Monday. 2. As for the obfuscate stuff, its most definitely breakable but at least 'protects' it from prying",
MTOMCAT-108,"why not reading the password from a file ? And -httpsPasswordFile= (the file will contains only the password) this comment ""Lifted from Jetty org.mortbay.jetty.security.Password"" can/will lead in some ip issues. With such source code import, license headers must be preserve and I don't see that in your patch. ",
MTOMCAT-108,"Reading plain text from a file wouldn't help the security scan software would still find it, it needs to be non-human-readable. Uses the Apache 2.0 License which says I can reuse the code and I just put your style of comment in the header so it would match the rest of your code. ",
MTOMCAT-108,1. Cleaned up formatting. 2. Added a @see to PaswordUtil pointing back to Jetty Source. 3. Will apply cleanly after your patch to MTOMCAT-109 ,
MTOMCAT-108,applied. SNAPSHOT will deployed by jenkins build. Reopen if any issues. Thanks for the patch! ,
MTOMCAT-108,MTOMCAT-108 The httpsPort flag starts another http thread not an https thread Submitted by Brad Giaccio.,
MTOMCAT-108,How long will it take for this to make its way to the Maven repo?   ,
MTOMCAT-108,"Keith until a release, Currently you can use 2.0-SNAPSHOT to test. see http://tomcat.apache.org/maven-plugin-2.0-SNAPSHOT/snapshot-test.html ",
MTOMCAT-108,"Olivier.  Sorry, I meant when will it be available in the SNAPSHOT?  Currently the SNAPSHOT is showing a last build date of Novermber 11th, 2011. ",
MTOMCAT-108,"Keith what Repository are you looking in https://repository.apache.org/content/groups/snapshots-group/org/apache/tomcat/maven/tomcat7-war-runner/2.0-SNAPSHOT/ The files from Tue Dec 20 00:16:29 UTC 2011, the ones after  that (i.e Today) appear to have fi",
MTOMCAT-108,http://tomcat.apache.org/maven-plugin-2.0-SNAPSHOT/snapshot-test.html says https://repository.apache.org/content/groups/snapshots-group ,
MTOMCAT-108,"My bad, I was looking at the old repo location. Keith On Wed, Dec 21, 2011 at 1:51 PM, Olivier Lamy (Commented)",
MTOMCAT-118,Code,
MTOMCAT-118,apologizes mvn --version output attached to environment field ,
MTOMCAT-118,argh I see this can work only with 3.0.3+. I will fix that but until you must use 3.0.3/3.0.4 ,
MTOMCAT-118,Thanks for the work around - I've no problem with upgrading maven anyway ,
MTOMCAT-118,before 3.0.3 maven doesn't support mojo fields Collections other than String,
MTOMCAT-119,"The reason for this is actually in org.apache.catalina.startup.ContextConfig processContextConfig implementation for tomcat 7, which is different from tomcat 6.  Digging into that code shows in tomcat 6 some careful ""does this file exist?"" checks before any xml stream parsing happens.  In tomcat 7 an InputSource is created against the file without checking for the file's existence (which is allowed).  After that an attempt is made to create a stream from that InputSource, which fails, but moving forward from that the method only checks for the presence/absence of the InputSource instance instead of whether or not the stream was created successfully. Whether this is a bug in tomcat 7 or not I do not know.",
MTOMCAT-119,"I'm attaching perhaps a simpler example to replicate the matter.  This attached sample works fine, because in webapps/META-INF I have a context.xml file with this content:  To test the sample, from the base root, run. mvn clean install tomcat:run-war Then check from a browser: http://localhost:8080/doubleit/services/doubleit?wsdl that you can see a WSDL document (i.e., working). If I remove the context.xml file, I get the similar file-not-found exception as the original poster. ",
MTOMCAT-119,"Patch attached.  Note one slight potential drawback, is that now both tomcat7:run & tomcat7:run-war will check for the default META-INF/context.xml if contextFile is not explicitly configured (and use that file), however neither will report anything if that file is not present (as it doesn't need to be).  In the past only tomcat7:run-war checks for that file if contextFile is not set. This patch fixes the exception because now if META-INF/context.xml is not present (and contextFile not explicitly configured for another file), no file is sent to Tomcat and hence the latter will use whatever context defaults it uses.  However, if you explicitly configure any file via contextFile that doesn't exist Tomcat will continue to throw a File Not Found exception (as it should), alerting the developer to the problem. Unrelated issue fixed, in AbstractRunWarMojo the @Parameter property ""maven.tomcat.warDirectory"" was not defined, making users unable to provide a setting via the CLI -Dmaven.tomcat.warDirectory option (although it is defined in that class' parent AbstractRunMojo, I've found there's no inheritance of individual @Parameter values, just the @Parameter tag itself.) ",
MTOMCAT-119,fixed http://svn.apache.org/r1432054. Thanks ! ,
MTOMCAT-119,MTOMCAT-119 tomcat7:run-war fails because it expects META-INF\context.xml. Submitted by Glen Mazza (Revision 1432054),
MTOMCAT-123,Test project that demonstrates bug. ,
MTOMCAT-123,fixed r1297241. SNAPSHOT deployed. ,
MTOMCAT-123,MTOMCAT-123 Can't add test scoped dependency filters to web.xml. (Revision 1297241),
MTOMCAT-125,Do you have more logs ? (especially container start ). How do you add dependency on your jdbc driver when you use the plugin ?,
MTOMCAT-125,"I add the jdbc driver and commons-dbcp to the plugin's dependencies. I have a sample project, but I donot know how to attach it here. I have posted it in the mailing list. ",
MTOMCAT-125,I attached the sample for reproducing it. ,
MTOMCAT-128,fixed. If you could try 2.1-SNAPSHOT. ,
MTOMCAT-128,MTOMCAT-128 The plugin ignores and/or parses web.xml incorrectally when using tomcatWebXml option (Revision 1440473),
MTOMCAT-128,"I can't, receiving 302 Found error.",
MTOMCAT-128,any network issues accessing to  http://repository.apache.org/content/repositories/snapshots ? what is the content of those files:,
MTOMCAT-128,"Yes, I have this problem.",
MTOMCAT-128,weird which maven version are you using ? Last one follow redirect. To fix that you can probably use directly https://repository.apache.org/content/repositories/snapshots as snapshots repository. (note https rather than http) ,
MTOMCAT-128,Can you access this URL https://repository.apache.org/content/repositories/snapshots/org/sonatype/plugins/maven-metadata.xml?,
MTOMCAT-128,why this groupId org.sonatype.plugins ?? this is not something hosted @ asf ,
MTOMCAT-128,Is there anything wrong here?,
MTOMCAT-128,so maven see 404 and doesn't care of that. Can you cleanup your local repo ? then try again with -U,
MTOMCAT-128,Code,
MTOMCAT-128,Please use https://repository.apache.org/content/repositories/snapshots (note it's httpS ) as maven 3.0.3 can have issues with following redirect and/or upgrade to maven 3.0.4 ,
MTOMCAT-128,"I've update maven to 3.0.4. But, It seems that the fix didn't no effect.",
MTOMCAT-128,please provide a sample project to reproduce. ,
MTOMCAT-128,Sample maven project ,
MTOMCAT-128,Thanks. But I understand more your trouble now. tomcatWebXml field is to override the usual global web.xml used by tomcat (usually located in $CATALINA_HOME/conf/web.xml). See in  target/tomcat/conf/web.xml. But what you can do to have interpolations of src/main/webapp/WEB-INF/web.xml is something like that,
MTOMCAT-128,"Hi. Just one question: why tomcatWebXml field is used to override the usual global one if I'm using an embedded Tomcat server? Shouldn't, thus, exist the embeddedTomcatWebXml field as well? Regards ",
MTOMCAT-128,"Hello, we are in the same situation, we need to use a web.xml not placed in WEB-INF/classes, in order to use the maven-filtered one, that is available in ""target"" directory not in ""src/main/webapp"" directory. I just modified tomcat7-maven-plugin to add a new parameter, just adding those lines to AbstractRunMojo.java. Regards. ",
MTOMCAT-129,"A workaround is to run maven with -Dmaven.tomcat.path="""" and don't set <path> to something else in the plugin's configuration (<path></path> is ok) ",
MTOMCAT-133,"I try to map servlet to ""/"" in web.xml, that works. For information, in Spring documentation, you have this quote. Mapping to '/' under Tomcat. Apache Tomcat maps its internal DefaultServlet to ""/"", and on Tomcat versions <= 7.0.14, this servlet mapping cannot be overridden programmatically. 7.0.15 fixes this issue. Overriding the ""/"" servlet mapping has also been tested successfully under GlassFish 3.1.The related issue in tomcat : https://issues.apache.org/bugzilla/show_bug.cgi?id=51278",
MTOMCAT-133,"Here is new information to solve this bug: The code to add mapping. In Tomcat 7 standalone, the conflits set is empty. In Maven Tomcat 7 plugin, the conflicts set contains the String ""/"". I investigate, and one difference I found is that the ""Wrapper"" for the jsp servlet and the default servlet is ""overridable"" in tomcat 7 but not in maven plugin context.",
MTOMCAT-133,"The difference comes from the declaration of default servlets. In the plugin, the declaration is made into ""Tomcat"" class, in the method initWebappDefaults. And in this way, the wrapper is not set as overridable.",
MTOMCAT-133,fixed. ,
MTOMCAT-133,Code,
MTOMCAT-134,Proposed fix ,
MTOMCAT-134,patch applied. Thanks! ,
MTOMCAT-134,Code,
MTOMCAT-135,Code,
MTOMCAT-136,can you paste running mvn -e ?,
MTOMCAT-136,Code,
MTOMCAT-136,more details I need: do you have a dependency of type war in your project ?,
MTOMCAT-136,should be fixed now. SNAPSHOT deployed. ,
MTOMCAT-136,Code,
MTOMCAT-138,artifactId has changed for tomcat7 see documentation added here http://svn.apache.org/viewvc?view=revision&revision=1324716 ,
MTOMCAT-138,Code,
MTOMCAT-138,SNAPSHOT doc deployed here http://tomcat.apache.org/maven-plugin-2.0-SNAPSHOT/tomcat7-maven-plugin/adjust-embedded-tomcat-version.html ,
MTOMCAT-139,Code,
MTOMCAT-139,Code,
MTOMCAT-141,fixed r1325731. ,
MTOMCAT-141,Code,
MTOMCAT-142,Great to see this is scheduled for 2.0.  Note also that the contextFile does not have a $ {maven.tomcat.*}  expression that can be used to specify it on the command line. that would be useful if you are going to hack around in there a bit. ,
MTOMCAT-142,you mean applying filtering for the file with Maven sys props ? ,
MTOMCAT-142,"I guess that's what I mean, but I wouldn't have used those words.  For instance the path config can be provided on the command line with -Dtomcat.maven.path=/foo, but there is no corresponing setting for the contextFile (as can be seen here http://tomcat.apache.org/maven-plugin-2.0-SNAPSHOT/tomcat7-maven-plugin/run-mojo.html or in the plugin.xml). ",
MTOMCAT-142,oh ok I see. I was talking about something different. So  you could use -Dfoo.mypath= ,
MTOMCAT-142,If I could do that it would be useful too.  But not as easy for you as just exposing an expression. ,
MTOMCAT-142,Not a problem I already did that here http://maven.apache.org/shared/maven-filtering/ in a shared maven library  ,
MTOMCAT-142,all done. Have fun  ,
MTOMCAT-142,Code,
MTOMCAT-142,"Olivier, you marked this as fixed, but it doesn't seem to work for me.  I grabbed a snapshot (20120413.171048-58) - maybe your change is not deployed yet?  Specifiying -Dmaven.tomcat.contextFile on the command line I can see that the value is picked up when I look at the console output, but it still completely ignores my context.xml.  Can you send me a context.xml that works to change the context root?  ",
MTOMCAT-142,weird. Are you using an absolute or relative path in -Dmaven.tomcat.contextFile ? I have something very simple based on http://svn.apache.org/repos/asf/archiva/trunk/archiva-modules/archiva-web/archiva-webapp/src/test/tomcat/tomcat-context-archiva.xml  ,
MTOMCAT-142,"That app has the same artifact I'd as the path in your context.xml, I think. What happens if you change the path in the context.xml? JIRA SUCKS ON AN IPHONE. ",
MTOMCAT-142,"If I use <Context path=""/foo""> the webapp is available on /foo  ",
MTOMCAT-142,"OK, yes that works.  But <Context path=""""/> does not work (and I think that's correct), and <Context path=""/""/> breaks in the same way as MTOMCAT-141. ",
MTOMCAT-142,oops my bad. Should be better now with r1326764. (SNAPSHOT deployed) ,
MTOMCAT-142,Code,
MTOMCAT-142,"Yes, that works now.  But only with path=""/"".  Is that right?  Or should it be path=""""? ",
MTOMCAT-142,"sure only ""/"" works. if path="""" that's considered as no path defined. ",
MTOMCAT-143,contextFile not used with current trunk ? Sure ?? Any sample project to show this issue ? ,
MTOMCAT-143,"Here is my configuration in pom.xml. Since the middle of the last week, this doesn't work anymore. The definition of my users in the specified tomcat-users.xml is not used. ",
MTOMCAT-143,Should be fixed now. Please test with last deployed SNAPSHOT ,
MTOMCAT-143,Code,
MTOMCAT-144,I need more informations. contextPath of the webapp and getServletContext().getResourcePaths(path) (value of path ??) ,
MTOMCAT-144,please provide a sample project to reproduce. ,
MTOMCAT-144,Code,
MTOMCAT-144,fixed and SNAPSHOT deployed. ,
MTOMCAT-158,"This patch will serve files out of WAR overlays if they are not found in docBase. It only applies to the tomcat6 component. If I receive positive feedback, I will work it into the tomcat7 component as well ",
MTOMCAT-158,"So applying and quickly testing the patch, it cause issue (no backward compatible change) on a project I work (sorry I didn't get time to investigate more). The project is located @ http://svn.apache.org/repos/asf/archiva/trunk/. To reproduce just use.",
MTOMCAT-158,"The patch also apparently doesn't work when loading the overlays from the reactor. Olivier, given this issue, how are you developping Archiva with so many WAR overlays? Are you installing the overlays and restarting tomcat each time? I see you have some configuration of the maven-dependency-plugin but it's with skip=true. ",
MTOMCAT-158,"I'd like to use this with tomcat7, please. ",
MTOMCAT-166,dup of MTOMCAT-116 ,
MTOMCAT-168,that must be fixed with 2.0-SNAPSHOT. Can you try it ? see http://tomcat.apache.org/maven-plugin-2.0-SNAPSHOT/snapshot-test.html ,
MTOMCAT-168,please provide more infos. ,
MTOMCAT-169,"Patch to allow wars defined via ""webapps"" config to be deployed. Change from addContext to addWebapp. Please advise if this is correct and what the implications of changing are.  ",
MTOMCAT-169,"Hi, Your patch file doesn't provide any informations on which class has been modified. Thanks ",
MTOMCAT-169,"Sorry about that. MTOMCAT-170 has the same problem, too. I'll generate a new patch shortly and then attach it to both tickets. ",
MTOMCAT-169,"patch generated from local development repository. Note that it will conflict with M170, but I wanted to have the patches stand by themselves.",
MTOMCAT-169,patch applied with modification. you must configure <asWebapp> for the additional webapp. ,
MTOMCAT-169,Code,
MTOMCAT-170,"Simply calls ""createWebappLoader"" in order to inherit the configuration of the main web application configuration. It would be ideal to be able to configure this on a case by case basis (per webapp) but this could be a reasonable start.  ",
MTOMCAT-170,patch file doesn't say which class has been patched. Thanks,
MTOMCAT-170,Patch file for issue - generated from local development repo.  ,
MTOMCAT-170,patch applied. Thanks ! ,
MTOMCAT-170,Code,
MTOMCAT-173,That's not a problem as classpath elements (the output directory of your dependency from reactors is added to the class path). Which maven version are you using ? (I have to say that work only with maven 3).,
MTOMCAT-173,I'm using: Apache Maven 3.0.4 (r1232337; 2012-01-17 09:44:56+0100) ,
MTOMCAT-173,So should work fine. Do you have any sample project to reproduce your isssue ? ,
MTOMCAT-173,"I just found what the problem is: I created a new project using your archetype. Example directory structure: I usually build and start all modules from the projects root directory, in this case: mvn -am -pl dep-issue-webapp clean install && mvn -pl dep-issue-webapp tomcat7:run. if I start the tomcat plugin this way, all direct dependencies of the dep-issue-webapp module will not be added to the classpath by Maven, the startup of the webapp will fail. If I instead do this, mvn. Maven will add all direct dependencies to the classpath and the webapp will start correctly. ",
MTOMCAT-173,use: mvn -pl :dep-issue-webapp -am tomcat7:run  ,
MTOMCAT-173,"mvn -pl :dep-issue-webapp -am tomcat7:run that actually works, but it means that I would ALWAYS have to build the webapp and ALL dependencies before I can start it. That may not be a problem in a small test project like the one the archetype creates, but this is a big problem in a project where I have lot of dependencies with (tens of) thousands of lines of code that have to be compiled over and over again if I just want to restart my webapp. ",
MTOMCAT-173,I miss you here ! Don't clean your projects and you won't rebuild/recompile everything. use only mvn -pl :dep-issue-webapp tomcat7:run (without -am) if you don't want to rebuild/recompile everything. ,
MTOMCAT-173,"> Don't clean your projects. oops, of course you're right about that. use only mvn -pl :dep-issue-webapp tomcat7:run (without -am) if you don't want to rebuild/recompile everything. if I don't use ""-am"", Tomcat will not have all necessary libraries in the classpath. if I use ""-am"" , Tomcat will have all necessary libraries in the classpath, but this won't work if any of my dependencies are also WAR artefacts which also have a Tomcat configuration since Maven will run the Tomcat plugin for the first module with plugin configuration in the dependency chain. Also, dependencies will only be added to the classpath if they are compile time dependencies, whereas a WAR artefact only needs runtime dependencies if it doesn't contain code that needs compiling. For me the only working workaround at the moment is to run ""mvn tomcat7:run"" directly in the directory of the webapp module. ",
MTOMCAT-173,"Hello, I have the same issue with tomcat7-maven-plugin:2.0. But it's during an integration test with junit. I have the structure below: The module project-domain isn't added to the classpath and the startup of the tomcat fails. In the maven debug log I see too 'skip adding artifact project-domain as it's in reactors'. In DefaultClassLoaderEntriesCalculator#calculateClassPathEntries at line 116, the method isInProjectReferences rejects the project-domain as it's in projectReferences of project-webapp. So is it possible to add a cheap workaround like a configuration parameter 'forceIncludeProjectReferences' in order to add those project references to module classpath? Thanks. ",
MTOMCAT-173,fixed http://svn.apache.org/r1408507 ,
RAT-3,"The text is a free unpaginated sample from the start of the file. However, stylesheet used to generate plain text report contained errant whitespace. This has now been removed.",
RAT-3,"The report now looks like this, This was using version 0.7",
RAT-3,"Fixed in r1203953 , feel free to test/check.",
RAT-3,Neither the first line nor the header are indented with svn revision 1204460,
RAT-7,I agree. See the perl scripts in https://svn.apache.org/repos/private/committers/relicense/src/perl/ insert_license.pl and update-AL20.pl. It has patterns for each version of Apache License header. There are also some variations in existence. I spent a lot of effort detecting each license type (also detect third-party code that uses the Apache License).,
RAT-7,Agreed. Want to help with the code?,
RAT-7,Is this task a duplicate of RAT-16 or what's the difference here? Thanks.,
RAT-7,I'm not sure what RAT-16 really is but I think RAT-16 is resolved and this one is the oposite of it. Over time we've had several license headers in use at the ASF and RAT doesn't properly recognize the ones that are outdated by now.,
RAT-7,"The scripts mentioned above needed to first detect that it is an AL-type header, then detect other patterns to distinguish between the variants.",
RAT-7,"Here are some modified snippets from those Perl scripts. Improvements: 	Needs to handle double-justified text, hence multiple whitespace. Ignore whitespace. 	Pattern matching needs to be multi-line .	Perhaps need to handle ""[Tt]he Apache"".",
RAT-7,"The FullTextMatchingLicense base class I extracted out of the OASIS license for MIT and GPL may already be usable for that. It matches on multiple lines, ignores whitespace (in fact everything that's not a character or a number) and is mostly case-insensitive.  It doesn't do regexes at all, though, it performs a substring match.  Actually I now realize this isn't true as it uses a regex internally, it would just strip out any special characters from the pattern the way it works right now.  Shouldn't be too difficult to adapt, in particular since we don't need to care about backwards compatibility (the class is new).",
RAT-11,Report correct num of unapproved licences,
RAT-11,The best way to fix this issue is by resolving RAT-14.,
RAT-11,"Oops, I didn't notice this issue when filing RAT-35. Resolving this as a reverse duplicate as the patch in RAT-35 got already applied.",
RAT-12,Correct SCM info,
RAT-12,Committed. Many thanks.,
RAT-29,I'm not seeing this error on Windows + Cygwin. I'm on Maven 2.0.7 rather than 2.0.8 in the original  report,
RAT-29,no NPE but some test failures :-/,
RAT-29,I upgraded to Maven 2.0.9 and the NPE is now gone.,
RAT-33,I'm guessing the source of this is <subscribe>rat-dev-unsubscribe@incubator.apache.org</subscribe> from the pom.xml.,
RAT-33,"Fixed in the pom.xml, will be fixed on the site automatically when we redeploy it.",
RAT-34,I stumbled over this when I refactored the AbstractLicenseAppender code.  The same is true for XML files without an XML decl which would need a slightly different resolution (the XML decl has to be added so we can add the comment). I'd prefer the second option myself in particular since we don't have any documentation anyway.,
RAT-34,fixed with svn revision 1147539,
RAT-35,The attached trivial patch fixes this.,
RAT-35,"Applied, thank you!",
RAT-36,"Proposed patch (against rat/main/trunk, revision 734690).",
RAT-36,Updated patch that modifies also the test cases to match the updated report contents.,
RAT-36,"Applied, thank you!",
RAT-39,Ensure no line-endings issues. Those seem to have mixed line-endings. A RAT committer on UNIX needs to run 'dos2unix' for those files.,
RAT-39,"Ensure that svn:eol-style settings are ""native"" for all text files. Mostly okay. The following are ones with issues. The committers who did 'svn add' for these files need to see. Also the files listed with dos2unix problems above, need to have 'svn propset svn:eol-style native'",
RAT-39,"Some xml files have missing ""XML Declaration"". See attached patch-xml-decl.txt",
RAT-39,I use Ant's fixcrlf task for that in a build file like the attached one. Thanks David,
RAT-39,Ant build file to fix inconsistent line ends,
RAT-41,Patch attached.,
RAT-41,"Applied, thank you!",
RAT-43,"Maybe this has been solved as a side effect of something else, or a configuration problem? Attached is a recent report using ant, no problems with it.",
RAT-43,"I can't reproduce it myself anymore either, it is quite possible there is/was a bug in the svn trunk version of Ant I've been using at that point in time.",
RAT-44,Changed in SVN and deployed to the Incubator web site. Thanks!,
RAT-62,Attaching diff of the 4 files with missing headers.,
RAT-62,"Added license headers to 3 files, preferring to exclude the BUILD.txt file. The tests should now just report on the 5 purposely failed files.",
RAT-63,attached a patch that adds the right URI to the -taskdef ant target. Thanks,
RAT-63,fixed in svn revision 815659. Thanks,
RAT-64,Here's a patch that corrects all the links and makes an attempt at cohesive navigation between all the pieces.,
RAT-64,Why not using preset menus [1] instead of enumerating modules and parents? 1 http://maven.apache.org/plugins/maven-site-plugin/examples/sitedescriptor.html (Including Generated Content),
RAT-64,Applied Felix's patch.,
RAT-70,Add missing section closing,
RAT-70,"Committed r907529 , thanks!",
RAT-71,"Some work has been done to the main RAT site, can you check against this patch and confirm if it is still required / upto date. Thanks",
RAT-71,"Patch still required and up to date. 	Missing page would be at [0] 	Or goto [1] and click on 'Goals'.",
RAT-71,"Committed r907528, thanks!",
RAT-80,"Hi, is the issue still relevant? Did you try the most current trunk? Could you provide more details of the failure (e.g. svn revision)? Thanks",
RAT-80,"No, I think this issue was fixed.",
RAT-81,"I think it depends on the java VM as well, I don't seem to get the MalformedInputException on my Ubuntu box using OpenJDK6. svn revision 1147110 contains an attempt at a testcase but it cannot assert anything since the precondition (there is a MIE) is never true for me. I think I know how to fix the problem using NIO but so far am reluctant as I don't have a good way to regression test it.",
RAT-81,svn revision 1147525 now contains the NIO based code which I think should mark the files as binary for you.  It would be good if you could verify for me whether RAT's trunk behaves as desired now.,
RAT-81,"Let's be optimistic and assume it is fixed, please reopen if the problem pops up again once 0.8 has been released.",
RAT-81,"It does not seem right to me to mark XML files with invalid contents as binary. Binary implies that the file does not need a license, but that is not the case. Such files should still have a valid license (unless excluded), so RAT should report the file as unreadable or similar.",
RAT-81,Also I'm not sure that the test files do have any errors in them,
RAT-82,Code,
RAT-82,Can you provide a justification for why it won't be fixed? I think this SHOULD be fixed.,
RAT-82,"Hi Emily, the justification is inside the email to the dev list I linked in my comment. The short version is that the Apache Software License is used by projects outside the ASF as well and their source codes will not have the first line of the license text you want the matcher to look for.  If we changed the matcher it wouldn't detect the license for all those other projects anymore.",
RAT-82,Thanks Stefan. Failed to spot the link previously.,
RAT-86,should be fixed with svn revision 1080572,
RAT-86,SVN version still isn't working for me.  I had to add the line.,
RAT-86,"Oops, yes, an oversight, sorry. Should finally be fixed with svn revision 1081440",
RAT-87,refactored the test class and made sure tests are trying as hard as possible to remove all temporary files.,
RAT-92,fixed with svn revision 1156143,
RAT-94,should be fixed with svn revision 1156170,
RAT-98,"The current behavior of the Maven plugin is to log the files that are excluded in the build log. It can look like this example, where I have excluded the BUILD.txt file. Did you want for this to be included in the report?",
RAT-98,"Yes, the details need to appear in the report so that it contains all the necessary information. Ideally this should show all files that are actually excluded, not the file names in the <exclude> tags - i.e. don't show missing files, but do show files that match wildcard entries. The console log is transitory, so should not be the only place where such information is reported.",
RAT-98,1,
RAT-98,"It would be helpful to show the active include and exclude configuration on the report as well; this should be after any defaults have been applied. [The defaults may change between releases, so it is helpful to have the active settings documented.] For excludes of directory trees, it probably does not make sense to list individual files (there will generally be too many). It's really only for the more specific excludes where the file names might be useful - the exclude may be too general.",
RAT-100,should be fixed with svn revision 1196463,
RAT-101,"Yes, I saw that when I published the release. Actually I'd prefer a download page like the Commons projects have and it is on my TODO list (I need to do the same for log4net) - I just need to investigate how it is done in Commons.  Do you have any pointers beyond ""read the Commons parent POM""?",
RAT-101,"Commons projects use the commons-build plugin to create download_component.xml which creates the .html as part of mvn site; the download_component.cgi file is manually created and referenced in site.xml. I suspect the plugin may be too Commons-specific to use, but you could just copy an existing download_*.xml file and modify that manually.",
RAT-101,"Download page is now more user friendly and contains direct links to the mirrors and unmirrored links for KEYS, signatures and hashes.",
RAT-101,Thanks. all looks OK now.,
RAT-102,Patch with the corrected typo,
RAT-102,"Applied, thanks for the patch",
RAT-107,Attaching apache-rat-project-RAT-107.patch which fixes the issue. Additional integration test is included.,
RAT-107,Is this ok to exclude?,
RAT-107,Presumably these services files could have the comment at the top as well? Not sure if required. @richardcloudsoft what do you think?,
RAT-107,"I excluded json as it doesn't support comments, I had a quick look at json files in jClouds (just as an example of an Apache TLP) and their json files don't include the apache header",
RAT-107,Code,
RAT-109,Thanks. Patch applied.,
RAT-109,"This patch does not work, at least for my project. It fails because mkdirs does not create the directories, but it does not create the directories because it already exists. It should only throw that exception if the folder does not exist and it fails to create. Patch is pasted below (I tested it for my case at the very least).",
RAT-109,Agreed. John is right. Sorry,
RAT-109,Verified that John is/was right.,
RAT-109,should be fixed with svn revision 1208763,
RAT-109,"There's a small window in the patch code which can potentially cause a failure, see",
RAT-109,applied with svn revision 1208826,
RAT-114,modification to pom files to ignore things in m2e,
RAT-114,Hi John. Apologies for taking a while to look at this one. Committed. Many thanks for the patch.,
RAT-116,"Agreed, Rob",
RAT-116,Documentation updated in r1405598 to better show what is excluded. We should probably open new issues if we want to exclude more files by default.,
RAT-117,Patched pom that makes mvn clean install run through.,
RAT-117,Since there's no component tentacles I filed the bug here (relates to email on mailing list from 2012-06-18) - sorry.,
RAT-117,"Thanks for recording the bug here. I lack project creation karma for JIRA, so I've opened an issue. We can move the bug later.",
RAT-117,Applied. Many thanks for supplying the patch.,
RAT-120,really simplify verify.bsh with a verify.groovy,
RAT-120,Applied (as part of RAT-122). Many thanks for the patch.,
RAT-121,NOTE: patch for RAT-120 include.,
RAT-121,Applied (as part of RAT-122). Many thanks for the patch.,
RAT-122,NOTE it include RAT-120 and RAT-121. Could you guys ask for a git mirroring ? I will be easy for more for providing different patches.,
RAT-122,Thanks for raising https://issues.apache.org/jira/browse/INFRA-5328,
RAT-122,(I'm going to assume that the 'missing-license' is just for your local use),
RAT-122,Applied. Many thanks for the patch.,
RAT-123,Reworked main page,
RAT-124,Fixed in r1405594. http://svn.apache.org/viewvc?view=revision&revision=1405594,
RAT-126,"Hi Bernd, we're pushing towards releasing Apache Rat 0.9 very soon. If you could supply a patch including appropriate tests, we might be able to squeeze a fix in. Otherwise, it will probably need to wait until 0.10 (though I hope we'll be able to release more often in the new future). Robert",
RAT-126,"Hello, I just noticed that according to the Source and Comments the defaultExcludes should already contain .git. And the defaultExcludes are also turned on bye default. At least this is what the -X debug run of Maven tells me. But still it says ""No Excludes"". So I am a bit confused. Maybe it was fixed since 0.8 (there are some changes but no obvious fix). Can anybody confirm that the excludes feature is excluding any default directorries? The default list in plexus DirectoryScanner includes. Any idea?",
RAT-126,Executing a freshly built 0.9-SNAPSHOT from the command line also indicates that there are no excludes.,
RAT-126,"The 'No excludes' message is quite confusing. I've improved this message and added extra debug logging. For me, mvn org.apache.rat:apache-rat-plugin:0.9-SNAPSHOT:check now ignores the .git/ directory, via plexus DirectoryScanner.DEFAULTEXCLUDES. I'm using mvn 3.0.5. So, I think this might be fixed. Please retest once the release candidate is available.",
RAT-126,"The fix looks good, it (suddenly?) also ignores the .git directory. BTW: I wonder if it is better to list only ignored resources instead of the exclude patterns. Especially for the number printed this is much more interesting.",
RAT-126,"Good - the plexus dependency has been updated for this release, which is where the standard. I found printing the excludes useful for debugging, but yes, listing the ignored resources sounds useful. I'll see if it would be easy and safe to add before the release.",
RAT-126,yup 3.0.10 include more default excludes (for git) and more performant for directories scanning. And backward compatible,
RAT-126,"I've committed a patch which prints those resources excluded and those included, for example",
RAT-126,Could this issue be safely marked as fixed in 0.9...? Robert,
RAT-126,I think this is fixed by the upgrade to the plexus dependency. Please reopen if not.,
RAT-128,Here are some useful recent discussion points.,
RAT-128,"Fixed some instances, which is unfortunate. Also ASL is used in various public variables.",
RAT-128,Code,
RAT-135,Fixed in r1545782.,
RAT-137,Also http://creadur.apache.org/rat/apache-rat-plugin/examples/basic.html,
RAT-137,Website not yet rebuilt,
RAT-137,This has been fixed and the site has been deployed.,
RAT-140,Code,
RAT-144,Refactored arrays into lists in rev1614759.,
RAT-145,Code,
RAT-148,Checked in as rev1612959. Thanks for reporting this issue.,
RAT-148,Thank you!,
RAT-151,Fixed in r1545767.,
RAT-152,Fixed in r1545772.,
RAT-153,Fixed in r1545775.,
RAT-154,Fixed in r1545780.,
RAT-155,Fixed in r1545785.,
RAT-156,Fixed in r1545788.,
RAT-158,Having such warnings now too on JDK 8 with rat 0.10.,
RAT-158,"As far as I can tell, this is nothing to do with RAT.",
RAT-158,"It only occurs when I use the rat plugin, which leads me to point to your project. Apache Taglibs seems to agree http://mail-archives.apache.org/mod_mbox/tomcat-dev/201311.mbox/%3C5287C162.6030208@kippdata.de%3E",
RAT-158,"RAT is used extensively elsewhere without issues as far as I know. It does not have a direct dependency on xerces. However, the plugin code does depend on doxia which in turn depends on xerces. The doxia module is part of Maven. Probably worth asking on the Maven user list.",
RAT-158,"Experiancing the same error message ... as soon as I add the plugin to my build, and execute ""apache-rat:check"" I get the exact same error. My Environment is Windows 7 64bit with Java 1.7.0_45 64bit using Maven 3.0.5 (But getting the same error on 3.1.1)",
RAT-158,Confirmed on Windows 7 with Maven 3.0.5 using Java 1.7.0_60 and Java 1.8.0_05. Cannot reproduce using Java 1.6.0_45. All versions are 64-bit.,
RAT-158,"Confirmed on Ubuntu 14.04, mvn 3.2.1, JDK 1.8.0.11 64bit.",
RAT-158,This java bug seems to be related: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8016153,
RAT-158,Following something,
RAT-158,"Hi Phillipp, I don't think that is the problem described in this issue. The problem is not that the value of the limit property is wrong, but rather that the newer JDKs are setting properties that Xerces don't know about. I had a look in the latest release of Xerces and the problem is still there, so upgrading to that version will not work.",
RAT-158,"I see the same problem with commons VFS site builds (using RAT) with recent Java 7 and 8. I wonder, why is external used/needed here anyway?",
RAT-158,"I was able to get this to work, but I don't know if it works in all situations.",
RAT-158,"Thank you Christopher: you found the cause and immediate workaround! I opened a Jira issue at Doxia to remove this nasty dependency in Doxia: http://jira.codehaus.org/browse/DOXIA-526. It would be nice if next Rat version could implement the workaround, even if Doxia is not yet released with the fix at root cause",
RAT-158,notice the workaround is now part of the future asf parent pom version 17: see MPOM-69,
