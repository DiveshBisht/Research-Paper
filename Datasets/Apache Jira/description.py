from bs4 import BeautifulSoup
import urllib.request
import csv

project_name = 'LUCENE' #'MTOMCAT' 'RAT'
data = [['ISSUE_ID','PRIORITY','STATUS','RESOLUTION','CREATION_DATE','CONTEXT']]
#mtomcat
#list_bug_id = ['103','104','105','106','108','118','119','123','125','128','129','133','134','135','136','138','139','141','142','143','144','158','166','168','169','170','173','177','178','183','184','185','186','193','197','198','202','205','210','211','214','215','216','219','220','221','225','227','232','233','237','238','249','255','257','258','259','262','264','266','269','270','276','279','281','283','286','287','289','295','297','298','300','305','307','308','309','310']
#lucene
#list_bug_id = ['7616','7629','7630','7631','7635','7642','7646','7647','7650','7651','7652','7657','7662','7668','7670','7672','7674','7676','7679','7682','7683','7685','7687','7690','7695','7698','7704','7708','7711','7715','7717','7718','7719','7724','7726','7728','7731','7735','7749','7755','7757','7758','7759','7760','7761','7766','7767','7769','7775','7777','7779','7780','7783','7791','7793','7794','7795','7797','7799','7803','7805','7807','7808','7809','7810','7814','7817','7818','7819','7820','7821','7822','7824','7825','7830','7831','7832','7833','7836','7838','7842','7843','7847','7848','7849','7851','7859','7861','7864','7865','7869','7870','7871','7872','7878','7879','7884','7885','7887','7888','7890','7892','7894','7896','7903','7909','7911','7914','7916','7919','7920','7925','7926','7931','7932','7933','7938','7941','7942','7943','7944','7945','7946','7950','7952','7956','7957','7961','7963','7965','7967','7968','7971','7977','7978']
#rat
#list_bug_id = ['3','7','11','12','29','33','34','35','36','39','41','43','44','62','63','64','70','71','80','81','82','86','87','90','92','94','97','98','100','101','102','107','109','114','116','117','120','121','122','123','124','126','128','133','134','135','137','140','141','142','144','145','146','148','151','152','153','154','155','156','158','159','160','168','169','173','174','175','176','177','178','179','180','188','190','207','211','212','214','215','222','224','225','228','229','230','234','235','236','238','239','240']

base_url = 'https://issues.apache.org/jira/browse/' + project_name + '-'
for j in list_bug_id:
    url = base_url + str(j)

    with urllib.request.urlopen(url) as url:
       html = url.read()
    soup = BeautifulSoup(html, "html5lib")
    
    issue_id = project_name + '-' + str(j)

    creation_date = soup("span",{"id":"create-date"})
    #description = soup("div", {"class":"user-content-block"})
    priority = soup("span", {"id":"priority-val"})
    status = soup("span", {"id":"status-val"})
    resolution = soup("span",{"id":"resolution-val"})
    context = soup("h1", {"id":"summary-val"})

    context_body =  context[0].text.strip()
    priority_body = priority[0].text.strip()
    status_body = status[0].text.strip()
    resolution_body = resolution[0].text.strip()
    creation_date_body = creation_date[0].text.strip()
    data.append([issue_id, priority_body, status_body, resolution_body, creation_date_body, context_body])


with open('description.csv','w',newline='',encoding="utf-8") as fp:
    a = csv.writer(fp, delimiter=',')
    a.writerows(data)
