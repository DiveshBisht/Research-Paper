ISSUE_ID,USER,DATE,COMMENT
LUCENE-7616, Trejkaz,03/Jan/17 02:19,"As an additional nasty point, sometimes this method is called from places like BooleanQueryNodeBuilder, where it goes into an error message to show the user. So this error message also shows the wrong syntax, but it also isn't immediately clear how a QueryNodeBuilder would know what syntax was used to create the QueryNode it has been passed..."
LUCENE-7629, Mike Drob,11/Jan/17 23:12,"I suspect it might be a JVM issue with nothing that we can do to fix it, but couldn't find any other JIRAs reporting the same thing so it made sense to create one."
LUCENE-7630, Uwe Schindler,13/Jan/17 12:51,"Hi, could you create a Pull Request and add the link here?

About your branch: I would not use cloneAttributes() because thats slow for this simple case. cloneAttributes() only helps if you want to modify the attributes in the AttributeSource that was created, but is not useful for simple save/restore use cases.

For your case, you should simple use captureState(), save the state object and then call restorestate() instead of clearAttributes(). After restoring you can adapt term text and positions/offsets. In addition when you clone or capture state, the call to clearAttributes() is useless and also slows down. When restoring states, everything is restored, so the additional clearing before is not needed."
LUCENE-7630, Nathan Gass,13/Jan/17 15:05,"I commited the suggested improvements and made a pull request https://github.com/apache/lucene-solr/pull/138.

The NGramTokenFilter probably has the same issue. I can port the fix to that class when everything is correct."
LUCENE-7630, Uwe Schindler,13/Jan/17 15:06,"The NGramTokenFilter probably has the same issue. I can port the fix to that class when everything is correct.

Please do! You can update the current PR. Otheriwise PR looks fine."
LUCENE-7630, Nathan Gass,13/Jan/17 16:48,done
LUCENE-7630, Uwe Schindler,16/Jan/17 08:55,"Thanks, I will merge and commit this after some testing!"
LUCENE-7630, Uwe Schindler,16/Jan/17 11:14,Thanks Nathan!
LUCENE-7631, Mike Drob,14/Jan/17 01:29,"Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future. We can deal with fixing existing rawtypes or a few other classes of warning in the future.

David Smiley - you seemed interested in this conversation last time it came up."
LUCENE-7631, David Smiley,16/Jan/17 16:41,Thanks for filing this issue!  Are all the changes in this patch necessary to get the build to pass?  So to clarify... no code (outside what the patch touches) needs adjustments?
LUCENE-7631, Mike Drob,16/Jan/17 17:38,"Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.

Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.

I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that."
LUCENE-7631, Uwe Schindler,16/Jan/17 18:43,"Thanks Mike!
I am for using this patch. Robert's suggestion was to enable ""all"" warnings, but IMHO this is a bad idea, because if somebody compiles with a later Java version, the build may suddenly fail (because a later version of the compiler added a new warning type).

I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!

The only downside of this patch is that we no longer get any warnings displayed that are currently disabled (rawtypes, unchecked). So we should fix them asap (in a separate issue).

BTW: Maybe we can enable rawtypes and unchecked errors earlier in Lucene and leave them disabled in Solr. As far as I remember we already have a separate warning setting for Solr. This may be the reason why Solr does not show any problems!?"
LUCENE-7631, Uwe Schindler,16/Jan/17 18:44,"Yes on Solr your change is not enabled: https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30

We should also review Solr (maybe in a separate issue)."
LUCENE-7631, Mike Drob,20/Jan/17 22:58,"I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!
Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.

I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?"
LUCENE-7631, Uwe Schindler,21/Jan/17 08:51,"Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.

What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error)."
LUCENE-7635, Masaru Hasegawa,16/Jan/17 08:49,Here is a patch.
LUCENE-7635, Hoss Man,18/Jan/17 21:59,"i'm not very familiar with Kuromoji but i believe the lines you 're deleting in this patch are intended to catch comments at the end of a line – not just the begining, ie...


# comment at start of line

朝青龍,朝青龍,アサショウリュウ,カスタム人名 # end line comment, has a comma in it
                                   # spans more then one line
abcd,a b cd,foo1 foo2 foo3,bar     # Another end line comment



Since it seems like the intent of the UserDict format is to be ""CSV with '#' comments"" it seems like the comment stripping should be moved to o.a.l.analysis.ja.util.CSVUtil where it can be done if-and-only-if the '#' is not part of a quoted value...


朝青龍,朝青龍,アサショウリュウ,カスタム人名  # end line comment, has a comma in it
                                    # spans more then one line
abcd,a b cd,foo1 foo2 foo3,bar      # Another end line comment
""quoted#sharp"",other,""quoted,stuff"" # yet another end line comment



ie: add a if(c == '#' && !insideQuote) block (similar to the existing COMMA conditional) to CSVUtil.parse() that would (trim and) add the final value to result and break out of the for loop.

?"
LUCENE-7635, Masaru Hasegawa,24/Jan/17 02:11,"It was done long time ago. I don't remember original intention... But it doesn't seem to be common for dictionaries to have comments at the end of line (synonym, stop).
Wonder if it's better to follow the same rule."
LUCENE-7646, Michael McCandless,20/Jan/17 13:53,"Note that you can use {noformat} around code blocks here to preserve indendentation.

Can you post the NullPointerException?

Was this already fixed by LUCENE-7576?  Though your ""silly"" regexp (..) should have been a NORMAL case I think?

I turned your code fragment into a test case (will attach patch) and it doesn't hit an NPE ...."
LUCENE-7646, Michael McCandless,20/Jan/17 13:54,"Patch w/ test case, but it does not show the NPE..."
LUCENE-7646, Tom Mortimer,23/Jan/17 11:35,"Mike, thanks for the comments. I didn't notice that when I posted the code like that, it changed the regex from


.*.*


to


..


which doesn't cause a problem. I've since found the same problem with the ""non-silly"" expression 


.*



Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.lucene.index.AutomatonTermsEnum.<init>(AutomatonTermsEnum.java:79)
	at RegExpNPE.main(RegExpNPE.java:20)"
LUCENE-7646, Tom Mortimer,23/Jan/17 12:04,I've just tried with 6.4 and it does indeed seem to have been fixed by LUCENE-7576. I'd not realised I was using the API incorrectly. Thanks!
LUCENE-7646, Michael McCandless,23/Jan/17 12:09,"Phew, thanks for bringing closure Tom Mortimer!"
LUCENE-7647, Adrien Grand,20/Jan/17 15:02,"Here is a patch. On the writing side, things are easy since there is a single instance that is used from a single thread and for a short amount of time, so I just made the compressor implement Closeable. However things are a bit more complicated on the reading side because of clones and the fact that we do not close them all. So to keep things simple, I just changed the codec to create Inflater instances on demand."
LUCENE-7647, Robert Muir,20/Jan/17 15:33,What does it do to performance to create a deflater instance every time? This seems very inefficient.
LUCENE-7647, Robert Muir,20/Jan/17 15:38,s/deflater/inflater of course
LUCENE-7647, Adrien Grand,20/Jan/17 15:47,"For fetching the top hits I think it is fine anyway, if there is an issue I suspect it would be more with merging. I can try to run luceneutil with this change next week. Do you have ideas to make it more efficient maybe?"
LUCENE-7647, Robert Muir,20/Jan/17 15:55,"I think its first important to understand how it impacts performance, including worst cases. That means merging with deletes and lots of results and stuff too: not just best-cases like top hits only.

Alternative solutions are possible depending on the impact: e.g. pool managed by the top Decompressor and passed via clone(), and decompress could simply release back to the pool. This is kind of a standard pattern, but of course it adds complexity. We should avoid it if its really not necessary."
LUCENE-7647, Adrien Grand,23/Jan/17 15:29,"I ran a merge that cantains 1M documents from the wikipedia benchmark including deleted docs, in order to test the worst case. Here is what the info stream reports about stored fields before/after the change:

Before:

SM 0 [2017-01-23T15:03:34.956Z; Lucene Merge Thread #0]: 41827 msec to merge stored fields [996093 docs]
SM 0 [2017-01-23T15:06:49.785Z; Lucene Merge Thread #0]: 41722 msec to merge stored fields [996093 docs]
SM 0 [2017-01-23T15:14:09.943Z; Lucene Merge Thread #0]: 42138 msec to merge stored fields [996093 docs]



After:

SM 0 [2017-01-23T15:17:33.241Z; Lucene Merge Thread #0]: 42050 msec to merge stored fields [996093 docs]
SM 0 [2017-01-23T15:20:00.656Z; Lucene Merge Thread #0]: 42320 msec to merge stored fields [996093 docs]
SM 0 [2017-01-23T15:22:04.047Z; Lucene Merge Thread #0]: 42520 msec to merge stored fields [996093 docs]



I think this is either noise a an acceptable slow down. That makes sense since we always decompress about 16K of data. Initialization of the Inflater is likely much less costly than decompressing that amount of data."
LUCENE-7647, Robert Muir,25/Jan/17 15:51,Thanks for running the benchmark!
LUCENE-7650, Otmar Caduff,21/Jan/17 11:49,Java file for reproducing the error.
LUCENE-7650, Mikhail Khludnev,30/Jan/17 14:43,"since Complexphraseqp parses query twice, you need escape it twice. That is."
LUCENE-7650, Otmar Caduff,31/Jan/17 16:24,"Thanks! That helped. Not an issue anymore for me.
I think it would make sense to add to ComplexPhraseQueryParser's javadoc the following comment (or something alike):

Special care has to be given when escaping: because some parts of the query might be parsed multiple times, these parts have to be escaped as often."
LUCENE-7651, Uwe Schindler,22/Jan/17 09:15,"I set to ""critical"", because this also breaks our latest release 6.4, which can't be build from source with Java 8 update 121."
LUCENE-7651, Uwe Schindler,22/Jan/17 09:16,I will try to figure out if moving the Javascript to a separate file helps here. In HTML 5 inline Javascript in HTML files is a no-go.
LUCENE-7651, Uwe Schindler,22/Jan/17 09:30,"I checked it out, it disallows any <script>, also externally linked ones in the -bottom parameter of Javadocs. This is amajor pain. As adding this special allow-javascript parameter is brekaing builds with older versions and its hard to detect the exact build version in Ant, we have 2 options:


	Remove prettify.js
	Inject the prettify code somehow else into the docs (e.g. by copying all script files together and append them to the javadoc-generated JS file after javadoc succeeds)."
LUCENE-7651, Uwe Schindler,22/Jan/17 09:35,"This implements first option:


 lucene/common-build.xml | 14 --------------
 1 file changed, 14 deletions(-)

diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 48cf457..61948bb 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -2107,20 +2107,6 @@ ${ant.project.name}.test.dependencies=${test.classpath.list}
         <link offline=""true"" href=""${javadoc.link}"" packagelistLoc=""${javadoc.packagelist.dir}/java8""/>
         <bottom><![CDATA[
           <i>Copyright &copy; ${year} Apache Software Foundation.  All Rights Reserved.</i>
-          <script src='{@docRoot}/prettify.js' type='text/javascript'></script>
-          <script type='text/javascript'>
-            (function(){
-              var oldonload = window.onload;
-              if (typeof oldonload != 'function') {
-                window.onload = prettyPrint;
-              } else {
-                window.onload = function() {
-                  oldonload();
-                  prettyPrint();
-                }
-              }
-            })();
-          </script>
         ]]></bottom>
         
         <sources />



I checked our sources. We allready append the CSS of prettify to Javadocs's main CSS files. As Java 8 contains also a Javadocs-generated scripts.js file, we could do the same here. I am working on that..."
LUCENE-7651, Uwe Schindler,22/Jan/17 09:46,This would be my proposal.
LUCENE-7651, Uwe Schindler,22/Jan/17 09:50,Sorry last patch had a missing pair of brackets. This fixes it any also works locally. Stupid Javascript! KRRRR!
LUCENE-7651, Uwe Schindler,22/Jan/17 10:12,"Attached is a patch that also updates Prettify to latest version. The used one produced Javascript errors in Chrome, so I updated. I also removed the useless additional language plugin files."
LUCENE-7651, Uwe Schindler,22/Jan/17 10:38,"Final updates, I think this is committable.

Any suggestions?"
LUCENE-7651, Uwe Schindler,22/Jan/17 11:16,Add correct license header to CSS file.
LUCENE-7651, Uwe Schindler,22/Jan/17 12:13,"I just checked. The ""hack"" also works with IBM J9, so I see no problem doing this. I was afraid that maybe IBM does not have script.js in the Javadoc output.

Nevertheless, this is not risky like the previous hack. If the Javadocs generator no longer produces script.js or stylesheet.css the Code Prettify would just no longer work, but not break Javadocs or build."
LUCENE-7651, Uwe Schindler,22/Jan/17 12:18,I sent the following message to OpenJDK: http://mail.openjdk.java.net/pipermail/javadoc-dev/2017-January/000281.html
LUCENE-7651, Uwe Schindler,22/Jan/17 17:31,"I committed this to master and branch_6x. I will now disable all 6.4 branch builds. If we will have a further bugfix release on this branch, we should backport this. Please reopen in that case."
LUCENE-7651, Uwe Schindler,22/Jan/17 17:32,"Fixed for now, please reopen for every bugfix release on any branch that was not yet updated to use this."
LUCENE-7651, Uwe Schindler,01/Feb/17 12:46,"BTW, the release notes of Java 8u121 was updated to mention this change: http://www.oracle.com/technetwork/java/javase/8u121-relnotes-3315208.html
It still breaks our previous releases, but we can tell people that its caused by Oracle, not us."
LUCENE-7651, Uwe Schindler,07/Feb/17 10:43,"I backported this change to Lucene 5.5.4, because otherwise we cannot run the build with Java 8 anymore. The problem is the following: Java 7's Javadocs do not add a ""script.js"" file to the Javadocs output, so this fix injects the javadocs, but because a ""script.js"" is nowhere referenced in the HTML files, prettyprint is not loaded.

We have the following possibilities:

	revert this backport commit, but it prevents smoketester from suceeding (as it checks also Java 8). It also makes it impossible to build the 5.5.4 release with Java 8
	add more hacks to inject a script tag into the head element, but that's really complicated as you have to do it in every HTML file!
	ignore the fact that Javadocs do not code-prettyprint correctly anymore in Java 7. The Javadocs are fine, just the code exaples are no longer syntax highlighted.



I'd go for the third item. Any comments? If we go this route, I will add a comment to the Changelog that prettyprinting Javadocs is no longer working, if docs are build with Java 7."
LUCENE-7651, Uwe Schindler,07/Feb/17 10:55,"We don't have access to recent paid only Java 7 JDKs, but as the Javadocs fix was declared a security issue, I assume that it is also applied to Java 7, so without this fix Java 7 paid updates will fail the build, too."
LUCENE-7652, Adrien Grand,23/Jan/17 13:36,"Can you also read what the value of LRUQueryCache.ramBytesUsed is?

LRUQueryCache has references to current open IndexReaders due to close listeners. I believe you are analysing a heap dump and recursively adding everything that is referenced by your LRUQueryCache while it does not make sense ot take open IndexReaders into account.

The keys of LRUQueryCache.cache are instances of SegmentCoreReaders, and I've checked many of the keys, the only reference to them is LRUQueryCache.cache, given LRUQueryCache.cache is an IdentityHashMap, that means you can't even get to them outside of the cache because you can't get a key that's equivalent to one of these in the cache.

This indicates that you are leaking index readers. When there are no readers that reference a segment anymore, that segment is closed, which triggers the eviction of all associated entries in the query cache. You should review your usage of Lucene to make sure that there is a call to close() for every index reader that you acquire."
LUCENE-7652, Lae,23/Jan/17 19:22,"Hi Adrien, ramBytesUsed is only ~500KB. Thanks for the info regarding leaking index readers, I'll try to track those down."
LUCENE-7652, Adrien Grand,25/Jan/17 15:01,Another possibility is that you are affected by this bug: https://issues.apache.org/jira/browse/LUCENE-7657.
LUCENE-7652, Adrien Grand,25/Jan/17 15:02,Closing now as this is either an IndexReader leak from the application side or a duplicate of LUCENE-7657.
LUCENE-7652, Lae,25/Jan/17 23:21,"I have found our application was indeed leaking, we basically have something like:

Directory dir = FSDirectory.open(path);
DirectoryReader reader = DirectoryReader.open(dir);


reader was closed after used but dir was never closed, therefore causing this leak.

I have not yet verified whether we are impacted by LUCENE-7657."
LUCENE-7657, Adrien Grand,24/Jan/17 09:54,"Here is a patch that makes sure the queries returned by Weight.getQuery() do not reference a TermContext. An alternative way to fix the issue would be to remove the reference from TermContext on the reader context, but it is not straightforward. Opinions welcome."
LUCENE-7657, Adrien Grand,24/Jan/17 10:22,"I was doing more testing and this patch does not work. We only unset the context in createWeight, so wrapper queries like BooleanQuery or ToParentBlockJoinQuery would still have a reference to the index reader if the wrap a TermQuery that has a TermContext. So I guess the only way would be to remove the reference from TermContext to the index readers."
LUCENE-7657, Adrien Grand,24/Jan/17 10:38,Here is a patch that removes the reader context reference from TermContext.
LUCENE-7657, Michael McCandless,24/Jan/17 17:43,"+1

Clever solution to factor out this Object identity; I wonder if we should fix IndexReader.getXXXCacheKey() similarly ... later!"
LUCENE-7662, Mike Drob,07/Feb/17 21:31,Michael McCandless - what do you think of this patch?
LUCENE-7662, Michael McCandless,08/Feb/17 10:52,"Thanks Mike Drob; I think this patch looks good, except it makes some tests angry, e.g.:


   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestLucene62SegmentInfoFormat -Dtests.method=testRandomExceptions -Dtests.seed=F65CD1D4D104665D -Dtests.locale=zh -Dtests.timezone=Asia/Khandyga -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] ERROR   0.03s J3 | TestLucene62SegmentInfoFormat.testRandomExceptions <<<
   [junit4]    > Throwable #1: org.apache.lucene.index.CorruptIndexException: Problem reading index. (resource=a random IOException (_e.cfe))
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([F65CD1D4D104665D:9E73BF104F0A3FFD]:0)
   [junit4]    > 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:142)
   [junit4]    > 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:74)
   [junit4]    > 	at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:143)
   [junit4]    > 	at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:195)
   [junit4]    > 	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:103)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:473)
   [junit4]    > 	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:103)
   [junit4]    > 	at org.apache.lucene.index.BaseIndexFileFormatTestCase.testRandomExceptions(BaseIndexFileFormatTestCase.java:563)
   [junit4]    > 	at org.apache.lucene.index.BaseSegmentInfoFormatTestCase.testRandomExceptions(BaseSegmentInfoFormatTestCase.java:50)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]    > Caused by: java.nio.file.NoSuchFileException: a random IOException (_e.cfe)
   [junit4]    > 	at org.apache.lucene.store.MockDirectoryWrapper.maybeThrowIOExceptionOnOpen(MockDirectoryWrapper.java:575)
   [junit4]    > 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:744)
   [junit4]    > 	at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:119)
   [junit4]    > 	at org.apache.lucene.store.MockDirectoryWrapper.openChecksumInput(MockDirectoryWrapper.java:1072)
   [junit4]    > 	at org.apache.lucene.codecs.lucene50.Lucene50CompoundReader.readEntries(Lucene50CompoundReader.java:105)
   [junit4]    > 	at org.apache.lucene.codecs.lucene50.Lucene50CompoundReader.<init>(Lucene50CompoundReader.java:69)
   [junit4]    > 	at org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat.getCompoundReader(Lucene50CompoundFormat.java:71)
   [junit4]    > 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:99)
   [junit4]    > 	... 44 more



Maybe we just need to relax that base test case to accept the new CorruptIndexExcpeption as well, and look to its cause to check the exception message?

Also, I think it'd be a bit better to use our expectThrows method in the test case, wrapped around the one line where you try to open an index reader, instead of the @Test(expected = ...), which would pass if CorruptIndexException was hit anywhere in that test case?"
LUCENE-7662, Mike Drob,08/Feb/17 20:57,"Those are good suggestions, I'll get them into the next version of this patch.

Looking at the code in MockDirectoryWrapper, some of the ""a random IOException"" stuff looks really hackish, especially where we are checking for string messages to match. I'm uncomfortable with how brittle some of that is. We already have FakeIOException available and I think it would be good to use that instead in several places. Do you think we should handle that here, or I can file a new issue for it."
LUCENE-7662, Mike Drob,08/Feb/17 22:55,Updated patch with some test clean up.
LUCENE-7662, Michael McCandless,09/Feb/17 00:20,"Thanks Mike Drob, the new patch looks great, and +1 to do that test cleanup here.  I'll push soon!"
LUCENE-7662, Michael McCandless,09/Feb/17 01:41,"Hmm something is still angry:


   [junit4] Suite: org.apache.lucene.index.TestMissingIndexFiles
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMissingIndexFiles -Dtests.method=testMissingDoc -Dtests.seed=4D7CBCD6B337257 -Dtests.locale=de-CH -Dtests.timezone=Etc/GMT-10 -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] FAILURE 0.04s J2 | TestMissingIndexFiles.testMissingDoc <<<
   [junit4]    > Throwable #1: junit.framework.AssertionFailedError: Expected exception CorruptIndexException
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([4D7CBCD6B337257:6405AAA9369B3658]:0)
   [junit4]    > 	at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2703)
   [junit4]    > 	at org.apache.lucene.index.TestMissingIndexFiles.testMissingDoc(TestMissingIndexFiles.java:52)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]    > 	Suppressed: java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still 1 open files: {_0.cfs=1}
   [junit4]    > 		at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:841)
   [junit4]    > 		at org.apache.lucene.index.TestMissingIndexFiles.testMissingDoc(TestMissingIndexFiles.java:53)
   [junit4]    > 		... 36 more
   [junit4]    > 	Caused by: java.lang.RuntimeException: unclosed IndexInput: _0.cfs
   [junit4]    > 		at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:732)
   [junit4]    > 		at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:776)
   [junit4]    > 		at org.apache.lucene.codecs.lucene50.Lucene50CompoundReader.<init>(Lucene50CompoundReader.java:78)
   [junit4]    > 		at org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat.getCompoundReader(Lucene50CompoundFormat.java:71)
   [junit4]    > 		at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:99)
   [junit4]    > 		at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:74)
   [junit4]    > 		at org.apache.lucene.index.StandardDirectoryReader$1.doBody(StandardDirectoryReader.java:62)
   [junit4]    > 		at org.apache.lucene.index.StandardDirectoryReader$1.doBody(StandardDirectoryReader.java:54)
   [junit4]    > 		at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:666)
   [junit4]    > 		at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:77)
   [junit4]    > 		at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:63)
   [junit4]    > 		at org.apache.lucene.index.TestMissingIndexFiles.lambda$testMissingDoc$0(TestMissingIndexFiles.java:52)
   [junit4]    > 		at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2694)
   [junit4]    > 		at org.apache.lucene.index.TestMissingIndexFiles.testMissingDoc(TestMissingIndexFiles.java:52)
   [junit4]    > 		... 36 more"
LUCENE-7662, Mike Drob,09/Feb/17 04:46,"Thanks. That is frustrating. I ran it 10 times and somehow never hit that or a similar seed. When the test uses the compound format, since there is no .doc file to remove, the index doesn't get corrupted and correctly never throws the exception. I couldn't figure out how to disable compound format from the test, so instead we can attempt to delete the doc or the .cfe file.

I also made a change to check that we do delete something, otherwise the index would never be corrupt here. Since I can't imagine all possible future index file layouts, this seems prudent."
LUCENE-7662, Michael McCandless,10/Feb/17 22:02,"Thanks Mike Drob!

That is frustrating. I ran it 10 times and somehow never hit that or a similar seed. 

The joys of randomized tests!  I'll review and push soon..."
LUCENE-7662, Michael McCandless,11/Feb/17 01:26,Thank you Mike Drob!
LUCENE-7668, Michael McCandless,30/Jan/17 11:54,Simple patch; I had to also fix CannedTokenStream to set the canned token type for the test case.
LUCENE-7668, Uwe Schindler,30/Jan/17 12:51,"Why don't you capture all attributes with captureState and restore them instead of clearAttributes? After that you can change token text and offsets/position. There is a TODO about this in the source code.

The problem mentioned here also affects payloads or other attributes like flags, keyword, or the japanese ones.

I fixed the same issue in NGramFilters a week ago."
LUCENE-7668, Michael McCandless,30/Jan/17 20:33,"Ahh in fact it's already doing the captureState/restoreState here, and so in fact there is no bug!  I wrote the test first, and it was only failing because CannedTokenStream was failing to carry over the type I had asked for.

Here's a new patch, just adding the test case, and removing dead code from WDGF.

Thanks Uwe Schindler."
LUCENE-7668, Uwe Schindler,30/Jan/17 23:36,"Hah, thanks for bringing to closure!  Thanks for removing the dead TypeAttribute!

Maybe we should fix CannedTokenStream to do what its comment says (captureState/restoreState)."
LUCENE-7668, Michael McCandless,30/Jan/17 23:48,"Maybe we should fix CannedTokenStream to do what its comment says (captureState/restoreState).

I would love to, but I'm not quite sure how to do it.  CannedTokenStream gets an array of Token in ... how can I do a restoreState from a Token?"
LUCENE-7668, Uwe Schindler,30/Jan/17 23:58,"There is one possibility: As CannedTokenStream is a source and owns the attributes, it could use the Token Attributefactory (see the final field on deprecated Token class) in its ctor. Because of this one could use copyTo of Token to copy it into the Token behind all attributes.

But as CannedTokenStream only supports the attributes of Token, we could also just ensure all of those are copied. So look which attributes are implemented and use those!

Not sure what's the better idea."
LUCENE-7668, Michael McCandless,31/Jan/17 11:27,"OK I tried your first idea Uwe Schindler, using Token's attribute factory for CannedTokenStream and then casting the offset attribute to Token and using copyTo.  It seems to work, as crazy as it looks!"
LUCENE-7668, Uwe Schindler,31/Jan/17 13:05,"Looks good! Thanks for adding the comment about the crazyness - I see no better way to do this (without casting)! It is also good that you left the clearAttributes(), because without it, the ""extra"" attributes maybe added by TokenFilters, would not be cleared and then the Asserting* tests would fail."
LUCENE-7668, Michael McCandless,31/Jan/17 17:01,Thank you Uwe Schindler!
LUCENE-7670, Steve Rowe,31/Jan/17 06:05,"Patch that uses the SearcherManager(Directory,...) ctor instead of the SM(IndexWriter,...) ctor in the case of opening a suggester over an already built index.

Committing shortly."
LUCENE-7674, Adrien Grand,02/Feb/17 22:22,This particular error means that there is a problem in the way your index is structured since you had at least one segment that did not have a parent doc as a last document. This is wrong because block joins work on blocks of documents that contain 0-n children followed by one parent so the last document is necessarily a parent document.
LUCENE-7674, Tim Underwood,02/Feb/17 22:42,"Thanks Adrien Grand!  I'm trying to figure out if this an issue on my side (very possible) or if it's a Solr or Lucene issue.

All my indexing goes through Solr (via SolrJ) and as far as I can tell I'm not attempting to index any child documents without a corresponding parent document.  I'm not even sure if Solr or SolrJ would allow me to do that.

Does it make sense that optimizing the index would cause the problem to go away?

I think I was able to snag a copy of the index that was causing problems before the optimized version was able to replicate.  Any suggestions/pointers for trying to track down whatever docs are problematic?  Will running CheckIndex on it tell me anything useful?"
LUCENE-7674, Mikhail Khludnev,03/Feb/17 07:11,"Tim Underwood, it usually happens when uniqueKey is duplicated, it causes deleting former parent doc. 
It can be verified with org.apache.lucene.search.join.CheckJoinIndex, although it doesn't have main() method. 

Adrien Grand, what if will invoke CheckJoinIndex logic lazily somewhere in org.apache.lucene.search.join.QueryBitSetProducer.getBitSet(LeafReaderContext)? It won't cost much as well it should be lazy, but provides more predictable behaviour for users."
LUCENE-7674, Tim Underwood,03/Feb/17 18:13,"Thanks Mikhail Khludnev!  Running CheckJoinIndex on my bad index (assuming I got my parentsFilter right) says:


java.lang.IllegalStateException: Parent doc 3324040 of segment _vfo(6.3.0):C28035360/10475131:delGen=86 is deleted but has a live child document 3323449



Running CheckJoinIndex on the optimized version of the index doesn't complain.

So... that leaves me wondering where the bug is.  I am frequently (via Solr) re-indexing parent/child documents that duplicate existing documents based on my unique key field but my understanding is that Solr should automatically delete the old parent and child documents for me.  Maybe thats a bad assumption.

It looks like maybe I'm running into one or more of these issues: SOLR-5211, SOLR-5772, SOLR-6096, SOLR-6596, SOLR-6700

Sounds like I should probably just make sure I explicitly delete any old parent/child documents that I'm replacing to be on the safe side."
LUCENE-7674, Tim Underwood,03/Feb/17 18:49,I also noticed that I have some deleteByQuery calls that target parents documents but not their children (my assumption being that Solr or Lucene would also delete the corresponding child documents).  Perhaps that is what is causing the orphan child documents.  I'll be sure to explicitly delete those also.
LUCENE-7674, Mikhail Khludnev,03/Feb/17 21:06,"LUCENE-7674.patch introduces CheckingQueryBitSetProducer which checks parent segment's bitset before caching and switches {!parent} {!child} to use it. It laid well, beside of, and it's interesting! BJQParserTest.testGrandChildren(). When we have three levels: parent, child, grand-child and searching for children (2nd level), it requires to include all ascendant levels (parent) in bitset. This, will break existing queries for those who run more than two level blocks. But such explicitly strict behavior solves problems for those who tires to retrieve intermediate levels by [child] then, I remember a couple of such threads in the list. 
What do you think?"
LUCENE-7674, Mikhail Khludnev,03/Feb/17 21:09,"Tim Underwood, you've got everything right! Thanks for gathering those pet peeves in the list. Here is one more, SOLR-7606 - it's my favorite ones. I need to tackle them sooner or later."
LUCENE-7674, Mikhail Khludnev,07/Feb/17 08:01,"Adrien Grand , Uwe Schindler, what's your opinion about CheckingQueryBitsetProducer and restricting multilevel blocks?"
LUCENE-7674, Adrien Grand,07/Feb/17 10:21,"It feels wrong to me that we enforce these rules at search time, while they should be enforced at index time. I think the true fix to all these block join issues would be to make Solr know queries that describe the parent and child spaces rather than expect users to provide them at search time. Then once it knows that, it could reject update/delete operations that would break the block structure, fail queries that use a parent query that is not one of the expected ones, maybe add a FILTER clause to the child query to restrict it to the child space in case some fields are used at multiple levels, etc."
LUCENE-7674, Uwe Schindler,07/Feb/17 10:29,"I agree with Adrien. The current block join support in Solr is a desaster, because it was released to early. Just nuke the broken APIs and create a new one, so Solr internally knows from schema/mapping how to block join and also prevent misformed updates. This is also worth a backwards compatibility break! Doing expensive runtime checks on every query just to keep a broken API/implementation is not a good idea. Break hard and come with a better API, the users will still be more happy, trust me. I know so many users who f*ck up the block joins, as Solr does not enforce it correctly. Do the following:

	remove Solr ID fields from child documents (why do we have them? This also makes updates to child documents impossible)
	always hide child documents on ""normal"" queries and return them only with the parent document (like Elasticsearch does)
	automatically add block join queries if fields of the child documents are part of the query
	add some extra queries to specifically search on childs and return childs only (hiding parents, of course)
	if somebody updates a parent document, delete also all childs and create a new block
	hide the block join filter. Solr should have an internal marker field to support block join, which is never exposed"
LUCENE-7674, Mikhail Khludnev,07/Feb/17 11:09,"Oh.. I've got your point, guys. Thanks. I'd probably raise gsoc ticket and try to scratch backlog."
LUCENE-7674, David Smiley,07/Feb/17 18:23,+1 to Adrien Uwe's remarks.  It was released too early.
LUCENE-7674, Mikhail Khludnev,16/Feb/17 22:02,"Ok. I started to scratch the spec at SOLR-10144. Everybody are welcome. Meanwhile, I tried to reproduce this exact failure to come up with more informative message. But it seems like it's impossible - recently redesigned BlockJoinQuery ignores children behind the last parent in segment."
LUCENE-7676, Robert Muir,06/Feb/17 14:39,"looks good.

i think the @Test can be dropped from the test case."
LUCENE-7676, Adrien Grand,06/Feb/17 14:45,+1 too
LUCENE-7676, Christine Poerschke,08/Feb/17 19:01,Thank you both for the reviews.
LUCENE-7679, Alan Woodward,07/Feb/17 10:37,"Here is a patch that re-organises how MemoryIndex builds its internal field Info structures.  If an IndexableField is passed to addField(), we re-use as many of its FieldType settings as possible.

I needed to make FieldInfo.setDocValuesType() public, but that brings it into line with .setPointDimensions() so I don't think it's too bad a change?"
LUCENE-7679, Alan Woodward,09/Feb/17 11:19,Thanks Martijn
LUCENE-7682, Michael Braun,09/Feb/17 18:59,"I think I know why some of this is going on - in NearSpansOrdered stretchToOrder handles figuring out the effective position length it needs to search over and advances each spans to the relevant distance for a match. The second span is advanced just enough so the first instance of 'feed' matches (which satisfies the query), and matchEnd is set to that ""feed"" occurrence's end position (and matchWidth updated as well), and it stops after that, so NearSpansOrdered effectively does not see that last occurrence of feed when twoPhaseCurrentDocMatches() is called (from getTermToSpans in PhraseHelper).  This first end position of the first ""feed"" occurrence is what's used instead of the last end position within the slop."
LUCENE-7682, David Smiley,25/Feb/17 15:58,Are you saying then that there seems to be a bug in NearSpansOrdered (and not any highlighter)?  Presumably the original Highlighter WSTE would be affected as well?  Can we test this?  If we can get to the bottom of this ASAP then we have a chance of getting a fix into v6.4.2.
LUCENE-7682, Paul Elschot,26/Feb/17 12:08,"For queries requiring t1 near t2 with enough slop, t1 t1 t2 matches twice, but t1 t2 t2 matches only once. This behaviour was introduced with the lazy iteration, see:
https://issues.apache.org/jira/browse/LUCENE-6537?focusedCommentId=14579537&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14579537

This is also a problem for LUCENE-7580 where matching term occurrences are scored: there the second occurrence of t2 will not influence the score because it is never reported as a match.

LUCENE-7398 is probably also of interest here.

To improve highlighting and scoring, we will probably have to rethink how matches of span queries are reported.
One way could be to report all occurrences in the matching window, and forward all the sub-spans to after the matching window.
Would that be feasible?"
LUCENE-7682, Michael Braun,01/Mar/17 21:34,Should this be marked as a bug for a module other than the highlighter then since it also affects scoring?
LUCENE-7682, David Smiley,02/Mar/17 02:21,"Michael, don't worry about how you filed it; this happens often – report the bug and then dig deeper and see it's indirectly caused by something else.  Here, it's not clear is LUCENE-7398 or LUCENE-7580 will fix it; I've been following these issues; we'll see.   Paul Elschot I'm glad you noticed this issue; this is in your area of interest"
LUCENE-7683, Adrien Grand,08/Feb/17 19:02,I think it would make sense to make Scorer.getWeight final and FilterScorer delegate getChildren.
LUCENE-7685, Adrien Grand,10/Feb/17 11:04,Here is a patch.
LUCENE-7685, Michael McCandless,10/Feb/17 11:16,"+1

What on earth did this comment mean?


This makes rewritten query equal the original, so that user does not have to .rewrite() their query before searching:



Why would a user have to (in the past) manually rewrite their query before searching ..."
LUCENE-7687, Alexandre Rafalovitch,10/Feb/17 15:23,This reads to me as the underlying issue of whether ComplexPhrase query parser uses underlying (automatically-constructed) MultiTerm analysis chain (and takes into account multiterm-ready ascii folding filter) or does its own expansion using pure tokens.
LUCENE-7690, Michael McCandless,11/Feb/17 16:27,I'll dig; looks related to LUCENE-7662.
LUCENE-7690, Michael McCandless,11/Feb/17 18:57,Thanks Steve Rowe!
LUCENE-7695, Alexandre Rafalovitch,15/Feb/17 14:48,"This would have been better discussed on the mailing list first, I feel.

I suspect what might be happening here is that one of the terms is hitting synonym expansion and perhaps that is not supported. This is strengthened by the fact that the words in the exception do not match the word you gave triggering it.

So, I would check the type definition, synonym file it uses and the synonyms in there. If I am right, the bigger question then is whether ComplexPhraseQueryParser is expected to support synonyms. If yes, then that would be the actual issue here."
LUCENE-7695, Mikhail Khludnev,15/Feb/17 14:52,"CPQP transforms only certain queries to spans. So, the failure is obvious and patches are welcome."
LUCENE-7695, Markus Jelsma,15/Feb/17 15:05,"Hello Alexandre Rafalovitch, 

The terms i used in the examples do not have synonyms defined, actually, the synonym file is so far still empty. About the words not matching, you are right, i copy/pasted another exception, i was looking for words and word combinations that do and do not cause trouble. Apologies for the confusion.

Thanks,
Markus"
LUCENE-7695, Markus Jelsma,15/Feb/17 15:30,"I cannot seem to import stuff from Lucene's analysis module into a unit test that's in Lucene's queryparser module.

E.g. 

import org.apache.lucene.analysis.synonym.SynonymFilter;
import org.apache.lucene.analysis.synonym.SynonymMap;



doesn't work in org.apache.lucene.queryparser.complexPhrase.TestComplexPhraseQuery. Any ideas on how to test it?"
LUCENE-7695, Mikhail Khludnev,15/Feb/17 15:40,you can try to approach org.apache.lucene.analysis.MockSynonymAnalyzer in TestComplexPhraseQuery
LUCENE-7695, Markus Jelsma,15/Feb/17 16:30,"Patch for master. I cannot get the unit tests to run (ant keeps hanging here) so i applied a crude fix and tested it via Solr and it works.

When processing the SynonymQuery i actually have no idea what it should do with more than 1 term, i think it should rewrite itself again but i am really not sure.

Or should it create SpanTermQuery for each term and wrap those in a SpanOrQuery and add that to the list of allSpanClauses?"
LUCENE-7695, Markus Jelsma,15/Feb/17 16:56,"Here's a patch where each Term in the SynonymQuery is wrapped as SpanTermQuery in a SpanOrQuery, which is then added to the allSpanClauses array.

If there is just one term in the SynonymQuery, it is added as a SpanTermQuery directly.

This seems more appropriate, but don't take my word for it."
LUCENE-7695, Markus Jelsma,15/Feb/17 17:49,"It seems the top level query can also be a SynonymQuery, at least via Solr. Updated patch to take care of that as well but it seem i broke something as well. It is now no longer possible to embed FuzzyQuery:


{!complexphrase}content_nl:""vergunningen~""



Won't work anymore. But working with multiple terms on the same position does work now, e.g. KeepWordFilter with stemmed terms. I need to go, but will take a peek later."
LUCENE-7695, Markus Jelsma,16/Feb/17 11:15,"New patch, all SynonymQuery's are turned into a SpanOrQuery now and it works, as it seems.

The next query works fine:

{!complexphrase}content_nl:""(emissi* OR investerin*)""~30



But this one doesn't:

{!complexphrase}content_nl:""emissi*""



Prefix or fuzzy queries both return a:

null:java.lang.IllegalArgumentException: Unknown query type ""org.apache.luc
ene.search.PrefixQuery"" found in phrase query string ""emissi*""
        at org.apache.lucene.queryparser.complexPhrase.ComplexPhraseQueryParser$ComplexPhraseQuery.rewrite(ComplexPhraseQueryParser.java:289)



Haven't got a clue yet why this doesn't work, but have it wrapped in a boolean query does."
LUCENE-7695, Mikhail Khludnev,03/Mar/17 22:05,what about LUCENE-7695.patch ?
LUCENE-7695, Markus Jelsma,06/Mar/17 14:53,"Hello Mikhail Khludnev, your patch works nicely!"
LUCENE-7695, Markus Jelsma,08/Mar/17 10:00,"Removed fix/version 6.4.2.
Thanks Mikhail!"
LUCENE-7698, Ere Maijala,17/Feb/17 09:07,This seems to be a regression in Solr 6.4.0. At least a quick test shows correct results in 6.3.0.
LUCENE-7698, Ere Maijala,17/Feb/17 09:59,Looks to me like LUCENE-7603 broke this.
LUCENE-7698, Michael McCandless,17/Feb/17 10:34,"Hmm, no good, sorry about this ... thank you for reporting this Ere Maijala; I'll try to make a Lucene test case showing this."
LUCENE-7698, Michael McCandless,17/Feb/17 12:30,"OK I see what's happening: this filter (CommonGramsQueryFilter) deletes the unigram tokens, but keeps posLength=2 on the bigram tokens, which makes a disconnected graph, and then the query parser does the wrong thing.

I think the right fix is for it to set posLength to 1 when it drops unigram tokens .. I'll work on a patch."
LUCENE-7698, Michael McCandless,17/Feb/17 16:03,"OK here's a patch fixing CommonGraphsQueryFilter to not create a disconnected graph.  Ere Maijala could you please try this and see if it fixes your use case?  Thanks.

I also added an experimental option to QueryBuilder (base class for query parsers) to disable graph handling, as a safety for other tokenizer components that may create disconnected graphs."
LUCENE-7698, Ere Maijala,20/Feb/17 08:35,"Michael McCandless, thanks for the fix. An initial check indicates that the patch fixes my use case. I ran the tests in branch_6x. The patch didn't quite apply cleanly to branch_6_4 and after applying manually a test didn't compile:


common.compile-test:
    [mkdir] Created dir: /Users/eremaijala/src/solr/lucene/build/analysis/common/classes/test
    [javac] Compiling 279 source files to /Users/eremaijala/src/solr/lucene/build/analysis/common/classes/test
    [javac] /Users/eremaijala/src/solr/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsQueryFilterFactory.java:103: error: cannot find symbol
    [javac]     assertGraphStrings(stream, ""testing_the the_factory factory works"");
    [javac]     ^
    [javac]   symbol:   method assertGraphStrings(TokenStream,String)
    [javac]   location: class TestCommonGramsQueryFilterFactory
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error"
LUCENE-7698, Michael McCandless,22/Feb/17 09:58,OK thanks for confirming Ere Maijala; I'll fix that test on back port.
LUCENE-7698, Michael McCandless,22/Feb/17 10:55,Thank you Ere Maijala!
LUCENE-7704, Michael McCandless,23/Feb/17 12:18,"Hi Sebastian Yonekura Baeza, actually, this is by design: it is up to you to downcase the rules you add to the SynonymMap.Builder, and then that ignoreCase option will ignore the case of the incoming tokens during analysis.

I'm sorry the javadocs were missing (so you would not have known this is by design!!), so I've copied over the javadocs from the old SynonymFilter, and I've fixed your test case to down-case the rules, and now it's passing, in the attached patch."
LUCENE-7704, Michael McCandless,23/Feb/17 14:05,"OK I improved the javadocs explaining ignoreCase, and folded in your test case (thank you Sebastian Yonekura Baeza!) here: https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;a=commitdiff;h=3ad6e41910158a46025ff78330d78a31a7081887"
LUCENE-7704, Sebastian Yonekura Baeza,23/Feb/17 14:15,"Oh, sorry I missed those docs, given that it was a deprecated class I didn't pay much attention to it. 

Indeed, without the javadocs the parameter ignoreCase was kind of misleading. Thank you Michael McCandless for the clarification!"
LUCENE-7708, Jim Ferenczi,24/Feb/17 12:56,"The CJKBigramFilter is working correctly because it sets the position length attribute only if outputUnigrams is set.
So only the ShingleFilter is problematic since outputUnigrams is not check when position length is set. 
So for instance with shingles of size 2, the input ""foo bar baz"" would create two tokens  ""foo bar"" and ""bar baz"" with a pos len of 2 and an position increment in between which forms a disconnected graph.
I'll work on a patch shortly."
LUCENE-7708, Jim Ferenczi,24/Feb/17 14:11,"Here is one patch for the ShingleFilter.
When outputUnigrams is set to false, position length for a shingle of size N is the number of position created by shingles of smaller size: (N - minShingleSize) + 1.
Michael McCandless can you take a look ?"
LUCENE-7708, Steve Rowe,24/Feb/17 17:40,"+1 to the idea, but some tests are failing with the patch:


   [junit4] Tests with failures [seed: 4D8AED66905F8617]:
   [junit4]   - org.apache.lucene.analysis.shingle.ShingleFilterTest.testOutputUnigramsIfNoShinglesSingleTokenCase
   [junit4]   - org.apache.lucene.analysis.shingle.ShingleFilterTest.testOutputUnigramsIfNoShinglesWithMultipleInputTokens
   [junit4]   - org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapperTest.testOutputUnigramsIfNoShinglesSingleToken
   [junit4]   - org.apache.lucene.analysis.shingle.TestShingleFilterFactory.testOutputUnigramsIfNoShingles"
LUCENE-7708, Jim Ferenczi,24/Feb/17 18:42,"Thanks Steve !
I pushed a new patch that solves the tests failures."
LUCENE-7708, Steve Rowe,24/Feb/17 19:17,"I'm beasting 1000 iterations of TestRandomChains with the patch, and run 110 found the following reproducing seed - maybe it's ShingleFilter's fault?  (I didn't investigate further):

edit: this seed fails on unpatched master, so the patch on this issue isn't to blame.  I created a separate issue: LUCENE-7711


  [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains
   [junit4]   2> TEST FAIL: useCharFilter=false text='\ufac4\u0552H \ua954\ua944 \ud0d2\uaddd\ub6cb\uc388\uc344\uca88\ud224\uc462\uaf42 g '
   [junit4]   2> Exception from random analyzer: 
   [junit4]   2> charfilters=
   [junit4]   2>   org.apache.lucene.analysis.charfilter.HTMLStripCharFilter(java.io.StringReader@3fb9d00e, [<HOST>, <HANGUL>, <IDEOGRAPHIC>, <SOUTHEAST_ASIAN>])
   [junit4]   2> tokenizer=
   [junit4]   2>   org.apache.lucene.analysis.standard.StandardTokenizer(org.apache.lucene.util.AttributeFactory$1@c893af9b)
   [junit4]   2> filters=
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.KeywordRepeatFilter(ValidatingTokenFilter@7e1e9fe2 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.cjk.CJKBigramFilter(ValidatingTokenFilter@12c3fb1b term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(ValidatingTokenFilter@31c463b5 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false, 49)
   [junit4]   2>   org.apache.lucene.analysis.in.IndicNormalizationFilter(ValidatingTokenFilter@3f72787 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false)
   [junit4]   2> offsetsAreCorrect=false
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=E532502212098AC7 -Dtests.slow=true -Dtests.locale=ko-KR -Dtests.timezone=Atlantic/Jan_Mayen -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] ERROR   0.76s | TestRandomChains.testRandomChains <<<
   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset; got startOffset=10,endOffset=9
   [junit4]    >        at __randomizedtesting.SeedInfo.seed([E532502212098AC7:D8D37943551B9707]:0)
   [junit4]    >        at org.apache.lucene.analysis.tokenattributes.PackedTokenAttributeImpl.setOffset(PackedTokenAttributeImpl.java:110)
   [junit4]    >        at org.apache.lucene.analysis.shingle.ShingleFilter.incrementToken(ShingleFilter.java:345)
   [junit4]    >        at org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:67)
   [junit4]    >        at org.apache.lucene.analysis.in.IndicNormalizationFilter.incrementToken(IndicNormalizationFilter.java:40)
   [junit4]    >        at org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:67)
   [junit4]    >        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:731)
   [junit4]    >        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:642)
   [junit4]    >        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:540)
   [junit4]    >        at org.apache.lucene.analysis.core.TestRandomChains.testRandomChains(TestRandomChains.java:853)
   [junit4]    >        at java.lang.Thread.run(Thread.java:745)
   [junit4] OK      1.64s | TestRandomChains.testRandomChainsWithLargeStrings
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {dummy=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{}, maxPointsInLeafNode=542, maxMBSortInHeap=7.773738401752009, sim=RandomSimilarity(queryNorm=false): {}, locale=ko-KR, timezone=Atlantic/Jan_Mayen
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=400845920,total=514850816
   [junit4]   2> NOTE: All tests run in this JVM: [TestRandomChains]
   [junit4] Completed [1/1 (1!)] in 6.03s, 2 tests, 1 error <<< FAILURES!"
LUCENE-7708, Michael McCandless,24/Feb/17 21:12,"+1, thanks Jim Ferenczi!"
LUCENE-7708, Steve Rowe,24/Feb/17 21:13,"+1, LGTM, all lucene/analysis/common/ tests pass for me with the latest patch.

Also, 1000 beasting iterations of TestRandomChains didn't trigger any failures with this patch (other than the unrelated one at LUCENE-7711)."
LUCENE-7708, Jim Ferenczi,24/Feb/17 22:48,Thanks Steve Rowe and Michael McCandless !
LUCENE-7708, Shawn Heisey,16/Mar/17 14:20,"There's no fix version here.  CHANGES.txt says it's in 6.5.0.

(looking for possible causes of a shingle filter problem confirmed in Solr 6.3 and 6.4, this couldn't be the cause)"
LUCENE-7708, Jim Ferenczi,16/Mar/17 14:46,"Shawn Heisey one shingle filter problem is fixed in LUCENE-7708 and appears in 6.3 when the support for graph analysis has been added to the QueryBuilder. 
The other shingle filter problem I can think of is when the number of paths is gigantic and produces an OOM. I opened LUCENE-7747 to fix this.
Although I think that the workaround for now is to be disable graph query analysis when the analyzer contains a shingle filter that produces shingles of different size. The graph analysis in this case builds all possible path since each position has different side paths."
LUCENE-7708, David Smiley,16/Mar/17 15:52,"Jim Ferenczi what we do after committing/all-done is ""Resolve"" the issue (not ""Close"").  That dialog box will give you the option to set the fix-version.  Later on during the release process, there should be a JIRA step that involves bulk-closing all issues resolved for this version."
LUCENE-7708, Jim Ferenczi,16/Mar/17 17:50,Thanks David Smiley. I updated the status.
LUCENE-7708, Shawn Heisey,16/Mar/17 20:52,"Looks like 6.5.0 isn't a valid version yet.  Easy enough to add, but if I do so, would I be doing the right thing?"
LUCENE-7708, Steve Rowe,17/Mar/17 15:39,"Looks like 6.5.0 isn't a valid version yet. Easy enough to add, but if I do so, would I be doing the right thing?

I see Jim already set the version to 6.5, but FYI Shawn Heisey, historically people have excluded the trailing "".0"" in minor release labels here on JIRA."
LUCENE-7711, Steve Rowe,10/Jul/17 17:16,"Another reproducing failure on branch_7_0, from my Jenkins:


Checking out Revision 6cdc0060e5c3b93f0764d7e8e441fa21931fe60d (refs/remotes/origin/branch_7_0)
[...]
   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains
   [junit4]   2> TEST FAIL: useCharFilter=true text='arr]] [[ja:\u30de\u30b7\u30e5\u30fc\u30fb\u30d1\u30fc\u30eb '
   [junit4]   2> Exception from random analyzer: 
   [junit4]   2> charfilters=
   [junit4]   2> tokenizer=
   [junit4]   2>   org.apache.lucene.analysis.standard.StandardTokenizer()
   [junit4]   2> filters=
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.KeywordRepeatFilter(ValidatingTokenFilter@218bc4e4 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.core.DecimalDigitFilter(ValidatingTokenFilter@48b9b511 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.HyphenatedWordsFilter(ValidatingTokenFilter@6800dfe4 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.cjk.CJKBigramFilter(ValidatingTokenFilter@507e81ab term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false)
   [junit4]   2> offsetsAreCorrect=false
   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=2577EBB6844BD489 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=en-ZA -Dtests.timezone=America/Argentina/Jujuy -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] ERROR   18.9s J2 | TestRandomChains.testRandomChains <<<
   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=11,endOffset=13,lastStartOffset=13 for field 'dummy'
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([2577EBB6844BD489:1896C2D7C359C949]:0)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:769)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:431)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:393)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:239)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:481)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1717)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1462)
   [junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:171)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:650)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:540)
   [junit4]    > 	at org.apache.lucene.analysis.core.TestRandomChains.testRandomChains(TestRandomChains.java:856)"
LUCENE-7715, Adrien Grand,28/Feb/17 09:37,"I  haven't tried the patch, but I don't see how it deals with the initial state that all sub spans have a start position of -1. In master currently, nextStartPosition iterates over the pq until the start position of the top element is greater than -1, but I don't see that logic in your patch. There is a startDocument method but if I am not mistaken it is only called from two-phase iteration, so if NearSpansUnordered are consumed directly, that would work, but not if they are nested within other spans? (I'm not very familiar with spans so I could easily miss something.)"
LUCENE-7715, Paul Elschot,28/Feb/17 19:16,"... how it deals with the initial state that all sub spans have a start position of -1.

There is no need for that, the intermediate data structure is a priority queue that is not a Spans itself.

If the names of this priority queue (SpanTotalLengthEndPositionWindow) and its methods (startDocument/nextPosition) are misleading, they need to be improved.

The core search tests and precommit pass."
LUCENE-7715, Adrien Grand,02/Mar/17 10:17,"OK, I just applied the patch to understand how it works. It looks good to me, I'll merge it soon. Thanks Paul!"
LUCENE-7715, Paul Elschot,02/Mar/17 20:30,Thanks Adrien.
LUCENE-7717, Dmitry Malinin,28/Feb/17 13:11,"I think that in org.apache.lucene.search.uhighlight.MultiTermHighlighting.extractAutomata()
condition ""if (query instanceof AutomatonQuery)"" should be the last in ""if"" chain"
LUCENE-7717, Christine Poerschke,28/Feb/17 17:06,Hello Dmitry. - I am attaching potential test case adapted from your code snippet (no pun intended) in the description. The test passes locally for me though. Could you perhaps try running it locally too and adapt/adjust it and with/without the MultiTermHighlighting change you mention? Thanks. - Christine
LUCENE-7717, David Smiley,28/Feb/17 17:09,"At some point after MultiTermHighlighting.java was first written, PrefixQuery was altered to be a subclass of AutomatonQuery.  So PrefixQuery detection could simply be removed now, I think, since it's handled via AutomatonQuery condition.

I'm working on debugging to see why this fails & a proper test.  (the test would go in TestUnifiedHighlighterMTQ by the way)"
LUCENE-7717, David Smiley,28/Feb/17 20:22,"Here's my take on it:  The UnifiedHighlighter (and PostingsHighlighter from which it derives) processes the MultiTermQueries (e.g. wildcards) in the query and creates multiple CharacterRunAutomaton intended to match the same things.  CharacterRunAutomaton takes a Automaton as input, and when it does it's processing, it matches the Character code points (integers from 0 to 0x10FFFF) against the integers in the Automaton.  However, this strategy assumes that the Automaton was constructed based on character code points.  But AutomatonQuery.getAutomaton is intended to match byte by byte (integers 0 to 255).  PrefixQuery.toAutomaton will get 2 bytes for the the ""я"" in BytesRef form, and add 2 states.  This does not line up with the assumptions of CharacterRunAutomaton.

A short term immediate ""fix"" is simply to put AutomatonQuery last in the if-else list as Dmitry indicated.  As such, PrefixQuery will work again.  This was broken by LUCENE-6367 (Lucene 5.1).  TermRangeQuery, which also now extends AutomatonQuery, will likewise work – broken by LUCENE-5879 (Lucene 5.2).  Again, back when MultiTermHighlighting was first written, neither of those queries extended AutomatonQuery.  But there will be bugs for other types of AutomatonQuery (namely WildcardQuery and RegexpQuery) that have yet to be reported.

Robert Muir or Michael McCandless I wonder if you have any thoughts on how to fix this.  An idea I have is to not use a CharacterRunAutomaton in the UnifiedHighlighter; use a ByteRunAutomaton instead.  Then, add ByteRunAutomaton.run(char[] ...etc) that converts each character to the equivalent UTF8 bytes to match.  Even with that, I wonder if this points to areas to improve the automata API so that people don't bump into this trap in the future.  For example, maybe have the Automata self-report if it's byte oriented, Unicode codepoint oriented, or something custom.  Then, RunAutomaton could throw an exception if there is a mis-match.  However that would be a runtime error; maybe the Automata could be typed.

Any way, what I'd like to do is do a short term fix that addresses many common cases and the title of this issue.  And then do a more thorough fix in a follow-on issue.  Ishan Chattopadhyaya do you think this could go into 6.4.2 or are you only looking for ""critical"" issues?  It's debatable what's critical and not.  This bug has been around since 5.1 so perhaps it isn't.

(a patch will follow shortly)"
LUCENE-7717, David Smiley,28/Feb/17 20:41,"Here's a patch. It fixed the MultiTermHighlighting class in both the postingshighlight package as well as uhighlight.  It adds a test method to TestUnifiedHighlighterMTQ.  I also beefed up the test for a related method testWhichMTQMatched to avoid potential inadvertent changes to the CharRunAutomata toString that people might depend on.  It appears there was no breakage in this case but until I added more query types, wether it did or didn't break wasn't apparent."
LUCENE-7717, David Smiley,28/Feb/17 20:54,IntelliJ IDEA has clued me into this else-if having dead code paths for a long while now here and I'm kicking myself for putting it off – LOL.
LUCENE-7717, Ishan Chattopadhyaya,28/Feb/17 22:02,"David Smiley, I don't mind including this fix, if you think this is a low risk fix and should be included. Feel free to backport this one to the release branch. I'm anyway waiting for SOLR-10215 as of now."
LUCENE-7717, David Smiley,01/Mar/17 06:51,Closing.  I'll create a linked follow-up bug issue for WildcardQuery (also applies to Regexp) where we can discuss how to deal with that – the more overall fix.  I don't think that one should hold up a 6.4.2.  It'll likely result in removing the PrefixQuery and TermRangeQuery sections in MultiTermHighlighting.
LUCENE-7717, Dmitry Malinin,02/Mar/17 07:38,"Hello Christine
I saw your test. I think that 

	""I"" is not suitable data for test, because StandardAnalyzer converts data to lower case (but PrefixQuery doesn't)
	why ""random""? Сould be better make text array String[] texts = 
{""i"", ""я"", }
 and run for each?
	and I mean that result should be, for example, ""<b>q</b>"" for data ""q"" and query ""q*"" and so on for multi-byte data (assertEquals(""<b>""text""</b>"", snippetString)
Best regards"
LUCENE-7718, Steve Rowe,05/Mar/17 21:45,"Ishan, can you commit this change to master and branch_6x?"
LUCENE-7719, Michael McCandless,01/Mar/17 12:43,"Wow, this is a great catch Dmitry Malinin!  Thank you for opening the precursor issue.

AutomatonQuery.getAutomaton really must return a UTF8-oriented
automaton because that matches how the terms are indexed into Lucene,
and what the automaton will be intersected with, to run the query.

We should fix the javadocs to say this.

And it is sort of annoying that these differences are not strongly
typed, but the Automaton class is really agnostic to what ints you are
putting onto its transitions.

But, yeah, for highlighting, we are operating in UTF16 space, and so I
think we need some way to have the CharacterRunAutomaton interface
on top of a UTF8 automaton?  Maybe we should abstract out a separate
interface that MultiTermHighlighting would use?  It seems it only
uses the run method, to test if a given term is accepted?  And
then, as you suggested, we could easily convert the incoming char[] to
UTF8 BytesRef and use the ByteRunAutomaton.run on that."
LUCENE-7719, David Smiley,05/Apr/17 17:57,"Here is a patch for consideration.

First of all, I realized that this bug is very minor because only some AutomatonQuery.getAutomaton are binary (I thought they all were); others are char based.  The ones that are binary in Lucene are PrefixQuery and TermRangeQuery, both of which are special cased in MultiTermHighlighting.  So practically speaking I think the only users to see this bug would be anyone building a custom AutomatonQuery.  Nonetheless this should be cleaned up and actually tested.

The patch:

	Adds a fairly thorough test with lots of randomized unicode, testing PrefixQuery, TermRangeQuery, WildcardQuery, and FuzzyQuery
	removes the special casing of AutomatonQuery subclasses in MultiTermHighlighting.  AQ is handled generically now.
	added AutomatonQuery.isAutomatonBinary() with a new field to match.
	if the AQ.getAutomaton is not binary, we follow a simple/obvious path
	if the automaton is binary, then I produce a CharRunAutomaton implementing run() to navigate the Automaton char by char.  It makes a BytesRunAutomaton on the automaton.  I inlined the one and two byte UTF char logic from UnicodeUtil.UTF16toUTF8.  If the char needs more bytes, then I call out to UTF16toUTF8 and work off the generated byte array for the remaining chars.  I think this is more maintainable, albeit slower, than reproducing the logic.



I added a perf TODO in MultiTermHighlighting.java to have CompiledAutomaton expose the ByteRunAutomaton so that we don't need to rebuild it here.  The construction cost seems less than trivial as it determinizes the automaton and does other work.  Is this a big deal?  It seems kinda sorta; it depends."
LUCENE-7719, David Smiley,29/Apr/17 12:28,"Michael McCandless What do you think of the patch?  In particular, I wonder what you think of:

	AutomatonQuery.isAutomatonBinary().  This is a very simple/innocent addition.  It's a shame Automaton.isBinary (or something similar) doesn't exist.
	See my TODO last paragraph above.  Also note even if that were done, the CompiledAutomaton isn't exposed by AutomatonQuery any way; so we'd need an accessor.  Perhaps alternatively AutomatonQuery might expose both a CharRunAutomaton and ByteRunAutomaton (i.e. move some of the code in this patch to there)?  If that wouldn't potentially be useful to other users then nevermind.
	The approach to convert chars to bytes at each step"
LUCENE-7719, David Smiley,14/Jun/17 02:59,"Ping Michael McCandless since you've been involved with AutomatonQuery and automata in general. If you're too busy then I think the change to AutomatonQuery is innocent enough so I'm comfortable committing the patch as-is.

I'm not sure if this will make 7.0 or not but I don't think it matters – no back-compat issue / API issue."
LUCENE-7724, Steve Rowe,01/Mar/17 17:46,"Beasting with the repro line (after s/test/test-nocompile/ [1]) on the tip of branch_6x failed 3/150 iterations on my box -> 2% failure rate.

[1] ant compile-test ; (for a in {1..500} ; do ant test-nocompile <remainder of repro line> ; done) 2>&1 | tee ~/output.txt"
LUCENE-7726, Uwe Schindler,01/Mar/17 22:22,"...which does in fact seem to be invalid HTML ... aren't & always suppose to be encoded as & ... even in URLs?

That's the problem. Easy to fix. Just commit a fix or open issue about it. I noticed it already about a month ago, but had no time to fix it."
LUCENE-7726, Uwe Schindler,01/Mar/17 22:29,"I'm suprised the java8 javadocs/linter don't warn about this.

We don't have a full HTML validator involved. In addition, for HTML5, the entity escaping can be left out, if it is unambiguous. This mimics the behaviour most browsers out there always had (because most web devs out there did this wrong). So the produced HTML is valid (HTML5) and also leads to no problems in HTML4 browsers. But we should still fix it.

The requirement to escape also attributes is a requirement of just Java 9's Javac (which is a bug from the HTML5 perspective, but a good thing, too)."
LUCENE-7726, Uwe Schindler,01/Mar/17 22:43,"There is also another TODO (separate issue, I'll open one): pegdown (the markdown processor) is also incompatible to Java 9; but it reached its end of life. The developer points to a new replacement lib - I just have to rewrite the Java code a bit. This is on my ""mental TODO list...."" since build 154 of Java 9."
LUCENE-7726, Uwe Schindler,01/Mar/17 22:55,"Patch fixing this issue. You can still not build full documentation because of failing ""pegdown"" processor, but that's a separate issue."
LUCENE-7726, Uwe Schindler,01/Mar/17 23:13,Thanks Hoss Man for reminding me!
LUCENE-7726, Uwe Schindler,01/Mar/17 23:25,"FYI, here is the HTML5 definition of ""ambiguous ampersand"": https://www.w3.org/TR/html5/syntax.html#syntax-ambiguous-ampersand
In fact, Java 9's Javadocs and Javac parser do not use that definition, which may be seen as a bug, but in fact this definition is just horrible and was added to make sloppy web developers happy..."
LUCENE-7728, Steve Rowe,05/Mar/17 19:54,"Another reproducing branch_6x failure from Policeman Jenkins https://jenkins.thetaphi.de/job/Lucene-Solr-6.x-MacOSX/737/:


   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGrouping -Dtests.method=testRandom -Dtests.seed=7D05E3F750606D -Dtests.slow=true -Dtests.locale=ar-EG -Dtests.timezone=America/Asuncion -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] FAILURE 3.08s J0 | TestGrouping.testRandom <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: expected:<394> but was:<235>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([7D05E3F750606D:723120EC4630D61E]:0)
   [junit4]    > 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:1288)
   [junit4]    > 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:1130)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=FastCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=FAST, chunkSize=11351, maxDocsPerChunk=2, blockSize=208), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=FAST, chunkSize=11351, blockSize=208)), sim=RandomSimilarity(queryNorm=true,coord=yes): {content=DFI(Saturated)}, locale=ar-EG, timezone=America/Asuncion
   [junit4]   2> NOTE: Mac OS X 10.11.6 x86_64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=3,threads=1,free=37784112,total=54788096"
LUCENE-7728, Adrien Grand,05/Apr/17 17:36,"Another reprodcing master failure: https://elasticsearch-ci.elastic.co/job/apache+lucene-solr+master/9262/console


   [junit4] Suite: org.apache.lucene.search.grouping.TestGrouping
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGrouping -Dtests.method=testRandom -Dtests.seed=42883A34582024BB -Dtests.slow=true -Dtests.locale=sr-Latn-BA -Dtests.timezone=America/Miquelon -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 6.98s J0 | TestGrouping.testRandom <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: expected:<477> but was:<468>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([42883A34582024BB:30C41F3BE94092C8]:0)
   [junit4]    > 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:1299)
   [junit4]    > 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:1141)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {groupend=FST50, sort1=BlockTreeOrds(blocksize=128), sort2=FST50, content=PostingsFormat(name=Direct), group=BlockTreeOrds(blocksize=128)}, docValues:{author=DocValuesFormat(name=Lucene70), sort1=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Lucene70), sort2=DocValuesFormat(name=Direct), group=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1782, maxMBSortInHeap=5.88541198629704, sim=RandomSimilarity(queryNorm=true): {content=DFI(Standardized)}, locale=sr-Latn-BA, timezone=America/Miquelon
   [junit4]   2> NOTE: Linux 4.4.35-33.55.amzn1.x86_64 amd64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=4,threads=1,free=320921624,total=331350016
   [junit4]   2> NOTE: All tests run in this JVM: [TestGrouping]"
LUCENE-7728, Steve Rowe,07/Jul/17 00:32,"Another reproducing master failure from my Jenkins (not sure of commit sha, since job history has expired, but the email notification arrived on July 1st):


  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGrouping -Dtests.method=testRandom -Dtests.seed=E6967AF0433A23C7 -Dtests.slow=true -Dtests.locale=en-ZA -Dtests.timezone=America/Argentina/La_Rioja -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
  [junit4] FAILURE 5.85s J0 | TestGrouping.testRandom <<<
  [junit4]    > Throwable #1: arrays first differed at element [0]; expected:<0.0> but was:<9.4267284E-4>
  [junit4]    > 	at __randomizedtesting.SeedInfo.seed([E6967AF0433A23C7:94DA5FFFF25A95B4]:0)
  [junit4]    > 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:1290)
  [junit4]    > 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:1124)
  [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
  [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {groupend=PostingsFormat(name=LuceneFixedGap), sort1=FST50, sort2=PostingsFormat(name=LuceneFixedGap), content=Lucene50(blocksize=128), group=FST50}, docValues:{author=DocValuesFormat(name=Direct), sort1=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct), sort2=DocValuesFormat(name=Memory), group=DocValuesFormat(name=Direct)}, maxPointsInLeafNode=613, maxMBSortInHeap=6.679295579171384, sim=RandomSimilarity(queryNorm=false): {content=DFI(Saturated)}, locale=en-ZA, timezone=America/Argentina/La_Rioja
  [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=381684000,total=514850816"
LUCENE-7731, Christine Poerschke,03/Mar/17 13:38,Thanks chillon.m!
LUCENE-7749, Adrien Grand,31/Mar/17 09:46,This is a bug indeed! Would you like to submit a patch? We are soon going to release 6.5.1 so I think it would be nice to have that fix in.
LUCENE-7749, Adrien Grand,31/Mar/17 14:07,I went ahead and worked on a patch to have it for 6.5.1 in time.
LUCENE-7749, Martin Amirault,03/Apr/17 01:21,"Adrien Grand , thank you for working the on fix.
I dont have lucene development set up here, but the patch may still need one more fix to work correctly (looking at 6.5.0 with your patch).
Actually LRUQueryCache#CachingWrapperWeight not delegating the scorerSupplier was one of the problem, but there is another problem related to BulkScorer, if you look at the last part of the stack trace:

...
	  at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:381)
	  at org.apache.lucene.search.Weight.bulkScorer(Weight.java:160)
	  at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:375)
	  at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.cache(LRUQueryCache.java:704)
	  at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:787)
	  at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:666)
	  at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:473)

BooleanWeight.java:385 always get the score with randomAccess=false, which again will ignore any IndexOrDocValuesQuery improvements.
Weight#bulkScorer should probably use the scorerSupplier method instead of the scorer method internally."
LUCENE-7749, Adrien Grand,03/Apr/17 08:04,"Hi Martin, thanks for looking at the patch. It is expected that BulkScorer always gets a scorer with randomAccess=false since bulk scorers always need to return all matching documents. I don't think this is an issue.

The stack trace that you shared happened because the query cache noticed that a BooleanQuery that is part of your query tree (it might be your top-level query) is being reused, and so it decides to build a cache entry for it (LRUQueryCache$CachingWrapperWeight.cache). Building a cache entry requires to consume all documents that match the query, so it makes sense that the scorer is created with randomAccess=false. Note that the fact that the boolean scorer is created with randomAccess=false does not necessarily mean that all scorers of sub clauses of the BooleanQuery will be created with randomAccess=false too: if the query is a conjunction then only the clauses that have the lowest cost will be created with randomAccess=false. As a consequence, the IndexOrDocValues optimization still applies if the range query is not the least costly clause."
LUCENE-7749, Martin Amirault,03/Apr/17 08:41,"I see. Thank you for checking !
One last suggestion: Weight#scorer method seems quite prone to confusion with the Weight#scorerSupplier, and could lead to additionnal problems like it happened with the LRUQueryCache. Wouldnt it be safer to expose only Weight#scorerSupplier, and make the Weight#scorer method protected ?"
LUCENE-7749, Adrien Grand,03/Apr/17 09:13,"I agree it is a bit error-prone and removing the scorer method or hiding it would fix the issue. The way I have been approaching this issue is that it is a fairly new and experimental API so I'd like to keep it optional for now (ie. query impls do not have to implement this method). Now that this feature is released, hopefully we'll get feedback about how well it works, which will in-turn give us opportunities to improve this API and once we are more confident about it, we can think about removing the trap that you observed. For the record, this is my personal opinion and might not be shared by other people who work on this project."
LUCENE-7749, Martin Amirault,03/Apr/17 09:32,"I agree, refactoring and api improvement should not be part of patch versions. Current code is good enough for the moment, and that could be improved as part of a more global refactoring in the future."
LUCENE-7755, Adrien Grand,28/Mar/17 12:59,Here is a patch.
LUCENE-7755, Martijn van Groningen,28/Mar/17 15:08,+1 good catch!
LUCENE-7757, David Smiley,22/Mar/17 05:08,"I suspect this is a duplicate of LUCENE-7719 based on the non-ASCII text I see in your example?  In 6.4.3, prefix queries were fixed – LUCENE-7717 but wildcard queries (not a simple prefix) were pushed off until someone has time to fix."
LUCENE-7757, Bjarke Mortensen,27/Mar/17 08:48,"Yes, most documents (if not all) in our index will have non-ascii characters  (although I'm not sure I can locate them in the example above).

I have tried to index a document without non-ascii characters, but highlighting for that document is still empty.
Are you saying that the presence of non-ascii characters in the index will mess highlighting up for all documents?

Thanks,
Bjarke"
LUCENE-7757, David Smiley,29/Mar/17 16:31,"I got to the bottom of this one; it's tricky.  I see two issues:

1. The UH's PhraseHelper uses WeightedSpanTermExtractor to convert the query to a SpanQuery.  WSTE has no knowledge of  ComplexPhraseQuery so it has some fallback logic.  PhraseHelper overrides isQueryUnsupported but it has a lingering TODO with a return true, thus any any query not known in advance is not going to be highlighted.  I think this should be modified to return false.  I did that locally and I also found it to then be necessary to override getLeafContext() to return a dummy context.  The PH can't produce a real leaf context (here) because this is the stage at which it is merely analyzing the query, no possible wildcard expansion is done (yet).  The query worked in the original Highlighter because there is no split phase.

2. ComplexPhraseQueryParser produces a special Query subclass ComplexPhraseQuery.  CPQ implements rewrite() that also calls rewrite() on the clauses.  It expects a real (not a dummy) leaf context.  So this works from a query execution standpoint, but I think it would be more friendly with the UH if CPQ didn't cascade the rewrite.  It's not a simple matter of commenting out the cascaded rewrite though... I will investigate further when I have more time."
LUCENE-7758, Adrien Grand,30/Mar/17 09:06,"This behaviour is irrational

Well, this is not exactly true. This is a token filter, meaning it can be applied on top on any set of other token filters. Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters), which also have a protected isTokenChar method that can be overriden in case you want to eg. split on whitespaces."
LUCENE-7758, Adrien Grand,30/Mar/17 09:06,"This behaviour is irrational

Well, this is not exactly true. This is a token filter, meaning it can be applied on top on any set of other token filters. Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters), which also have a protected isTokenChar method that can be overriden in case you want to eg. split on whitespaces."
LUCENE-7758, Mikhail Bystryantsev,30/Mar/17 11:29,"Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move
But why one feature should break another? I don't use synonyms or something like that, but I have no possibility to use token filter with properly offsets.

A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters)
Such workaround applicable only to cases when input text can be simple splitted on specified characters. In my case I want to use icu_tokenizer before edge_ngram for properly split by words. For example, imagine japan language."
LUCENE-7758, Adrien Grand,30/Mar/17 12:21,"I agree with what you are saying, it would like to have the ability to modify offsets in the cases that it makes sense too. I just wanted to react to your comment that the current behaviour is irrational. Moreover, I would not be surprised that highlighting the entire token is a desired behaviour for some users.

I haven't thought about it much but it feels to me that we would need a way to annotate token streams in order to know whether the content of the CharTermAttribute matches the original text between the offsets stored in the OffsetAttribute if we want to change offsets safely."
LUCENE-7758, Mikhail Bystryantsev,30/Mar/17 13:04,"I just wanted to react to your comment that the current behaviour is irrational
Ok, in other words: unexpected and may be confusing from the point of view of the average user.

I would not be surprised that highlighting the entire token is a desired behaviour for some users.
I think for such cases just should be implemented possibility to behaviour tuning, for example parameter like inherit_offsets: true | false."
LUCENE-7758, Uwe Schindler,30/Mar/17 13:18,"Moreover, I would not be surprised that highlighting the entire token is a desired behaviour for some users.

This is correct. Modifying offsets inside a TokenFilter is not going to be correct for highlighting for the reasons you are mentioning. This is a general issue with all token filters that are splitting tokens: The ""famous"" example is WordDelimiterFilter.

Assigning offsets is the responsibility of tokenizers. Tokenfilters should just look at tokens and modify them, but not split them or change their offsets. 

In addition, highlighting is not meant to produce ""exact"" explanations of every analysis step. It is more meant to allow highlighting whole tokens afterwards, so the user has an idea, which token was responsible for a hit."
LUCENE-7758, Mikhail Bystryantsev,30/Mar/17 13:51,"Assigning offsets is the responsibility of tokenizers. Tokenfilters should just look at tokens and modify them, but not split them or change their offsets.
But tokenizer can be only one, so there is no way to get tokens different than produced by specific single tokenizer. No way to customize without writing your own tokenizers. It is possible to combine token filters, but not tokenizers.

In addition, highlighting is not meant to produce ""exact"" explanations of every analysis step. It is more meant to allow highlighting whole tokens afterwards, so the user has an idea, which token was responsible for a hit.
I think this should be decided by Lucene users, not by anyone else. When you project your index and search behaviour, only you can decide how it should work based on your project requirements."
LUCENE-7759, Michael McCandless,30/Mar/17 13:19,"Sigh, +1."
LUCENE-7760, Steve Rowe,30/Mar/17 13:41,"+1

From http://mail-archives.apache.org/mod_mbox/lucene-java-user/201611.mbox/%3c36FCFD77-D873-4757-9D16-E89016F169E6@gmail.com%3e, where I most recently responded to a user question about the situation - this should be useful as a seed for javadoc fixes:


The behavior you mention is an intentional change from the behavior in Lucene 4.9.0 and earlier,
when tokens longer than maxTokenLenth were silently ignored: see LUCENE-5897[1] and LUCENE-5400[2].

The new behavior is as follows: Token matching rules are no longer allowed to match against
input char sequences longer than maxTokenLength.  If a rule that would match a sequence longer
than maxTokenLength, but also matches at maxTokenLength chars or fewer, and has the highest
priority among all other rules matching at this length, and no other rule matches more chars,
then a token will be emitted for that rule at the matching length.  And then the rule-matching
iteration simply continues from that point as normal.  If the same rule matches against the
remainder of the sequence that the first rule would have matched if maxTokenLength were longer,
then another token at the matched length will be emitted, and so on. 

Note that this can result in effectively splitting the sequence at maxTokenLength intervals
as you noted.

You can fix the problem by setting maxTokenLength higher - this has the side effect of growing
the buffer and not causing unwanted token splitting.  If this results in tokens larger than
you would like, you can remove them with LengthFilter.

FYI there is discussion on LUCENE-5897 about separating buffer size from maxTokenLength, starting
here: <https://issues.apache.org/jira/browse/LUCENE-5897?focusedCommentId=14105729&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14105729>
- ultimately I decided that few people would benefit from the increased configuration complexity.

[1] https://issues.apache.org/jira/browse/LUCENE-5897
[2] https://issues.apache.org/jira/browse/LUCENE-5400"
LUCENE-7760, Michael McCandless,31/Mar/17 09:52,OK thanks for the history here Steve Rowe ... I will fixup the javadocs based on this.
LUCENE-7760, Michael McCandless,02/Apr/17 19:54,"Patch, improving the javadocs and adding a couple test cases ..."
LUCENE-7760, Steve Rowe,02/Apr/17 22:25,"+1

Tests look good.  I like your simplification of my explanation (""[long tokens] are chopped up at [maxTokenLength] and emitted as multiple tokens""); a more precise description about rule matching, including the possibility of emitted tokens not being exactly maxTokenLength, is not likely to help many people."
LUCENE-7760, Steve Rowe,02/Apr/17 22:27,"Oh, I forgot to mention: UAX29URLEmailTokenizer has the same issue, and would benefit from the same javadoc fix (and tests)."
LUCENE-7760, Michael McCandless,03/Apr/17 12:41,Thanks Steve Rowe; I'll do the same for UAX29URLEmailTokenizer.
LUCENE-7760, Michael McCandless,03/Apr/17 18:41,"Another iteration, also fixing UAX29URLEmailTokenizer.  I also carried over the max token length check from StandardAnalyzer."
LUCENE-7760, Steve Rowe,03/Apr/17 23:46,"+1, thanks Mike.

Another iteration, also fixing UAX29URLEmailTokenizer. I also carried over the max token length check from StandardAnalyzer.

Cool, that check was from LUCENE-6682, where I should have also applied it to UAX29URLEmailTokenizer..."
LUCENE-7760, Michael McCandless,11/Apr/17 19:41,Thanks Steve Rowe.
LUCENE-7761, Adrien Grand,31/Mar/17 14:09,I'll merge it. Thanks Pablo!
LUCENE-7769, David Smiley,07/Apr/17 12:27,"Quite right; thanks for reporting this Dmitry Malinin!  This root cause is a larger issue of there being no Query traversal API in Lucene, which bites highlighters in the but all the time.  LUCENE-3041

I won't 'have time to fix this until another 6 hours from now... likely after the planned RC.  Ping Joel Bernstein"
LUCENE-7769, David Smiley,10/Apr/17 16:44,Here's a patch.  The problem here is also for SpanBoostQuery which I added a test for.
LUCENE-7775, Michael McCandless,10/Apr/17 23:08,Thanks JC; these are real bugs!  I pushed your suggested fixes.
LUCENE-7775, Michael McCandless,10/Apr/17 23:08,Thanks JC; these are real bugs!  I'll push those fixes.
LUCENE-7777, Michael McCandless,11/Apr/17 13:37,"Patch w/ test case showing the issue, and the fix; I also tried to simplify both the append and readBytes implementations..."
LUCENE-7777, Adrien Grand,11/Apr/17 13:42,+1 the new append/readBytes impls are much easier to read to me now
LUCENE-7777, Michael McCandless,11/Apr/17 15:52,Thanks Adrien Grand.
LUCENE-7777, Dawid Weiss,11/Apr/17 16:38,Much simpler to read. Nice issue number too.
LUCENE-7779, Michael McCandless,11/Apr/17 19:44,Simple patch w/ test case and fix.
LUCENE-7779, Adrien Grand,12/Apr/17 06:32,"I think the patch should try to treat the ByteSequencesReader and readerDone more similarly. For instance if you look at readPartition out of context, it feels wrong to set readerDone to false for a ByteSequencesReader that has been passed as a method argument: what if it is called for two different ByteSequencesReader in a row? Maybe we should make readerDone a boolean[1] that we pass to readPartition next to the ByteSequencesReader, or alternatively also store the ByteSequencesReader as a member of the OfflineSorter."
LUCENE-7779, Michael McCandless,12/Apr/17 13:51,"Thanks Adrien Grand, I agree it's inconsistent now.  I'll use the boolean[1] approach ... I wish I could simply return multiple values in Java like I can from Python."
LUCENE-7779, Michael McCandless,12/Apr/17 17:41,"New patch, passing an boolean[] isExhausted instead."
LUCENE-7780, Adrien Grand,12/Apr/17 07:09,Here is a simple patch.
LUCENE-7780, Adrien Grand,13/Apr/17 06:58,Thanks Michael McCandless.
LUCENE-7783, Adrien Grand,13/Apr/17 07:42,Patch.
LUCENE-7791, Michael McCandless,20/Apr/17 12:06,"Whoa, nice catch!  Is it possible to also add a test case in the patch showing the bug?"
LUCENE-7791, Przemysław Szeremiota,20/Apr/17 12:39,"I'll try, but don't have anything ready, so I don't know if I manage it today"
LUCENE-7791, Przemysław Szeremiota,20/Apr/17 13:11,"OK, test-patch: it fails on branch_6_5, and passes with patch; rudimentary test only for NumericDocValuesWriter, fails with AIOOBE:


   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIndexSorting -Dtests.method=testEmptyNonSortedIntField -Dtests.seed=B1B45F478095D85D -Dtests.slow=true -Dtests.locale=fr-BE -Dtests.timezone=Canada/Mountain -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.02s | TestIndexSorting.testEmptyNonSortedIntField <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: index=127, numBits=64
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([B1B45F478095D85D:EA0CCD4D0DFEC9E8]:0)
   [junit4]    > 	at org.apache.lucene.util.FixedBitSet.get(FixedBitSet.java:181)
   [junit4]    > 	at org.apache.lucene.index.NumericDocValuesWriter$SortingNumericIterator.next(NumericDocValuesWriter.java:257)
   [junit4]    > 	at org.apache.lucene.index.NumericDocValuesWriter$SortingNumericIterator.next(NumericDocValuesWriter.java:228)
   [junit4]    > 	at org.apache.lucene.codecs.memory.MemoryDocValuesConsumer.addNumericField(MemoryDocValuesConsumer.java:112)
   [junit4]    > 	at org.apache.lucene.codecs.memory.MemoryDocValuesConsumer.addNumericField(MemoryDocValuesConsumer.java:91)
   [junit4]    > 	at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addNumericField(PerFieldDocValuesFormat.java:111)
   [junit4]    > 	at org.apache.lucene.index.NumericDocValuesWriter.flush(NumericDocValuesWriter.java:96)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain.writeDocValues(DefaultIndexingChain.java:258)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain.flush(DefaultIndexingChain.java:142)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:444)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:539)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:653)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:3007)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3242)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3205)
   [junit4]    > 	at org.apache.lucene.index.TestIndexSorting.testEmptyNonSortedIntField(TestIndexSorting.java:774)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)"
LUCENE-7791, Jim Ferenczi,20/Apr/17 14:23,"Great catch indeed !


Also, check for range of values for given field is now happening based on original ID (e.g. ""upto < size""), so flushing can now lost some values, even without hitting AIOOBE.

I think it's ok since ""size"" corresponds to the max docID we have in the buffer so we cannot lost values here unless I am missing something ?
So the only problem here is that we don't check if the remapped doc id is greater than the capacity of the bitset.


please check changes from LUCENE-7579 for confirmation of lack of additional bugs in other flush-sorting writers.

I did and the other doc value sorters do not use a bitset to handle missing values. I think we are safe with this patch.

The patch looks good and the test too, this bug only appears in 6.x since the code is slightly different in 7.
I'll merge shortly unless Michael McCandless has something to add here ?"
LUCENE-7791, Przemysław Szeremiota,20/Apr/17 15:40,"Maybe I worded it unclear:

-      if (upto < size) {
-        int oldID = sortMap.newToOld(upto);


vs

+      int oldID = sortMap.newToOld(upto);
+      if (oldID < size) {


size and oldID is bounded by actual number of values in buffer, but upto is not (not necessarily, because it is reordered); so i have had a hunch that it could lead to skip legitimate values if sorted (upto) ID is greater than value buffer size (but original wouldn't be); but I don't have a proof/test for it. Anyways, patch as it is should work"
LUCENE-7791, Jim Ferenczi,20/Apr/17 18:08,"Ok I see it now. The code assumes that size==maxDoc and that's of course not true for sparse field. Applying the sort on a sparse numeric doc values was also broken (throws AIOOBE) so I reused your patch and published another one which also fixes the sorting on sparse numeric doc values.
Thanks so much for noticing this Przemysław Szeremiota !"
LUCENE-7791, Michael McCandless,20/Apr/17 20:28,"+1 to merge.

Maybe we should hold 6.5.1 RC2 for this?"
LUCENE-7791, Jim Ferenczi,20/Apr/17 20:36,"Ok I'll merge to 6.x first, add the tests to master in order to ensure that this bug does not impact 7.x. I hope I can merge to 6.5 if the 6.5.1 RC2 can be rebuilt, thanks for raising this Michael McCandless."
LUCENE-7791, Jim Ferenczi,20/Apr/17 23:18,"Ok this is now merged to 6.x and 6.5 and the fix will be part of the 6.5.1 release.
I still need to adapt the test for master.
Thanks Przemysław Szeremiota  and Michael McCandless !"
LUCENE-7791, Przemysław Szeremiota,21/Apr/17 06:46,"Jim Ferenczi What about SortingNumericIterator@NormValuesWriter? It throws too, and LUCENE-7791.patch misses it's fix?"
LUCENE-7791, Jim Ferenczi,21/Apr/17 09:16,"I don't know what happened but the fix for NormsValueWriter is not in my patch. I'll push the fix shortly with additional tests for this case, thanks for checking."
LUCENE-7791, Przemysław Szeremiota,21/Apr/17 12:42,"Jim Ferenczi Michael McCandless Thank you both very much, great (and quick!!) job! I look forward to release"
LUCENE-7791, Michael McCandless,22/Apr/17 09:49,Thank you Przemysław Szeremiota!
LUCENE-7793, Steve Rowe,20/Apr/17 22:44,Patch adding documentation-lint to smokeTestRelease.py.  I'll try it out on the next 6.5.1 RC.
LUCENE-7793, Steve Rowe,21/Apr/17 14:24,"The first patch failed when I ran it against the latest 6.5.1 RC2 because Lucene source dist doesn't have the helper scripts for documentation-lint that are under dev-tools/, which is not included:


-documentation-lint:
     [echo] checking for broken html...
    [jtidy] Checking for broken html (such as invalid tags)...
   [delete] Deleting directory /tmp/smoke_lucene_6.5.1_cd1f23c63abe03ae650c75ec8ccb37762806cc75/unpack/lucene-6.5.1/build/jtidy_tmp
     [echo] Checking for broken links...
     [exec] python3: can't open file '/tmp/smoke_lucene_6.5.1_cd1f23c63abe03ae650c75ec8ccb37762806cc75/unpack/dev-tools/scripts/checkJavadocLinks.py': [Errno 2] No such file or directory



This version of the patch only attempts to run documentation-lint when the project is Solr, which has dev-tools/.

I'm running this revised script now."
LUCENE-7793, Steve Rowe,21/Apr/17 14:50,"I'm running this revised script now.

The smoker succeeded with the patch, running ""ant validate"" on the Lucene source dist and ""ant validate documentation-lint"" on the Solr source dist."
LUCENE-7793, Steve Rowe,28/Apr/17 13:55,Committing shortly.
LUCENE-7794, Steve Rowe,20/Apr/17 23:43,"Patch, adds ""validate"" and ""documentation-lint"" to the ""ant clean test"" invocation.  The other component of ""precommit"", the ""check-working-copy"", isn't necessary in buildAndPushRelease.py, since its getGitRev() method already performs the same checks.

I'll test this (after disabling the dirty checkout tests in getGitRev())."
LUCENE-7794, Steve Rowe,21/Apr/17 13:39,I successfully built an RC with the patch.
LUCENE-7794, Steve Rowe,28/Apr/17 13:55,Committing shortly.
LUCENE-7795, Michael Braun,20/Apr/17 21:37,Added what the simple fix looks like - modified an existing test which is this case in which hasIllegalOffsets is true.
LUCENE-7797, Michael McCandless,22/Apr/17 09:56,"Indeed, the logic is backwards now.  I was wondering how Lucene could possible be working, since listAll is used when opening a non-near-real-time DirectoryReader, but then I realized it's only the static FSDirectory.listAll(Path) that tickles this bug since the non-static version always passes pendingDeletes.

Thank you for the patch w/ test case Atkins Chang ... I'll push soon."
LUCENE-7797, Michael McCandless,22/Apr/17 12:57,Thank you Atkins Chang.
LUCENE-7799, Jim Ferenczi,24/Apr/17 09:12,"There is an option called autoGenerateMultiTermSynonymsPhraseQuery which when activated builds phrase query for multi-terms synonyms. The naming is a bit misleading but it can achieve what you're describing. The idea, which should maybe be extended, is to build a phrase query when side paths of different length are encountered inside the query. With a WordDelimiterGraphFilter that preserves original tokens (preserveOriginal) this heuristic works perfectly since each split creates side paths of different length."
LUCENE-7799, Steve Rowe,24/Apr/17 14:21,"Thanks Jim, I was aware of autoGenerateMultiTermSynonymsPhraseQuery, but it only applies when graphs are detected, which requires side paths.  When WDGF doesn't preserve original tokens and splits a word, though, graph processing isn't invoked, so that option isn't effective."
LUCENE-7799, Jim Ferenczi,24/Apr/17 21:51,"Then we should maybe remove autoGenerateMultiTermSynonymsPhraseQuery and make autoGeneratePhraseQueries applicable to graph and non-graph query even when splitOnWhitespace=false. A nice side effect of this change is that it would correctly handle a simple synonym rule like ""ny, new york"". With autoGeneratePhraseQueries=true ""new york"" would be matched as a phrase query with the input query ""ny"" and as a simple disjunction with the input query ""new york"". Of course this should also be true when ""ny"" is preserved in the output (when graph processing is involved) which is why I think it is important to have a single option for graph and non-graph query."
LUCENE-7799, Steve Rowe,24/Apr/17 22:40,I agree that these two options should be collapsed into one.
LUCENE-7803, Manuel Gellfart,25/Apr/17 14:53,"Okay. I Guess I misused the FunctionScoringQuery.

I now added both the ""original"" Boolean Queries and the FunctionScoringQueries wrapping the ""original"" BooleanQuery to another Top Level Boolean Query instead of just adding the FunctionScoringQueries.

Now the boosting and the highlighting works correctly. I will close this issue."
LUCENE-7805, Steve Rowe,26/Apr/17 14:36,"Another reproducing failure from my Jenkins, this one on branch_6x, no FlattenGraphFilter here though:


Checking out Revision b90bfaba1f065598033b60f0ba5ffaa40053eb42 (refs/remotes/origin/branch_6x)
[...]
   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains
   [junit4]   2> TEST FAIL: useCharFilter=false text='\u26a7\u26e4\u26e7\u2672\u2694\u2604\u26a5\u26d1 \u0566\u054e\u057d\u0553\u057f\u0549 os\uecda\ud92c\udcd2  fhyysvvo vzebx \u30b2\u30d7\u30d8\u30dd\u30e3\u30f5\u30ea\u30dd\u30f7\u30ae\u30e6\u30c9 \uaa3e\uaa1a\uaa5b\uaa43\uaa49\uaa53\uaa19\uaa4f\uaa5c gvrfm fe '
   [junit4]   2> Exception from random analyzer: 
   [junit4]   2> charfilters=
   [junit4]   2>   org.apache.lucene.analysis.charfilter.HTMLStripCharFilter(java.io.StringReader@24cfdcc4)
   [junit4]   2>   org.apache.lucene.analysis.fa.PersianCharFilter(org.apache.lucene.analysis.charfilter.HTMLStripCharFilter@12dc3e69)
   [junit4]   2> tokenizer=
   [junit4]   2>   org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer(org.apache.lucene.util.AttributeFactory$1@cf373344)
   [junit4]   2> filters=
   [junit4]   2>   org.apache.lucene.analysis.gl.GalicianMinimalStemFilter(ValidatingTokenFilter@aec428a term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.KeywordRepeatFilter(ValidatingTokenFilter@79bef7cf term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false)
   [junit4]   2>   org.apache.lucene.analysis.cjk.CJKBigramFilter(ValidatingTokenFilter@55fed8e4 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false, 20)
   [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(ValidatingTokenFilter@226ff415 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,payload=null,keyword=false, 32)
   [junit4]   2> offsetsAreCorrect=false
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=19C23372FBAA95FB -Dtests.slow=true -Dtests.locale=bg -Dtests.timezone=Pacific/Ponape -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] ERROR   0.47s J7 | TestRandomChains.testRandomChainsWithLargeStrings <<<
   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset; got startOffset=41,endOffset=40
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([19C23372FBAA95FB:73998C63A2E4B508]:0)
   [junit4]    > 	at org.apache.lucene.analysis.tokenattributes.PackedTokenAttributeImpl.setOffset(PackedTokenAttributeImpl.java:110)
   [junit4]    > 	at org.apache.lucene.analysis.shingle.ShingleFilter.incrementToken(ShingleFilter.java:345)
   [junit4]    > 	at org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:67)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:730)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:641)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:539)
   [junit4]    > 	at org.apache.lucene.analysis.core.TestRandomChains.testRandomChainsWithLargeStrings(TestRandomChains.java:880)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene62): {dummy=PostingsFormat(name=Memory doPackFST= true)}, docValues:{}, maxPointsInLeafNode=920, maxMBSortInHeap=6.66438430375091, sim=RandomSimilarity(queryNorm=false,coord=crazy): {}, locale=bg, timezone=Pacific/Ponape
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=383185632,total=512229376"
LUCENE-7805, Steve Rowe,22/May/17 16:18,"Another reproducing failure from ASF Jenkins https://builds.apache.org/job/Lucene-Solr-NightlyTests-6.6/9/ - reproduces for me on master:


   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains
   [junit4]   2> TEST FAIL: useCharFilter=true text='<p>\\\""< \ud83c\udc7f\ud83c\udc50 kyxgfmsyozqzqq \u2c77\u2c7c\u2c74\u2c68 tgwqqnkz mj'
   [junit4]   2> Exception from random analyzer: 
   [junit4]   2> charfilters=
   [junit4]   2>   org.apache.lucene.analysis.fa.PersianCharFilter(java.io.StringReader@1429de93)
   [junit4]   2>   org.apache.lucene.analysis.pattern.PatternReplaceCharFilter(a, <HANGUL>, org.apache.lucene.analysis.fa.PersianCharFilter@4cf58820)
   [junit4]   2> tokenizer=
   [junit4]   2>   org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer(org.apache.lucene.util.AttributeFactory$1@c798683c)
   [junit4]   2> filters=
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.CodepointCountFilter(ValidatingTokenFilter@25c3f003 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word, 17, 31)
   [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(ValidatingTokenFilter@7611d032 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word)
   [junit4]   2>   org.apache.lucene.analysis.core.FlattenGraphFilter(ValidatingTokenFilter@19064e70 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word)
   [junit4]   2> offsetsAreCorrect=true
   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=E35CB6A9A0ADFBB8 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-6.6/test-data/enwiki.random.lines.txt -Dtests.locale=es-CR -Dtests.timezone=Asia/Krasnoyarsk -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 23.5s J2 | TestRandomChains.testRandomChainsWithLargeStrings <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: outputEndNode=3 vs inputTo=2
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([E35CB6A9A0ADFBB8:890709B8F9E3DB4B]:0)
   [junit4]    > 	at org.apache.lucene.analysis.core.FlattenGraphFilter.incrementToken(FlattenGraphFilter.java:335)
   [junit4]    > 	at org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:67)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:730)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:641)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:539)
   [junit4]    > 	at org.apache.lucene.analysis.core.TestRandomChains.testRandomChainsWithLargeStrings(TestRandomChains.java:880)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene62): {dummy=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{}, maxPointsInLeafNode=508, maxMBSortInHeap=7.7739547181262205, sim=RandomSimilarity(queryNorm=true,coord=no): {dummy=DFR I(F)B2}, locale=es-CR, timezone=Asia/Krasnoyarsk
   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=4,threads=1,free=35313576,total=261619712"
LUCENE-7805, Steve Rowe,22/Sep/17 12:57,"Another failure, from https://builds.apache.org/job/Lucene-Solr-NightlyTests-7.x/49 (reproduces for me on master):


Checking out Revision dd59822ac94ee0b5cc2bedcac38f60442c1af5e6 (refs/remotes/origin/branch_7x)
[...]
   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains
   [junit4]   2> TEST FAIL: useCharFilter=false text=' \ubc25 \ud834\udc6e\ud834\udc76\ud834\udc3e \u467e\ued9e\u0003#\ufb63\u032d \u2c61\u2c62\u2c61\u2c76\u2c60\u2c7f \u30da\u30dd\u30f2\u30cf\u30d4 \u2182\u2150\u218f\u215a\u2166\u216b\u2157\u215b\u216a\u2172\u215c\u2165 udb \ua835\ua830\ua833\ua833\ua83b\ua836\ua834\ua83b\ua83a \udb51\udff0\ud85a\udfde'
   [junit4]   2> Exception from random analyzer: 
   [junit4]   2> charfilters=
   [junit4]   2>   org.apache.lucene.analysis.pattern.PatternReplaceCharFilter(a, <KATAKANA>, java.io.StringReader@6a8a3a98)
   [junit4]   2> tokenizer=
   [junit4]   2>   org.apache.lucene.analysis.standard.StandardTokenizer()
   [junit4]   2> filters=
   [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(ValidatingTokenFilter@3d7cdb0 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1)
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.PatternKeywordMarkerFilter(ValidatingTokenFilter@7001ffb8 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false, a)
   [junit4]   2>   org.apache.lucene.analysis.cjk.CJKBigramFilter(ValidatingTokenFilter@7398e9e7 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false, -43)
   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter(ValidatingTokenFilter@6503b497 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false)
   [junit4]   2> offsetsAreCorrect=false
   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=5700948A30BC1917 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-7.x/test-data/enwiki.random.lines.txt -Dtests.locale=it-CH -Dtests.timezone=Africa/Luanda -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] ERROR   8.89s J0 | TestRandomChains.testRandomChainsWithLargeStrings <<<
   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=24,endOffset=31,lastStartOffset=27 for field 'dummy'
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([5700948A30BC1917:3D5B2B9B69F239E4]:0)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:767)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:430)
   [junit4]    > 	at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:392)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:239)
   [junit4]    > 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:481)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1717)
   [junit4]    > 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1462)
   [junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:171)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:650)
   [junit4]    > 	at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:540)
   [junit4]    > 	at org.apache.lucene.analysis.core.TestRandomChains.testRandomChainsWithLargeStrings(TestRandomChains.java:883)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:748)
[...]
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=false): {dummy=DFR I(n)L1}, locale=it-CH, timezone=Africa/Luanda
   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_144 (64-bit)/cpus=4,threads=1,free=185203120,total=275775488"
LUCENE-7807, Steve Rowe,26/Apr/17 14:25,"I used the TestIntRangeFieldQueries repro line with git bisect; the first failing commit is 907c43ce7af389c42ef200e5c2ecefbc5eee8a7a, which was committed under LUCENE-7449."
LUCENE-7807, Steve Rowe,29/Apr/17 01:01,"My Jenkins found a reproducing seed for testRandomTiny() for all 4 Test*RangeFieldQueries suites (I did not run git bisect to find the first failing commit for these):


   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestLongRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=CA203E745CDBBB7B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=ar-JO -Dtests.timezone=Pacific/Pago_Pago -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.02s J4  | TestLongRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 12): id=0 should not match but did
   [junit4]    >  queryRange=Box(-9223372036854775808 TO 9223372036854775807)
   [junit4]    >  box=Box(-7801183850520246677 TO -4964726929345376585)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([CA203E745CDBBB7B:8367E03202FA83D7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=true): {}, locale=ar-JO, timezone=Pacific/Pago_Pago
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=287973992,total=455081984




   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestDoubleRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=CA203E745CDBBB7B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=ko -Dtests.timezone=Europe/Samara -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.01s J8  | TestDoubleRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 12): id=0 should not match but did
   [junit4]    >  queryRange=Box(-Infinity TO Infinity)
   [junit4]    >  box=Box(-4.322208110367465E307 TO -2.940099941534568E307)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([CA203E745CDBBB7B:8367E03202FA83D7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=PostingsFormat(name=LuceneVarGapDocFreqInterval)}, docValues:{doubleRangeField=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=857, maxMBSortInHeap=5.262063554817104, sim=RandomSimilarity(queryNorm=true): {}, locale=ko, timezone=Europe/Samara
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=213636832,total=418906112




   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestFloatRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=CA203E745CDBBB7B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=es-MX -Dtests.timezone=America/Manaus -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.01s J6  | TestFloatRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 12): id=0 should not match but did
   [junit4]    >  queryRange=Box(-Infinity TO Infinity)
   [junit4]    >  box=Box(-8.078581E37 TO 3.0421576E37)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([CA203E745CDBBB7B:8367E03202FA83D7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=FSTOrd50}, docValues:{floatRangeField=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct)}, maxPointsInLeafNode=956, maxMBSortInHeap=7.104765246389017, sim=RandomSimilarity(queryNorm=true): {}, locale=es-MX, timezone=America/Manaus
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=278765944,total=403177472




   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIntRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=CA203E745CDBBB7B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=hu-HU -Dtests.timezone=Kwajalein -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.06s J9  | TestIntRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 12): id=0 should not match but did
   [junit4]    >  queryRange=Box(-2147483648 TO 2147483647)
   [junit4]    >  box=Box(-1181905225 TO 305792619)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([CA203E745CDBBB7B:8367E03202FA83D7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=PostingsFormat(name=MockRandom)}, docValues:{id=DocValuesFormat(name=Memory), intRangeField=DocValuesFormat(name=Direct)}, maxPointsInLeafNode=981, maxMBSortInHeap=6.92326027505176, sim=RandomSimilarity(queryNorm=false): {}, locale=hu-HU, timezone=Kwajalein
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=225819304,total=507510784"
LUCENE-7807, Steve Rowe,29/Apr/17 13:28,"ASF Jenkins found another reproducing seed for testRandomTiny() failures https://builds.apache.org/job/Lucene-Solr-NightlyTests-master/1288:


   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestLongRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=69A0759E7B6AFB0B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/test-data/enwiki.random.lines.txt -Dtests.locale=th-TH-u-nu-thai-x-lvariant-TH -Dtests.timezone=Asia/Jerusalem -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.10s J2 | TestLongRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 87): id=0 should not match but did
   [junit4]    >  queryRange=Box(-9223372036854775808 TO 9223372036854775807)
   [junit4]    >  box=Box(-5634937821465863869 TO 6781131293060666383)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([69A0759E7B6AFB0B:20E7ABD8254BC3A7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{id=DocValuesFormat(name=Lucene70), longRangeField=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1325, maxMBSortInHeap=5.728447065351258, sim=RandomSimilarity(queryNorm=false): {}, locale=th-TH-u-nu-thai-x-lvariant-TH, timezone=Asia/Jerusalem
   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=4,threads=1,free=104683560,total=353370112




   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestDoubleRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=69A0759E7B6AFB0B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/test-data/enwiki.random.lines.txt -Dtests.locale=hu-HU -Dtests.timezone=Asia/Kuala_Lumpur -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.04s J0 | TestDoubleRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 87): id=0 should not match but did
   [junit4]    >  queryRange=Box(-Infinity TO Infinity)
   [junit4]    >  box=Box(4.992828343822847E307 TO 7.931369402719117E307)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([69A0759E7B6AFB0B:20E7ABD8254BC3A7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=CheapBastard, sim=RandomSimilarity(queryNorm=false): {}, locale=hu-HU, timezone=Asia/Kuala_Lumpur
   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=4,threads=1,free=288978264,total=373817344




   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestFloatRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=69A0759E7B6AFB0B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/test-data/enwiki.random.lines.txt -Dtests.locale=ar-AE -Dtests.timezone=US/Eastern -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.01s J2 | TestFloatRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 87): id=0 should not match but did
   [junit4]    >  queryRange=Box(-Infinity TO Infinity)
   [junit4]    >  box=Box(-1.1309494E38 TO 1.5677334E38)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([69A0759E7B6AFB0B:20E7ABD8254BC3A7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{floatRangeField=DocValuesFormat(name=Asserting), id=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=201, maxMBSortInHeap=5.395318562767361, sim=RandomSimilarity(queryNorm=true): {}, locale=ar-AE, timezone=US/Eastern
   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=4,threads=1,free=434881432,total=498073600




   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIntRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=69A0759E7B6AFB0B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/test-data/enwiki.random.lines.txt -Dtests.locale=de-GR -Dtests.timezone=Asia/Thimbu -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.01s J2 | TestIntRangeFieldQueries.testRandomTiny <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: wrong hit (first of possibly more):
   [junit4]    > FAIL (iter 87): id=0 should not match but did
   [junit4]    >  queryRange=Box(-2147483648 TO 2147483647)
   [junit4]    >  box=Box(-366308337 TO 1411193155)
   [junit4]    >  queryType=CONTAINS
   [junit4]    >  deleted?=false
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([69A0759E7B6AFB0B:20E7ABD8254BC3A7]:0)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.verify(BaseRangeFieldQueryTestCase.java:287)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.doTestRandom(BaseRangeFieldQueryTestCase.java:158)
   [junit4]    > 	at org.apache.lucene.search.BaseRangeFieldQueryTestCase.testRandomTiny(BaseRangeFieldQueryTestCase.java:64)
[...]
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=PostingsFormat(name=Direct)}, docValues:{id=DocValuesFormat(name=Lucene70), intRangeField=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=528, maxMBSortInHeap=7.090080949615185, sim=RandomSimilarity(queryNorm=false): {}, locale=de-GR, timezone=Asia/Thimbu
   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_121 (64-bit)/cpus=4,threads=1,free=109562936,total=488636416"
LUCENE-7807, Steve Rowe,17/May/17 04:24,"Another reproducing master seed from ASF Jenkins for all 4 tests' .testRandomTiny() - here's the Int one:


ant test  -Dtestcase=TestIntRangeFieldQueries -Dtests.method=testRandomTiny -Dtests.seed=7FAAAB755DD26282 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/test-data/enwiki.random.lines.txt -Dtests.locale=fr-LU -Dtests.timezone=America/Creston -Dtests.asserts=true -Dtests.file.encoding=US-ASCII"
LUCENE-7807, Adrien Grand,23/May/17 15:51,"I opened LUCENE-7847 which is the cause of the last failure, but all reported seeds seem to be the same issue. It only fails with tiny ranges because this bug only occurs when the query seems to match all documents, which gets more and more unlikely as we add more documents given that values are computed randomly."
LUCENE-7807, Adrien Grand,24/May/17 06:50,"Closing now that LUCENE-7847 has been addressed. Thanks Steve Rowe for having logged those failures, we should have dug earlier."
LUCENE-7808, Erik Hatcher,27/Apr/17 00:16,(initially failing) tests and implementation fixes
LUCENE-7808, Alan Woodward,27/Apr/17 08:41,"Can you use Objects.hash() for the hashcode implementations?
	SpanPayloadCheckQuery.rewrite() needs to rewrite its inner query as well (see LUCENE-7481)



Otherwise +1"
LUCENE-7808, Erik Hatcher,27/Apr/17 12:08,"Alan Woodward thanks for feedback.  I did use Objects.hash() in the .hashCode's that I worked on - is that not quite correct?   As for rewrite() - I'm not sure what is needed there exactly.   For some reason I added a .rewrite to SpanPayloadCheckQuery, but not to PayloadScoreQuery - but I guess I didn't do that fully?   Do you have an example or some other hints as to how to implement what is needed?  Happy to do the work while I'm in there!"
LUCENE-7808, Erik Hatcher,27/Apr/17 12:35,"Alan Woodward - I think I'm getting warmer.   I implemented this (currently failing) test:


  public void testRewrite() throws IOException {
    SpanMultiTermQueryWrapper fou = new SpanMultiTermQueryWrapper(new WildcardQuery(new Term(""field"", ""fou*"")));
    SpanMultiTermQueryWrapper fiv = new SpanMultiTermQueryWrapper(new WildcardQuery(new Term(""field"", ""fiv*"")));

    SpanNearQuery sq = new SpanNearQuery(new SpanQuery[] {fou, fiv}, 0, true);

    List<BytesRef> payloads = new ArrayList<>();
    payloads.add(new BytesRef(""pos: 0""));
    BytesRef payload2 = new BytesRef(""pos: 1"");

    SpanPayloadCheckQuery query = new SpanPayloadCheckQuery(sq, payloads);

    checkHits(query, new int[]{1});
  }



The above errors with ""java.lang.IllegalArgumentException: Rewrite first!""."
LUCENE-7808, Erik Hatcher,27/Apr/17 12:56,fix for SpanPayloadCheckQuery's rewrite+test - solves LUCENE-7481
LUCENE-7808, Erik Hatcher,27/Apr/17 12:58,"Alan Woodward - I fumbled for a moment, but think I understand the rewrite issue and have made a test and fix in my latest patch.   I imagine that PayloadScoreQuery needs the same treatment, so I'll tackle that as well here."
LUCENE-7808, Erik Hatcher,27/Apr/17 19:23,"this patch adds the rewrite to PayloadScoreQuery too, along with test.  ready to commit to cover this JIRA and LUCENE-7481"
LUCENE-7809, Steve Rowe,28/Apr/17 17:43,"I can't reproduce this problem in a different local repo - I'm guessing something weird happened when I switched to an older branch, ran ant clean-idea idea, then switched back to a newer branch and did the same thing.

I fixed the problem in IntelliJ in the original repo by ""importing"" the missing module using its own existing .iml file."
LUCENE-7810, Hoss Man,29/Apr/17 02:10,"The attached patch includes a trivial test case demonstraing the bug I found – but this only executes one code path of JoinUtils.createJoinQuery(...) – there are almost certainly other code paths with similar bugs that should also be tested. (see nocommit comments in test)

A few NOTEs:


	At present, due to some code I don't really understand in how Solr only leverages JoinUtils in rewritten queries, it appears that this bug does not impact current Solr usecases.  The patch also includes a trivial test showing that the ""wrapper"" queries solr creates around JoinUtils don't have this same equality problem
	i discovered this bug purely by fluke because in my originally POC code for SOLR-10583 I used JoinUtils.createJoinQuery(...) directly instead of refactoring Solr's JoinQParserPlugin code so i could re-use it – doing that refactoring is my nextstep for that issue, so I won't personally be pursuing fixing this bug (particularly since i'm not really sure what a good fix should look like)"
LUCENE-7810, Hoss Man,29/Apr/17 19:48,"updated summary that i realize now was value/missleading about the ""false positive"" situation."
LUCENE-7810, Adrien Grand,05/May/17 13:41,"+1 this is a bug. Actually the join query does not even matter for equality since it does not impact the matching hits. This query should only check the field (through its parent MultiTermQuery) and the list of the terms to match.

I can add it to my TODO list if nobody else has cycles."
LUCENE-7810, Hoss Man,05/May/17 18:18,"Thanks for sanity checking me ... i was worried i must have been overlooking something obvious in the intended usage of JoinUtils.

+1 this is a bug. Actually the join query does not even matter for equality ... This query should only check ... list of the terms to match.

you mean the from query, correct?

FWIW: I gather the reason that was used at all is because there's no easy easy/efficient way to do an equality check directly on the BytesRefHash containing the terms"
LUCENE-7810, Martijn van Groningen,12/May/17 14:26,"Good catch Hoss Man! If nobody is working on this then I can fix this bug. So far only the TermsQuery seems to not take into account the join field.

you mean the from query, correct?

I think this is what Adrien Grand means, because equality checking the collected terms would be too expensive. 

I think the TermsIncludingScoreQuery, TermsQuery, PointInSetIncludingScoreQuery and PointInSetQuery should also take index reader context id into like the GlobalOrdinalsQuery is doing. Otherwise the fromQuery + join field key can still be invalid. (mainly when docs on the from side added)"
LUCENE-7810, Martijn van Groningen,15/May/17 06:44,Added patch based on Hoss Man's patch that adds more tests and fixes equals() method in TermsIncludingScoreQuery and TermsQuery. The global ordinal based queries did already implement equals() correctly and the numeric join's equals() method was also working correctly because it is comparing the actual collected points.
LUCENE-7810, Adrien Grand,16/May/17 15:32,"I actually meant the BytesRefHash in my previous comment as comparing only the from query could still lead to false positives on different index readers that have segments in common. However Martijn's idea to take the index reader context identifier into account in equals/hashCode is better I think as it would make comparisons faster.

These queries should also take the score mode into account for equals/hashCode I think? I read your comment about the fact that it is not needed for query caching, but I think two queries should only be equal if they matche the same docs and give them the same scores. If we want to be able te reuse cache entries that have different score modes, we could rewrite to a TermsQuery in createWeight, similarly to how BooleanQuery rewrites all MUST clauses into FILTER clauses when needsScores is false?


ScoreMode scoreMode1 = scoreModes.toArray(new ScoreMode[0])[random().nextInt(scoreModes.size())];



I think you could do RandomPicks.randomFrom(random(), scoreModes).

Otherwise +1"
LUCENE-7810, Martijn van Groningen,17/May/17 14:12,">  If we want to be able te reuse cache entries that have different score modes, we could rewrite to a TermsQuery in createWeight, similarly to how BooleanQuery rewrites all MUST clauses into FILTER clauses when needsScores is false?

Good idea. I'll make this change."
LUCENE-7810, Martijn van Groningen,19/May/17 15:03,Adrien Grand I've updated the patch. Score mode is now taken into account in equals(...) and hashcode(...) methods and in case a scoring query is used when no scores are needed then it the query gets replaced with the non scoring variant.
LUCENE-7810, Adrien Grand,19/May/17 15:38,+1 maybe just replace query.rewrite(IndexReader) with searcher.rewrite(query) when you rewrite in createWeight since the latter guarantees to rewrite recursively until the query is fully rewritten
LUCENE-7810, Hoss Man,23/May/17 16:31,"Looks like jira-bot missed the 6x backport commit...

http://git-wip-us.apache.org/repos/asf/lucene-solr/commit/e30fbd68"
LUCENE-7810, Adrien Grand,23/May/17 16:40,Should we backport to branch_6_6 since we are re-spinning?
LUCENE-7810, Martijn van Groningen,23/May/17 19:10,The change has been backported to 6.6 branch too now.
LUCENE-7814, David Smiley,03/May/17 04:48,"RandomizedTesting FTW; eventually a year in this millennia turned up with the right circumstances   It could have been made more likely by some point (time instance) optimizations I'm working on now which will get its own issue, plus some crude beasting.  

Here's a patch.  

The only pertinent part of DateNRStrategyTest is the new test method testLastMillennia; the rest is related to some WIP."
LUCENE-7817, Adrien Grand,05/May/17 13:07,"Thanks for catching it! The change looks good, would you like to work on a test as well?"
LUCENE-7817, Christoph Kaser,05/May/17 13:56,Thanks for the review! I added a test for nullness to TestLRUQueryCache.testFineGrainedStats and pushed it into the PR.
LUCENE-7817, Christoph Kaser,10/May/17 09:50,"Is there anything else missing I can add? 
If possible (and sensible), i would really like to get this into the next lucene version because it causes problems in our code which I solve by manually patching the LRUQueryCache."
LUCENE-7817, Adrien Grand,11/May/17 14:05,"Sorry, I have been busy with other things over the last few days. I just merged your patch, thank you! It should be included in the upcoming 6.6 release. We'll soon start the release process so it should be a matter of a week or two before it is available."
LUCENE-7817, Christoph Kaser,11/May/17 14:10,"Perfect, thank you!"
LUCENE-7817, Hoss Man,22/May/17 17:25,"Note: adrien did followup commits fixing CHANGES.txt...

http://git-wip-us.apache.org/repos/asf/lucene-solr/commit/85167838
http://git-wip-us.apache.org/repos/asf/lucene-solr/commit/fb865507
http://git-wip-us.apache.org/repos/asf/lucene-solr/commit/2dadd84f"
LUCENE-7818, Amrit Sarkar,07/May/17 09:41,"That's a legit code bug, wonder how it was not reported till now."
LUCENE-7818, Amrit Sarkar,07/May/17 09:42,"BTW this is part of LUCENE than SOLR, it should be reported in Lucene project."
LUCENE-7818, Amrit Sarkar,07/May/17 09:46,Seems like QueryNodeOperation.java is never referred and used anywhere.
LUCENE-7818, Amrit Sarkar,07/May/17 09:48,Already listed: LUCENE-5365
LUCENE-7818, Uwe Schindler,07/May/17 11:13,I marked this as a duplicate of LUCENE-5365. Issue closed.
LUCENE-7821, Hoss Man,11/May/17 00:07,patch with simple test demonstrating problems
LUCENE-7821, Steve Rowe,12/May/17 00:18,"Patch with beefed up tests and fixes for classic query parser, flexible query parser, and Solr's standard/""lucene"" query parser.

I think it's ready to go."
LUCENE-7821, Yonik Seeley,12/May/17 00:27,"nice catch! +1 to the syntax of 

[<something> TO <something>]


I don't see any changes to the grammar files in the patch?"
LUCENE-7821, Steve Rowe,12/May/17 00:32,"I don't see any changes to the grammar files in the patch?

Hoss'ss patch is test-only.  Mine includes grammar file changes."
LUCENE-7821, Yonik Seeley,12/May/17 00:56,"Right you are... I previously searched visually and then for "".jj"", but I must have had a typo."
LUCENE-7821, Steve Rowe,12/May/17 19:20,"If there are no objections, I'll commit this tomorrow, including to 6.6."
LUCENE-7822, Michael McCandless,11/May/17 08:27,"+1

Maybe make a patch and post here?  Thanks."
LUCENE-7822, Adrien Grand,11/May/17 13:48,"For such a small file, maybe we should call eg. CodecUtil.checksumEntireFile before starting to read it. This would give a clearer error message?"
LUCENE-7822, Michael McCandless,12/May/17 14:31,"For such a small file, maybe we should call eg. CodecUtil.checksumEntireFile before starting to read it. This would give a clearer error message?

+1"
LUCENE-7822, Martin Amirault,16/May/17 02:09,"Created a patch for master branch, along with a test to reproduce the problem.

Actually the code doing the checksum is here with CodecUtil.checkFooter but is not called in a finally clause. Using it in a finally clause would hide some exceptions, so I decided to go with Adrien Grand suggestion"
LUCENE-7822, Adrien Grand,19/May/17 09:07,"I'm not happy with swallowing the IAE entirely. What about doing something like this:


diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java b/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
index e463259..677152c 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -289,11 +289,18 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
     long generation = generationFromSegmentsFileName(segmentFileName);
     //System.out.println(Thread.currentThread() + "": SegmentInfos.readCommit "" + segmentFileName);
     try (ChecksumIndexInput input = directory.openChecksumInput(segmentFileName, IOContext.READ)) {
+      Throwable priorE = null;
+      SegmentInfos infos = null;
       try {
-        return readCommit(directory, input, generation);
+        infos = readCommit(directory, input, generation);
       } catch (EOFException | NoSuchFileException | FileNotFoundException e) {
-        throw new CorruptIndexException(""Unexpected file read error while reading index."", input, e);
+        priorE = new CorruptIndexException(""Unexpected file read error while reading index."", input, e);
+      } catch (Throwable t) {
+        priorE = t;
+      } finally {
+        CodecUtil.checkFooter(input, priorE);
       }
+      return infos;
     }
   }
 
@@ -399,8 +406,6 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
 
     infos.userData = input.readMapOfStrings();
 
-    CodecUtil.checkFooter(input);
-
     // LUCENE-6299: check we are in bounds
     if (totalDocs > IndexWriter.getActualMaxDocs()) {
       throw new CorruptIndexException(""Too many documents: an index cannot exceed "" + IndexWriter.getActualMaxDocs() + "" but readers have total maxDoc="" + totalDocs, input);



This would add the CorruptIndexException as a suppressed exception to the IAE so that it is clear this exception was caused by a corruption? It would also be aligned with eg. how Lucene70SegmentInfoFormat (which also calls Version.fromBits) handles unexpected exceptions."
LUCENE-7822, Martin Amirault,19/May/17 09:41,"I did not know about the checkFooter(ChecksumIndexInput in, Throwable priorException) method, looks indead the best to use it.

Please check these 2 points:

	org.apache.lucene.index.SegmentInfos#readCommit(Directory, ChecksumIndexInput, long) is used inside org.apache.lucene.replicator.nrt.ReplicaNode probably need to check the footer there as well
	org.apache.lucene.index.SegmentInfos#readCommit(Directory, ChecksumIndexInput, long) do not need anymore to take a ChecksumIndexInput as paramater, a IndexInput is now enough



Maybe doing the CodecUtil.checkFooter in a finally inside the org.apache.lucene.index.SegmentInfos#readCommit(Directory, ChecksumIndexInput, long) would be better ?"
LUCENE-7822, Adrien Grand,19/May/17 10:49,"Maybe doing the CodecUtil.checkFooter in a finally inside the org.apache.lucene.index.SegmentInfos#readCommit(Directory, ChecksumIndexInput, long) would be better ?

+1

org.apache.lucene.index.SegmentInfos#readCommit(Directory, ChecksumIndexInput, long) do not need anymore to take a ChecksumIndexInput as paramater, a IndexInput is now enough

Maybe I'm missing something but you still need a ChecksumIndexInput since checkFooter needs one?"
LUCENE-7822, Martin Amirault,22/May/17 00:27,"Maybe I'm missing something but you still need a ChecksumIndexInput since checkFooter needs one?
In the fix you suggested checkFooter was moved out of the org.apache.lucene.index.SegmentInfos#readCommit(Directory, ChecksumIndexInput, long) method, in which case parameter type could be changed as IndexInput.


This would add the CorruptIndexException as a suppressed exception to the IAE so that it is clear this exception was caused by a corruption? It would also be aligned with eg. how Lucene70SegmentInfoFormat (which also calls Version.fromBits) handles unexpected exceptions
From my point of view if the checksum do not match, a CorruptIndexException should be thrown, with exceptions possibly resulting from it set as suppressed exceptions, eg behaves as if the file checksum was done before actually parsing the file content.
In that case if the si file is small reading it twice is the easiest: first time for checksum using CodecUtil.checksumEntireFile as originally suggested, then parse the content.

I agree this is not efficient, maybe the behavior of checkFooter method could be changed ? :

	behave as current one if the Throwable is not an Exception
	Otherwise throws a CorruptIndexException with original exception add as a suppressed one



Anyway I do not have an overview of the whole source code so I let you have the last word"
LUCENE-7822, Robert Muir,22/May/17 00:36,"The test is not correct. Changing the contents of a file may not necessarily change the checksum: corruption cannot always be detected.

I don't think lucene should throw a different exception here, besides the fact it wouldnt be 100%, I don't think it should try so much to ""interpret"" what has happened and make this decision, its not the responsibility of the library. I don't think there should be catch X, rethrow Y, but instead just deliver original exceptions to the user when things are wrong. Otherwise debugging becomes too hard.

checkFooter exists as a compromise: it does not change the original exception in any way, but just adds some additional information to it, to help a user understand perhaps why it may have happened."
LUCENE-7822, Martin Amirault,22/May/17 00:51,"I see. 
Concerning test, I though altering a single byte would always be detected by a CRC check, maybe I am missing something ?"
LUCENE-7822, Robert Muir,22/May/17 01:02,"i would be careful making assumptions that rely upon the checksum being used. 

CRC32 happens to be used today because of the two choices in the JDK it is actually the fastest, but it might not always be the case in the future (I think they may have improved performance of these algorithms in jdk9/added new CRC with different polynomial, etc).

Like i said, I think the current exception is correct. I also feel the same way about LUCENE-7592: why in the world did we change a more specific exception to a generic one??? that is just not right. we should deliver the best exception possible and make the callers catch/rethrow if they want to ""change"" them: its an API."
LUCENE-7824, Matt Weber,12/May/17 14:22,Jim Ferenczi Maybe use a BytesRefHash and maintain a id-to-hash map so we still only have single copy of common term in memory and still have a unique id?
LUCENE-7824, Jim Ferenczi,12/May/17 16:01,I don't think we should try to optimize here. The number of terms should be small in a query so I would prefer to keep it simple and just create a new entry for each token like the cached token stream does.
LUCENE-7824, Matt Weber,12/May/17 16:08,"Sure, looks good then!"
LUCENE-7824, Jim Ferenczi,14/May/17 19:16,Thanks Matt Weber
LUCENE-7830, Steve Rowe,16/May/17 01:26,I think this is a duplicate of LUCENE-6438 - I put a patch up there a ways back.
LUCENE-7830, Ishan Chattopadhyaya,16/May/17 01:51,"Indeed, I had copied the working tree from another machine which caused the stale symlinks."
LUCENE-7831, Adrien Grand,16/May/17 09:43,Here is a patch.
LUCENE-7833, selckin,18/May/17 07:53,Adrien Grand https://github.com/apache/lucene-solr/blob/master/lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java#L315
LUCENE-7833, Adrien Grand,18/May/17 12:56,Good catch selckin! Would you like to work on a patch + test?
LUCENE-7833, selckin,18/May/17 13:31,"Original poster found it and talked about it on irc with a few of us, i just wanted to highlight it would be an easy fix so it doesn't get lost

I believe he said he was interested in creating a test for it"
LUCENE-7833, Bernhard Altendorfer,18/May/17 13:47,"I would love to try it, but unfortunately I don't have time .."
LUCENE-7833, Adrien Grand,18/May/17 14:13,"No worries, I'll take it."
LUCENE-7833, Adrien Grand,18/May/17 15:11,Here is a patch.
LUCENE-7833, Jim Ferenczi,18/May/17 15:17,"patch looks good, +1"
LUCENE-7836, Michael McCandless,18/May/17 10:43,"I suspect the problem is ""Hello World"" was indexed as 2 tokens, since you're indexing with whitespace tokenization, which means the regexp will never match a single token.

If you indexed ""Hello World"" as a single token then the regexp should match."
LUCENE-7838, David Smiley,26/May/17 20:15,Ah... you added a dependency on the sandbox module from another module.  That's quite surprising to me... I don't think that's legit?  New inter-module dependencies (of any kind) I think should also deserve communication on the JIRA issue and I don't see any mention here.  I also don't see a CHANGES.txt entry.  I don't see a patch file either but I admit I welcome that
LUCENE-7838, Tommaso Teofili,27/May/17 06:38,"you added a dependency on the sandbox module from another module. That's quite surprising to me...  I don't think that's legit?

why? As soon as we provide releases of lucene-sandbox I assume we expect people and other modules to use it.

New inter-module dependencies (of any kind) I think should also deserve communication on the JIRA issue and I don't see any mention here.

Since this is only impacting master branch I had thought there was no need to explicitly mention that; on the other hand FuzzyLikeThisQuery lives in sandbox therefore I had assumed there was no need to explicitly specify that in the issue.

I also don't see a CHANGES.txt entry

right, there's no such entry.

I don't see a patch file either but I admit I welcome that 

I'm not sure I get your point here, would you have expected a patch ?"
LUCENE-7838, Adrien Grand,30/May/17 09:26,"I agree with David we should be careful about adding new dependencies. Otherwise things can quickly become hairy, eg. because of circular dependencies, or because pulling a single module would pull most other modules through transitive dependencies, which defeats the purpose of having modules. Dependeng on sandbox makes it ever worse since the barrier for adding/removing code is supposed to be low, yet this new dependency means that special care needs to be taken if we want to remove FuzzyLikeThis."
LUCENE-7838, David Smiley,30/May/17 19:01,"CHANGES.txt:
I guess I need to be clearer.  Why isn't there a CHANGES.txt entry?  Beyond mentioning what the title says, mentioning the new dependency would be appropriate (required IMO).

patch

Nevermind; you were going the CTR path (which I welcome) instead of RTC.  CTR is outside our defacto norms of behavior here.  Maybe I should follow suit and we will try to change that"
LUCENE-7838, Tommaso Teofili,31/May/17 15:09,as per related thread on dev@ I'll drop the dependency over the sandbox module which is indeed not appropriate. If possible I'd like to keep the classifier but I'd not just copy paste the FLT code from sandbox to classification therefore it'll take a bit of time to tweak it as needed.
LUCENE-7838, Tommaso Teofili,29/Jun/17 08:03,"I've removed the dependency on the sandbox module and created a dedicated version of FLT named NearestFuzzyQuery in org.apache.lucene.classification.utils package.
The goal now is to refine NearestFuzzyQuery in order to get better classification results and remove some specifics of FLT."
LUCENE-7838, Tommaso Teofili,05/Jul/17 11:11,"I'm marking this as resolved, improvements will come in subsequent issues."
LUCENE-7842, Dawid Weiss,22/May/17 10:46,"Apologies, I think it's my fault."
LUCENE-7847, Adrien Grand,23/May/17 10:24,I found this while digging LUCENE-7807. Here is a patch but I'm still looking for ways to make tests more likely to expose such issues. Separately I tend to find it hard to reason about all possible range types in the compare logic. I think it'd be easier to read if we had a separate method for each query type?
LUCENE-7847, Adrien Grand,23/May/17 14:29,"Here is an updated patch. I make the tiny case run multiple times since it is cheap, and changed random value generation so that it is more likely to trigger ""interesting"" cases, such as a query range that is equal to the indexed range. This found a test bug in the ip range test, which used Object.equals instead of Arrays.equals to compate two byte[]."
LUCENE-7847, Nicholas Knize,23/May/17 16:29,"+1 
Love the simplicity of the scorer and thanks for making the test more evil!"
LUCENE-7848, Dawid Weiss,23/May/17 15:27,"Thanks Jim. I agree the output from the filter looks odd too, but that's probably another issue. If you wish to reproduce, the Solr config I used was this:

      <filter class=""solr.WordDelimiterGraphFilterFactory""
              preserveOriginal=""1""
              generateWordParts=""1""
              generateNumberParts=""1""
              stemEnglishPossessive=""1"" />


I then reproduced the same from code as well (just pass the corresponding flags and use a whitespace tokenizer). I don't have access to that code right now, but I can attach it later if you can't reproduce it yourself."
LUCENE-7848, Erik Hatcher,23/May/17 15:50,"I hit a snag with QueryBuilder#createSpanQuery too, and created (for the SOLR-1485 work) org.apache.solr.util.PayloadUtils with a createSpanQuery method.   It currently also doesn't take gaps into account (but the basic use cases don't involve sophisticated analysis there, so it was intentional to keep it initially simple), but I did have to work through some Lucene analysis API hurdles that I think QueryBuilder's createSpanQuery should fix along the way too.

See my comment and implementation here: https://github.com/apache/lucene-solr/blob/5d42177b9290b61c658154e42223408944cd4bc1/solr/core/src/java/org/apache/solr/util/PayloadUtils.java#L106-L128"
LUCENE-7848, Dawid Weiss,26/May/17 10:35,"Analyzer a4 = new Analyzer() {
      @Override
      public TokenStreamComponents createComponents(String field) {
        final int flags2 = 
            GENERATE_WORD_PARTS | PRESERVE_ORIGINAL | GENERATE_NUMBER_PARTS | STEM_ENGLISH_POSSESSIVE;
        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
        StopFilter filter = new StopFilter(tokenizer, StandardAnalyzer.STOP_WORDS_SET);
        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(filter, flags2, protWords));
      }
    };

    String in = ""aaa,bbb foo - bar"";
    PrintWriter pw = new PrintWriter(System.out);
    new TokenStreamToDot(in, a4.tokenStream("""", in), pw).toDot();
    pw.flush();



Here's the analyzer chain that I used for testing. I can't provide a full test – it was just an ad-hoc hacking session."
LUCENE-7848, Jim Ferenczi,15/Jun/17 08:56,"Here is a simple patch that support gaps in QueryBuilder#createSpanQuery and QueryBuilder#analyzeGraphPhrase.
QueryBuilder#createSpanQuery could also handle zero increment but that's probably another issue."
LUCENE-7848, Dawid Weiss,16/Jun/17 17:16,"On short holidays, Jim, but I'll try to review next week. Looking quickly through the patch it looks great!"
LUCENE-7848, Dawid Weiss,19/Jun/17 10:09,"Hi Jim,

The patch looks ok, although it doesn't solve the original problem – why, I don't know. This is the query in Solr:

funding_program:""SPECIAL PROJECTS - XXX,SPECIAL PROJECTS - YYY""



This (in Solr) gets translated into:

+SpanNearQuery(
  spanNear([
    funding_program:special, 
    funding_program:projects, 
    funding_program:-, 
    spanOr([
        spanNear([SpanGap(:1), funding_program:xxx,special], 0, true), 
        spanNear([SpanGap(:1), funding_program:xxx, funding_program:special], 0, true)
    ]), 
    funding_program:projects, 
    funding_program:-, 
    SpanGap(:1), 
    funding_program:yyy], 0, true))



Those odd-looking span gaps are emitted by the WordDelimiterGraphFilter (with the flags above); virtually the same config is used for indexing, but the query doesn't match the indexed content. A code-based test would be much better to pinpoint the problem here. I'll try to provide one."
LUCENE-7848, Dawid Weiss,19/Jun/17 10:34,"Here's a test (testLucene7848) that reproduces the behavior observed in Solr. To me this should work (right)?

I didn't take a look at token streams emitted vs. the query yet – have to switch context now, but it'd be a good start to figure out what's happening."
LUCENE-7848, Dawid Weiss,19/Jun/17 10:39,Token graph for the input (indexing and search is the same).
LUCENE-7848, Dawid Weiss,19/Jun/17 11:11,"Couldn't resist not to look. The index is fine. The query seems to be off though – here:

spanNear([field:SPECIAL, 
          field:PROJECTS, 
          field:-, 
          spanOr([spanNear([SpanGap(:1), 
                            field:xxx,SPECIAL], 0, true), 
                  spanNear([SpanGap(:1), 
                            field:xxx, 
                            field:SPECIAL], 0, true)]), 
          field:PROJECTS, 
          field:-, 
          SpanGap(:1), 
          field:yyy], 0, true)



The problem is in those gaps inside spanOr – the position increments get screwed up somehow. I created the above query manually and this one works just fine:

        Query q = SpanNearQuery.newOrderedNearQuery(field)
            .addClause(new SpanTermQuery(new Term(field, ""SPECIAL"")))
            .addClause(new SpanTermQuery(new Term(field, ""PROJECTS"")))
            .addClause(new SpanTermQuery(new Term(field, ""-"")))
            .addGap(1)
            .addClause(new SpanOrQuery(
                SpanNearQuery.newOrderedNearQuery(field)
                  .addClause(new SpanTermQuery(new Term(field, ""xxx,SPECIAL"")))
                  .addGap(1)
                  .build(),
                SpanNearQuery.newOrderedNearQuery(field)
                    .addClause(new SpanTermQuery(new Term(field, ""xxx"")))
                    .addClause(new SpanTermQuery(new Term(field, ""SPECIAL"")))
                    .build()
            ))
            .addClause(new SpanTermQuery(new Term(field, ""PROJECTS"")))
            .addClause(new SpanTermQuery(new Term(field, ""-"")))
            .addGap(1)
            .addClause(new SpanTermQuery(new Term(field, ""yyy"")))
            .build();



Note the leading gap is pulled outside, that's pretty much all the difference (another addGap(1) inside the ""xxx,SPECIAL"" condition is irrelevant here).

Could be a bug somewhere in span queries."
LUCENE-7848, Jim Ferenczi,19/Jun/17 12:04,"Hi Dawid,
Sorry I am also on vacations this week but looking at your example it seems that it's a problem with graph token in general. If you have side paths with different length at indexing time you need to use the flatten graph filter. Though it will not be able to index the correct positions for this example since ""xxx,special"" and ""xxx"", ""special"" should be indexed as a graph and Lucene does not handle graph at indexing time. I wonder why your manual query works, I might be missing something but this query should also not work unless you used another configuration for the WDGF (preserve original = false for instance should work at indexing time) ?"
LUCENE-7848, Dawid Weiss,19/Jun/17 12:22,"This isn't urgent and can wait. 

As for indexing graphs, I though it was the other way around, sorry about that. But even if, the index created for this particular input seems to be just fine, look:

field field
  term -
    doc 0
      freq 2
      pos 2
      pos 7
  term PROJECTS
    doc 0
      freq 2
      pos 1
      pos 6
  term SPECIAL
    doc 0
      freq 2
      pos 0
      pos 5
  term xxx
    doc 0
      freq 1
      pos 4
  term xxx,SPECIAL
    doc 0
      freq 1
      pos 4
  term yyy
    doc 0
      freq 1
      pos 9 



In fact, adding a flattening filter doesn't actually change anything here: the same index (and query) is produced."
LUCENE-7848, Michael Gibney,12/Jul/17 13:24,"""Could be a bug somewhere in span queries.""^ – I think the remaining problem here is that only one branch (the shortest) of a SpanOrQuery is evaluated, at which point the ""spanOr"" is designated a match (or not) of the width/positionEnd of the shortest branch. When the branches of a ""spanOr"" differ in length (as they will as a matter of course for uses of GraphFilters such as in the above test), the shorter branch is evaluated, but if a longer branch is also a match, it affects the offset of subsequent tokens, and the enclosing ""spanNear"" sees a larger-than-expected slop, and fails to match. 

LUCENE-7848-branching-spanOr.patch adjusts SpanOrQuery to support repeated calls to nextStartPosition() which return the same startPosition, but different endPositions. The subSpan clauses of the ""spanOr"" are popped off the priorityQueue, retained, and restored upon exhaustion of subSpans (when it's time to move on to the next potential match). Some corresponding changes were necessary to make NearSpansOrdered aware of the new ""spanOr"" behavior, and conditionally evaluate as many branches of ""spanOr"" clauses as necessary to match (or not) on the full ""nearSpan"".

There may be other modifications needed in code that can call the modified ""spanOr"" and would need to be aware of its new behavior, but with this patch applied, all the tests in the TestWordDelimiterGraphFilter pass (including the new testLucene7848()). 

EDIT: original patch had a bug, was re-uploaded a few hours after initially posted."
LUCENE-7848, Tim Allison,12/Jul/17 20:01,"Could be a bug somewhere in span queries.

Related to LUCENE-7398?"
LUCENE-7848, Jim Ferenczi,13/Jul/17 12:06,"Dawid, sorry for the delay


spanNear([field:SPECIAL, 
          field:PROJECTS, 
          field:-, 
          spanOr([spanNear([SpanGap(:1), 
                            field:xxx,SPECIAL], 0, true), 
                  spanNear([SpanGap(:1), 
                            field:xxx, 
                            field:SPECIAL], 0, true)]), 
          field:PROJECTS, 
          field:-, 
          SpanGap(:1), 
          field:yyy], 0, true)



The problem is in those gaps inside spanOr – the position increments get screwed up somehow. I created the above query manually and this one works just fine:

        Query q = SpanNearQuery.newOrderedNearQuery(field)
            .addClause(new SpanTermQuery(new Term(field, ""SPECIAL"")))
            .addClause(new SpanTermQuery(new Term(field, ""PROJECTS"")))
            .addClause(new SpanTermQuery(new Term(field, ""-"")))
            .addGap(1)
            .addClause(new SpanOrQuery(
                SpanNearQuery.newOrderedNearQuery(field)
                  .addClause(new SpanTermQuery(new Term(field, ""xxx,SPECIAL"")))
                  .addGap(1)
                  .build(),
                SpanNearQuery.newOrderedNearQuery(field)
                    .addClause(new SpanTermQuery(new Term(field, ""xxx"")))
                    .addClause(new SpanTermQuery(new Term(field, ""SPECIAL"")))
                    .build()
            ))
            .addClause(new SpanTermQuery(new Term(field, ""PROJECTS"")))
            .addClause(new SpanTermQuery(new Term(field, ""-"")))
            .addGap(1)
            .addClause(new SpanTermQuery(new Term(field, ""yyy"")))
            .build();



These two queries are valid and should return result. The first one represents exactly the graph produced by the WordDelimiterGraphFilter and the second one has an extra gap after ""xxx,SPECIAL"". This extra gap is not irrelevant, it's the only way to match the indexed form of the document with the path containing the term ""xxx,SPECIAL"". If you look at the indexed positions ""xxx,SPECIAL"" is at position 4 and position 5 has the term ""SPECIAL"". This is the flattened version of the graph but the query side builds the correct version and ignores that the positions are messed up by the indexer. If you add a manual gap then it allows ""xxx,SPECIAL"" to also ignore the next position (5, SPECIAL) and to jump directly to (6, PROJECTS).
Though the other path containing the splitted terms ""xxx"" and ""SPECIAL"" should match on both queries. I think this is the real problem and the fact that the second query match is just due to the additional gap that you added.
I don't have time at the moment to look at why the SpanQuery does not match the first query. It deserves a separate issue anyway so I think we should focus on whether the query produced by the QueryBuilder is valid or not. If it is then the patch can be merged and we can look at the other problem separately.
Michael Gibney can you open a new issue or add your comment and patch to https://issues.apache.org/jira/browse/LUCENE-7398 ? We should focus on the query building in this issue first."
LUCENE-7848, Dawid Weiss,14/Jul/17 08:19,"Hi Jim. Thanks for the analysis – I do understand these two queries should be identical, but they have a different match result – that's why I thought it's probably a span query issue rather than the builder's (whether you pull those gaps or push them inside the or shouldn't matter).

This time I'm on holidays, but I'll keep looking at LUCENE-7398, perhaps it sheds some light on what's going on."
LUCENE-7848, Michael Gibney,14/Jul/17 14:19,I think the remaining problem is that WordDelimiterGraphFilter is swallowing delim-only tokens and leaving a gap even when PRESERVE_ORIGINAL is true. LUCENE-7848-delimOnly-token-offset.patch fixes this (and addresses the problematic gaps).
LUCENE-7849, David Smiley,24/May/17 15:18,"Please assign to 6.6, not branch_6x; thanks.  I think we (the Lucene project) should work to remove ""branch_6x"" and ""6.x"" as JIRA versions."
LUCENE-7849, Karl Wright,24/May/17 16:28,"David Smiley, what branch in git is that?"
LUCENE-7849, David Smiley,24/May/17 18:19,"We assign fix versions for the version it will be released for.  It's atypical that there is a branch for a specific version at the time an issue is closed.  The release manager for a creates the branch (like branch_6_5 pattern) at the time of a feature freeze.

Woops! I realized I asked you to assign to 6.6 which is the release in-progress.  This is a bug fix so you could in fact port to branch_6_6  but since you opted not to, you can set the fix version to 6.7

(BTW I re-assigned the only other issue from branch_6x version and I then deleted that JIRA version)."
LUCENE-7851, Robert Muir,24/May/17 22:19,"lookupTerm has nothing to do with setDocument, because it looks for a term in the ""term dictionary"" and returns the ordinal for that term. That term dictionary is shared across the whole segment.

example docs and values:



doc
value(s)


0
dog


1
cat


2
dog,cat,cat




this is what the docvalues looks like


doc
ords
comment


0
1


1
0


2
0,1*





	as you see, this fieldtype loses both the original order (it was dog, cat) and frequency (cat was there twice) because its a SortedSet.



this is what the ""term dictionary"" looks like:


ord
term


0
cat


1
dog




lookupTerm(dog) is always 1, regardless of which document in the segment its in. its 1 because it sorts after ""cat"". The values are deduplicated across all documents in the segment in this way."
LUCENE-7851, Vesa Pirila,24/May/17 23:00,"Thank you for your clarification Robert Muir.

My original issue is actually that I'm trying to work around the lack of maps in Solr and have ""serialized"" one key-value pair into each string value in the multi-valued field. I would like to find the correct key-value pair using an O(log) or even an O(1) search rather than iterating over all the document's values for that field, and return the found value. What would be the best forum to seek advice for this kind of a requirement? Stack Overflow?"
LUCENE-7851, Robert Muir,24/May/17 23:19,"If you are searching all documents for a string value (does not matter if it contains a key=value thingy that you put in it), then the best way does not involve docvalues at all, it is to index those key-value pairs as terms in the inverted index and search on them with TermQuery.

But maybe you have different requirements or something I am missing.

The best forum for these kinds of questions is the java-user@lucene.apache.org mailing list. You can find instructions here: http://lucene.apache.org/core/discussion.html#java-user-list-java-userluceneapacheorg"
LUCENE-7865, Robert Muir,06/Jun/17 21:24,"You need to run ant clean.

The constant value is 175, so AIOOBE with 170 is impossible unless your compile/runtime icu versions don't match: your classes were compiled with an older version.

This is also why it does not reproduce for me.

The constant is deprecated actually, because of this reason (since it grows over time and runtime version may be different). We don't need to do anything fancy about that, we just support the version we test against and thats it."
LUCENE-7865, David Smiley,07/Jun/17 03:03,Thanks for clarifying the situation Rob.
LUCENE-7869, Martijn van Groningen,07/Jun/17 17:54,Attached a patch with a fix and a test.
LUCENE-7870, Dawid Weiss,08/Jun/17 12:16,"I understand the issue, but somehow don't believe calling loadClass on the classloader directly is the correct fix here. Sure, it'll work (for you), but you're bypassing the ""right"" way of loading Java classes (and who knows what will happen if/when modules are out). It is OSGi's workaround to define a context class loader with context-sensitive, non-deterministic class resolution and I believe the fix should be up where this is causing a problem."
LUCENE-7870, Andreas Sewe,08/Jun/17 12:27,"Hi Dawid,

I understand the issue, but somehow don't believe calling loadClass on the classloader directly is the correct fix here. Sure, it'll work (for you), but you're bypassing the ""right"" way of loading Java classes (and who knows what will happen if/when modules are out).

I did a lot of digging, but couldn’t find any definitive information as to whether Class.forName(...) is more right than ClassLoader.loadClass(...). Any pointers are thus greatly appreciated.

I do see, however, that java.util.ServiceLoader (which Lucene tries to mimic/replace) uses Class.forName(...), so that may be a sign.

It is OSGi's workaround to define a context class loader with context-sensitive, non-deterministic class resolution and I believe the fix should be up where this is causing a problem.

Fair enough. We (Eclipse Orbit) have to do some tweaks to Lucene’s JARs anyway, as AFAIK Lucene still doesn't publish them with OSGi manifests."
LUCENE-7870, Dawid Weiss,08/Jun/17 12:58,"I recall this came as a surprise to me too, actually – I once discussed this with Jeroen Frijters (author of the .NET-based VM; IKVM) and recall he insisted calling ClassLoader.loadClass directly was a mistake. I can't exactly recall his arguments, but I believe they had to be as explained below.

The JVM spec declares that a pair (classloader, classname) is essentially loaded once [1], quoting:

First, the Java Virtual Machine determines whether L has already been recorded as an initiating loader of a class or interface denoted by N. If so, this class or interface is C, and no class creation is necessary.

Class.forName essentially follows this behavior (in fact, it's a native method call to the JVM). Having a non-deterministic class loader (one that returns a different result from loadClass makes the whole class resolution process non-deterministic... So I think that, even though this isn't stated explicitly (or at least I can't find it), ClassLoader.loadClass should be consistent and deterministic, always returning (and resolving internally) to the same Class instance.

Unfortunately I don't know enough about OSGi's internals to offer you a different workaround; could be that there's none.

[1] https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-5.html#jvms-5.3.2"
LUCENE-7870, Dawid Weiss,08/Jun/17 13:05,"One quick follow up. Read section 5.3 too: it declares the defining and initiating loader of a class. From this description it's clear that my initial intuition was very likely correct – the initiating loader may not be the defining one, but if a class name and initiating loader have been resolved, no another attempt will be made. So if it's not a deterministic process for mutliple classloaders with delegation then it's violating the spec (and hence the behavior you observed).

I'd be curious what the OSGi people think about it – they had to face the same spec issues at some point."
LUCENE-7870, Andreas Sewe,08/Jun/17 13:38,"Thank you for the explanation. ContentFinder is certainly not a well-behaved class loader as per the spec:

A well-behaved class loader should maintain three properties: Given the same name, a good class loader should always return the same Class object. [...]"
LUCENE-7870, Uwe Schindler,08/Jun/17 16:43,"Hi,

I do see, however, that java.util.ServiceLoader (which Lucene tries to mimic/replace) uses Class.forName(...), so that may be a sign.

That is exactly the reason why it is made like this. In general we would likely to get rid of the Context ClassLoader in the SPI lookups of Lucene (you see we fall back to using the JAR's classloader, for looking up). IMHO, we should never touch the context loader, but as Java's service loader does this, too, we kept it for compatibility. The main reason is broken webapp containers that have strange context loaders fishing shit from somewhere.

Nevertheless, this is a problem of Eclipse/OSGI, because if Lucene would use the ""original"" ServiceLoader, it would cause the same problems for you. And it that case you would not even be able to ""patch"" Lucene. So fix the problem correctly! (see below) I know SPI and OSGI don't like each other but looking at Java 9's module system, they have a perfect way to handle this. With later Lucene versions we will switch to Java 9's ServiceLoader, so you can declare the services in the module declaration.

I'd separate the context class loader for those 2 apps (and set it equal to the loader that loaded the bundle)."
LUCENE-7870, Uwe Schindler,08/Jun/17 17:19,"My idea would be (as stated before): Get rid of the Context Classloader in SPI lookups! Lucene never uses it, it is just there for backwards compatibility. The current setup of SPI does not work with modules of Java 9 and it does not work with stuff in completely different classloaders. So OSGI fails in any case, if you have lucene-core.jar and lucene-backwards-codecs.jar as OSGI modules, because both would use different loaders. The context loader won't help.

The problem is that we may break some apps that rely on the context loader traversal. In my opinion, we may add a system property that is read on setup of NamedSPILoader / SPIClassIterator that can be set to true (e.g. lucene.useContextLoaderForSPI, defaulting to false). This may fix legacy apps and new apps would only traverse the classloader that loaded lucene-core.jar.

For Java 9 and ""Lucene as Java 9 module"") we have to refactor this anyways, becaue we need to respect module-info,java and look for SPI exports."
LUCENE-7870, Uwe Schindler,08/Jun/17 17:23,"FYI: Context class loaders were the worst idea ever in Java. I personally hate them and I would do anything - just to make them disappear from the spec! When drinking beers with Mark Reinhold in Brussels, I am always reminding him about this together with the inability to unmap byte buffers..."
LUCENE-7870, Dawid Weiss,08/Jun/17 18:05,"I mark this as invalid then; I think we've reached the consensus that it's not a bug, rather a clash of two different unfortunate loopholes."
LUCENE-7870, Andreas Sewe,09/Jun/17 07:38,"I mark this as invalid then; I think we've reached the consensus that it's not a bug, rather a clash of two different unfortunate loopholes.

Fair enough, Dawid Weiss. FYI, Eclipse Orbit will then ship patched versions of Lucene 5.x and 6.x that use contextClassLoader.loadClass(...) and thus work with the Equinox ContextFinder, but those are just for Eclipse-internal consumption.

I'd separate the context class loader for those 2 apps (and set it equal to the loader that loaded the bundle).

Uwe Schindler, that solution won’t work in OSGi, as any bundle may create new threads whose call stacks may cross any number of bundle boundaries. The OSGi container thus cannot manage the context class loader for you. And wrapping every call into Lucene with a try-finally that sets and resets the context class loader is not solution either. 

The current setup of SPI does not work with modules of Java 9 and it does not work with stuff in completely different classloaders. So OSGI fails in any case, if you have lucene-core.jar and lucene-backwards-codecs.jar as OSGI modules, because both would use different loaders.

I am not sure whether you are aware of buddy classloading, an OSGi extension in Equinox. This would allow the lucene-backwards-codecs.jar bundle to register itself with the lucene-core.jar bundle so that the former’s classes are visible to the latter’s class loader. Alas, buddy classloading won’t solve the present issue, as NamedSPILoader will currently also use the thread context class loader (in addition to the class loader of lucene-core.jar in steps 1.1 and 2.1). And this leads to interference of steps 1.2 and 2.2, the first step locking in a class for the second.

For Java 9 and ""Lucene as Java 9 module"") we have to refactor this anyways, becaue we need to respect module-info,java and look for SPI exports.

That being said, I’d love to see what Lucene will do for Java 9. (From an OSGi perspective, it would also be great if, as part of this refactoring, Lucene could get rid off “split packages”, i.e., using the same package in multiple bundles. These are always a headache in OSGi. Maybe this is a good opportunity to revisit LUCENE-1511 again.)

Is there a good place to lurk to watch the ”Lucene as Java 9 module” effort? An issue to watch? A branch to track? If you are already making larger changes, it would be good to have an eye on them from an OSGi perspective as well."
LUCENE-7870, Uwe Schindler,09/Jun/17 08:21,"Is there a good place to lurk to watch the ”Lucene as Java 9 module” effort? An issue to watch? A branch to track? If you are already making larger changes, it would be good to have an eye on them from an OSGi perspective as well.

Not yet. There is no urgency about that. You may have seen the Java 9 developer panel at JAX 2017 (see videos). In general the Java 9 module system is fine for new developments, but porting old applications to be modules is going to be hard, especially because of the split package problem. This is unliekely to change fast in Lucene, sorry. My current recommendation is to make the whole Lucene application a single module and go for it.

I am not sure whether you are aware of buddy classloading, an OSGi extension in Equinox

This may help with the problem, but its still no standard way. OSGI is ""broken"" in that respect. Java 9 has the default option and shares class loaders by default (which has pros and cons). But I don't want to make this a Java 9 vs. OSGI discussion, I just mention this.

Alas, buddy classloading won’t solve the present issue, as NamedSPILoader will currently also use the thread context class loader (in addition to the class loader of lucene-core.jar in steps 1.1 and 2.1). 

Of course. I made a proposal already to skip the stupid context classloader for Lucene's SPI lookups and only allow to enable it for ""broken"" webapp context classloaders (unfortunately this would only be possible via system property, as this must be setup very early during lucene-core startup). Let's discuss this in another issue.

For your current problems: Even with module systems, I'd always prefer to Maven-Shade different versions of the same lib- I'd also use the same approach like I'd suggest in Java 9: Package everything needed from Lucene's classes into one OSGI bundle.

Finally, the Lucene does not offer OSGI bundles because of the split packages problem. Anybody who want to use Lucene should setup a trivial Maven projects that depends on all required Lucene JARs and after that uses Maven Shade plugin and the OSGI-BND plugin to create a encapsulated singleton out of it. We know, that's not ideal, but running Lucene inside OSGI is a bad idea anyways (huge performance issues). For desktop apps like Eclipse this may be fine, but not for the main Lucene use case: Server side applications using Lucene running in a separate JVM (Lucene does not coexist good with other stuff in the same JVM as it has very special garbage collection requirements and uses heap and off-heap resources in very unconventional ways). See Solr, Neo4J and Elasticsearch as typical examples."
LUCENE-7870, Uwe Schindler,09/Jun/17 08:29,See LUCENE-7873
LUCENE-7870, Uwe Schindler,20/Jun/17 14:48,"Hi Andreas,
this issue should be solved in Lucene 7.0. We will not backport this as it breaks backwards compatibility: We no longer use the context class loader for looking up services. It takes the class loader that defined the interface. If you require another one (e.g. in OSGI without using buddy classloading) you can call Codec.reloadCodecs(ClassLoader) and similar methods. See the MIGRATE.txt in Lucene 7."
LUCENE-7871, Mikhail Khludnev,08/Jun/17 13:12,"I started LUCENE-7835 from extracting common logic between numDV and sortedDV  LUCENE-7871.patch (it's a little bit harish), and faced false positive for sortedDV. I'll try incorporate Adrien Grand clue regarding ConjunctionDISI"
LUCENE-7871, Adrien Grand,09/Jun/17 06:35,I don't think extracting common logic helps in that case since it needs to introduce a new abstraction.
LUCENE-7871, Mikhail Khludnev,09/Jun/17 07:02,"Adrien Grand, here is LUCENE-7871.patch what I have so far. ToParentIterator is the new abstraction based on ConjunctionDISI. btw, is ConjunctionDISI steady? Can we assert that ConjunctionDISI.docID is always the same as docVals subiterator docID? I just worry that's not true for Disjunction's one (obviously)."
LUCENE-7871, Adrien Grand,09/Jun/17 07:17,"Correct, ConjunctionDISI is always on the same doc ID as its subs. Can you move ToParentIterator to the join package and make it pkg-private? I think you did things this way in order to be able to extend DocValuesIterator but actually you can just extend DocIdSetIterator and define an additional advanceExact method?"
LUCENE-7871, Mikhail Khludnev,09/Jun/17 08:42,"Can you move ToParentIterator to the join package and make it pkg-private? 

Thanks it went really well. LUCENE-7871.patch I also moved some its' guts inside. 
Now BlockJoinSelector's methods accept DocIdSetIterator children, keeping all BitSet children-accepting methods deprecated.

The question is, what we allow ToParentBlockJoinSortField to accept as children? Is it Query?  or we have some other abstract per-segment DISI <<factory>>?"
LUCENE-7871, Adrien Grand,09/Jun/17 09:18,"Since the patch is for 7.0 only, it is fine to break backward compatibility, so let's just remove the methods that take a bitset instead of deprecating them? hasValue and seen seem to store the same information, don't they? Finally I'm still not a fan of the ToParentIterator and Accumulator abstractions. I liked it better before, even if that meant a bit of duplication."
LUCENE-7871, Mikhail Khludnev,09/Jun/17 12:03,"hasValue and seen seem t..
Ok. Thanks. I've collapsed them.

I did is non backward compatible due to child Query. But turning child Query to DISI turned out soo hard. I had to reproduce ValueSource.ValueSourceSortField trick with weight and context map. But now ToParentBlockJoinSortField should be rewriten before searching. I find it not really convenient, but looks like it's what ValueSourceSortField users live with, see SolrIndexSearcher.weightSort(Sort) (I know),  and TestFunctionQuerySort as well. I wonder if we can do this simpler? 

Finally I'm still not a fan of the 
Thankfully it doesn't sound like veto. Does it? I renamed it to the package level ToParentDocValues and pack both twins (sorted and numerics) into it. So, we can think that internally this code is duplicated. 
I propose this OO-hairish stuff because the current duplicated code introduced the bug, and I'm afraid it's caused exactly by this duplication."
LUCENE-7871, Mikhail Khludnev,13/Jun/17 07:56,"Adrien Grand, can you give a feedback for API ie for supplying children as a Query?"
LUCENE-7871, Mikhail Khludnev,27/Jun/17 06:35,"LUCENE-7871.patch 

	fixes TestBlockJoinSelector.java for case when SortedDV has no vals for kids.
	extracts ToParentDocValues into hairish OO piece
	introduces BlockJoinSelector.wrap() acepting childred as DISI, no usage yet, existing are deprecated.
Is there any veto, request to hold on? or it's fine to push it forward?"
LUCENE-7871, Mikhail Khludnev,30/Jun/17 13:53,"..going, going"
LUCENE-7871, Adrien Grand,04/Jul/17 14:36,Thanks Mikhail Khludnev!
LUCENE-7871, Mikhail Khludnev,04/Jul/17 15:06,follow up SOLR-11006 for refactoring children to DISI.
LUCENE-7872, Adrien Grand,09/Jun/17 06:29,Here is a patch.
LUCENE-7872, Jim Ferenczi,09/Jun/17 10:33,"+1
The patch looks good. 
TopGroups needs this too but that's another issue."
LUCENE-7872, Hoss Man,15/Jun/17 22:24,"Presumably you're targeting this for 7.0 since it's a public API change?

I'm +1 to the idea ... but I feel like if we're going to make this change to TopDocs we should bite the bullet and make the equivalent change to Solr's DocList API (rather then coerce with Math.toIntExact).

I'll try to work on an updated patch tomorrow?"
LUCENE-7872, Hoss Man,16/Jun/17 04:14,"updated patch with solr tweaks.

the end user/solrj APIs already used ""long"" for numFound, so this is mainly just cleaning up some poor asumptions/casting (mostly in Spellchecker tests)

Would be nice if someone else could give the solr changes a sanity check?"
LUCENE-7872, Adrien Grand,16/Jun/17 07:20,"Hoss Man Doesn't DocList only represent results from a single shard? In that case, keeping an int would still work since the number of docs in a shard is still bound by the integer max value. We only have the issue with TopDocs because it is sometimes used to represent top results for the union of multiple shards because of TopDocs#merge."
LUCENE-7872, Hoss Man,16/Jun/17 16:41,"Doesn't DocList only represent results from a single shard?

yes, but given how closely used it is with TopDocs, and since the end user APIs are allready (supposed to be) ""long"" based, it seemed like a good idea to follow it through – and in doing so let to the problems/fixes in places like SpellCheckCollation where ""hits"" was an int even though it was used to model ""totalHits"" (for a spellcheck collation) across shards.  

(which is the type of problem i was suspicious/worried about finding in usages of DocList when i saw your first patch ... based on the changes you had to make to TopGroupsResultTransformer)

ie: maybe it isn't strictly necessary to change DocList.matches() to be long, but:

	doing so helped find problems with existing code making assumptions about ""total hits"" (across shards) and should hopefully prevent similar problems in future code as well
	now seems like as good a time as any to make this change given how closely connected solr's usage of DocList/DocSlice is with TopDocs."
LUCENE-7872, Adrien Grand,21/Jun/17 07:56,"Thanks Hoss Man, it makes sense to me. I'll merge soon."
LUCENE-7878, Jim Ferenczi,14/Jun/17 15:52,"Thanks  Emanuel ! 
I confirm that this is a bug and that the bug only occurs when there is a single multi-words synonym in the query. In that case the should clause that is supposed to handle the synonym rule is removed by the query parser, I'll work on a fix."
LUCENE-7878, Jim Ferenczi,14/Jun/17 16:03,Here is a patch. The logic to simplify the boolean query produced by the graph analysis was wrong.
LUCENE-7878, Jim Ferenczi,14/Jun/17 17:54,A new patch that passes all the tests
LUCENE-7878, Emmanuel Keller,14/Jun/17 19:52,"Hi Jim,
I confirm is it fixed. Thanks for that extremely fast bug resolution 
It would be great if this patch could be applied also to 6.5 / 6.6 branches."
LUCENE-7878, Adrien Grand,16/Jun/17 08:29,test1 looks like a leftover. Otherwise +1!
LUCENE-7878, Jim Ferenczi,16/Jun/17 14:12,"Thanks @jpountz and Emmanuel for finding this nasty bug!

It is merged in 6.6 so if we do a 6.6.1 release it will be part of it, otherwise it will be available in 6.7."
LUCENE-7879, Robert Muir,14/Jun/17 23:19,you need to run ant eclipse from the top-level checkout so that you have a proper eclipse configuration. refresh your project after that.
LUCENE-7879, Jaewoo Kim,14/Jun/17 23:32,"Thanks for the reply. So I actually did that before and I ran the command again. I see that it succeeds with this echo: SUCCESS: You must right-click your project and choose Refresh.

However, the same Junit test fail is happening. So in theory, since I just forked and built lucene without changing it, all unit tests should work right?"
LUCENE-7879, Jaewoo Kim,14/Jun/17 23:39,Still not working
LUCENE-7879, Jintao Jiang,14/Jun/17 23:43,Have the same issue here.
LUCENE-7879, Robert Muir,15/Jun/17 00:11,"Your build path is not configured correctly (which that command should do). It ensures lucene/test-framework/src,properties,test is in the build path. That contains things tests depend on."
LUCENE-7884, Mike Drob,27/Jun/17 01:54,"Catching throwable and then trucking on in the face of whatever happened is a very dangerous pattern. What happens if you get an OOM there? The rest of your application could be in a bad state and I don't think we want to continue as we are. Better to explicitly catch NCDFE.

That said, are you running in the ""standard"" environment with Java 7, or the ""flexible"" environment with Java 8? Lucene requires Java 8, and running on 7 would get you this error. So I suspect it might be a setup issue."
LUCENE-7884, Yan Zhao,27/Jun/17 03:47,"Mike Drob Thx for your time!

Changed the patch to catch NCDFE.

Sadly I have to use the standard google app engine environment which still runs java 7. I am using the latest Lucene version (5.5.4) that supports java 7.

The problem is actually accessing nio `Paths` results a NCDFE since google disabled it.

Here is a snippet of stacktrace, thx!


java.lang.NoClassDefFoundError: java.nio.file.Paths is a restricted class. Please see the Google App Engine developer's guide for more details.
	at com.google.apphosting.runtime.security.shared.stub.java.nio.file.Paths.<clinit>(Paths.java)
	at org.apache.lucene.util.StringHelper.<clinit>(StringHelper.java:252)
	at org.apache.lucene.index.DocumentsWriterPerThread.<init>(DocumentsWriterPerThread.java:183)
	at org.apache.lucene.index.DocumentsWriter.ensureInitialized(DocumentsWriter.java:391)
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:445)
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1477)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1256)"
LUCENE-7884, Jan Høydahl,27/Jun/17 08:16,"I'm sorry, but Lucene has at least 230 uses of ""Paths"" and I suppose many other disabled nio APIs as well. So App engine standard simply does not support Lucene"
LUCENE-7884, Jan Høydahl,27/Jun/17 08:19,"Yan Zhao Thanks for wanting to contribute, we appreciate that a lot. But in this case the solution is to switch to an environment that supports Lucene.

Closing issue as ""Not a bug"""
LUCENE-7885, Adrien Grand,21/Jun/17 12:40,"Right, the behaviour is undefined in that case. Documentation for seekExact states: Attempts to seek to the exact term, returning true if the term is found.  If this returns false, the enum is unpositioned. and documentation for postings states Do not call this when the enum is unpositioned. If you want to go to the next term when a term is not found, maybe you should use seekCeil instead?

For the record, you might want to consider using AssertingDirectoryReader in your tests, which would detect this kind of issues."
LUCENE-7887, Michael McCandless,29/Jun/17 01:42,I'll dig; looks fun
LUCENE-7887, Michael McCandless,29/Jun/17 09:54,"I found the cause: IW was in the process of aborting, while threads were concurrently still trying to apply deleted packets.  I simplified the code here to not bother clearing the delete counters from abort."
LUCENE-7888, Steve Rowe,28/Jun/17 19:45,"Another non-reproducing failure, from my Jenkins:


Checking out Revision 85a27a231fdddb118ee178baac170da0097a02c0 (refs/remotes/origin/master)
[...]
   [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(7.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=61A674A498110EC0 -Dtests.slow=true -Dtests.locale=ja-JP -Dtests.timezone=Greenwich -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] FAILURE 0.07s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f0, reader=_4(7.0.0):c19 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([61A674A498110EC0:575A168B19E46DDC]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=false): {}, locale=ja-JP, timezone=Greenwich
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=220134656,total=306184192
   [junit4]   2> NOTE: All tests run in this JVM: [TestStringMSBRadixSorter, TestSpanTermQuery, TestOmitPositions, TestIndexableField, TestHighCompressionMode, TestDeterminizeLexicon, TestPackedTokenAttributeImpl, TestTopDocsCollector, TestIndexOrDocValuesQuery, TestDocValuesRewriteMethod, TestDocument, TestCrash, TestWildcardRandom, TestDocIdSetBuilder, TestFilterLeafReader, TestMergedIterator, TestMultiThreadTermVectors, TestAtomicUpdate, TestNorms, Test4GBStoredFields, TestFixedLengthBytesRefArray, TestFieldInvertState, TestBoolean2ScorerSupplier, TestLevenshteinAutomata, TestGraphTokenStreamFiniteStrings, TestStandardAnalyzer, TestSegmentReader, TestScorerPerf, TestBoostQuery, TestMergePolicyWrapper, TestComplexExplanations, TestPointQueries, TestMixedCodecs, TestPointValues, TestMultiMMap, TestLazyProxSkipping, TestTerms, TestIndexWriterThreadsToSegments, TestFilterWeight, TestDocumentsWriterDeleteQueue, TestCharFilter, TestDocInverterPerFieldErrorInfo, TestSimilarityProvider, LimitedFiniteStringsIteratorTest, TestNewestSegment, TestFSTs, TestClassicSimilarity, TestUnicodeUtil, TestQueryBuilder, TestSwappedIndexFiles, TestTimSorterWorstCase, TestBinaryDocValuesUpdates]"
LUCENE-7888, Steve Rowe,28/Jun/17 20:00,"Another non-reproducing failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/19961/ (log - and commit sha - no longer available; the notification email arrived on June 24 at 10:37PM):


[...]
  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
  [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(7.0.0):C2)
  [junit4]   1> TEST: now reopen
  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=3A4BC284D906CE1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=hr-HR -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8
  [junit4] FAILURE 0.65s J2 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
  [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f4, reader=_28(7.0.0):C936 expected:<12> but was:<11>
  [junit4]    > 	at __randomizedtesting.SeedInfo.seed([3A4BC284D906CE1A:CB7A0AB58F3AD06]:0)
  [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
  [junit4]    > 	at java.lang.Thread.run(Thread.java:748)
  [junit4] IGNOR/A 0.00s J2 | TestBinaryDocValuesUpdates.testTonsOfUpdates
  [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
  [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, maxDocsPerChunk=1, blockSize=26), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, blockSize=26)), sim=RandomSimilarity(queryNorm=false): {}, locale=hr-HR, timezone=Pacific/Pitcairn
  [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=302943312,total=518979584
  [junit4]   2> NOTE: All tests run in this JVM: [TestDirectPacked, TestFieldCacheRewriteMethod, TestBagOfPostings, TestEarlyTermination, TestReaderWrapperDVTypeCheck, TestNeedsScores, TestRoaringDocIdSet, TestShardSearching, TestSpansEnum, TestSegmentTermEnum, TestLongPostings, TestIndexReaderClose, TestLucene70NormsFormat, TestReqExclBulkScorer, TestField, TestSegmentTermDocs, TestSimilarityBase, TestGeoEncodingUtils, TestPayloadsOnVectors, TestCharTermAttributeImpl, TestDisjunctionMaxQuery, TestTermRangeQuery, TestLongValuesSource, TestCachingTokenFilter, TestOfflineSorter, TestTopDocsCollector, TestBufferedIndexInput, TestTermScorer, TestPerFieldPostingsFormat2, TestConsistentFieldNumbers, TestFieldsReader, TestConjunctions, TestSloppyPhraseQuery2, TestSetOnce, TestRollingUpdates, TestIndexWriterLockRelease, TestIndexWriterMergePolicy, TestRollingBuffer, TestBinaryDocument, TestSimpleFSLockFactory, TestIndexingSequenceNumbers, FiniteStringsIteratorTest, TestGraphTokenStreamFiniteStrings, TestSentinelIntSet, TestHugeRamFile, TestSortedNumericSortField, TestMultiCollector, TestSpanNotQuery, TestAllFilesHaveCodecHeader, TestTrackingDirectoryWrapper, TestControlledRealTimeReopenThread, TestDirectoryReader, TestDocValues, TestDoubleRangeFieldQueries, TestSpanCollection, TestDemoParallelLeafReader, TestSpans, TestTerms, Test2BBinaryDocValues, TestParallelCompositeReader, TestArrayUtil, TestPrefixQuery, TestAttributeSource, TestByteBlockPool, TestCompiledAutomaton, TestSimpleExplanationsOfNonMatches, TestDocValuesScoring, TestExceedMaxTermLength, TestNRTThreads, TestLazyProxSkipping, TestSimilarity2, TestSearchWithThreads, TestPolygon2D, TestGrowableByteArrayDataOutput, TestIndexCommit, TestBasics, TestSearcherManager, TestNorms, TestStandardAnalyzer, TestTopDocsMerge, TestMinimize, TestNRTReaderWithThreads, TestIndexWriterForceMerge, TestPerFieldPostingsFormat, TestCollectionUtil, TestFastDecompressionMode, TestSort, TestMultiDocValues, TestCustomSearcherSort, TestTermsEnum2, Test2BDocs, TestMixedCodecs, TestSpanExplanations, TestFastCompressionMode, TestStressIndexing2, TestMultiPhraseQuery, TestDeterminism, TestMergeSchedulerExternal, TestForceMergeForever, TestSameScoresWithThreads, TestMultiFields, TestLiveFieldValues, TestSpanSearchEquivalence, TestPayloads, TestDoc, TestFieldMaskingSpanQuery, TestExternalCodecs, TestRegexpQuery, TestIntBlockPool, TestComplexExplanationsOfNonMatches, TestParallelReaderEmptyIndex, TestDocument, TestFileSwitchDirectory, TestDirectory, TestRegexpRandom, TestMultiLevelSkipList, TestCheckIndex, TestBooleanQueryVisitSubscorers, TestMatchAllDocsQuery, TestSubScorerFreqs, TestIndexWriterConfig, TestPositionIncrement, TestSpanExplanationsOfNonMatches, TestFilterLeafReader, TestSameTokenSamePosition, TestAutomatonQueryUnicode, TestRamUsageEstimator, TestSpanFirstQuery, TestIsCurrent, TestNoMergePolicy, TestNoMergeScheduler, TestNamedSPILoader, TestBytesRef, TestCharFilter, TestTwoPhaseCommitTool, TestCloseableThreadLocal, TestVersion, TestReaderClosed, TestNGramPhraseQuery, TestIntsRef, Test2BPositions, Test2BPostingsBytes, Test2BTerms, TestByteArrayDataInput, Test2BPagedBytes, TestCharArraySet, TestDelegatingAnalyzerWrapper, TestStopFilter, TestBlockPostingsFormat, TestLucene50TermVectorsFormat, Test2BSortedDocValuesOrds, TestAllFilesCheckIndexHeader, TestAllFilesHaveChecksumFooter, TestBinaryDocValuesUpdates]
  [junit4] Completed [362/453 (1!)] on J2 in 6.22s, 29 tests, 1 failure, 1 skipped <<< FAILURES!"
LUCENE-7888, Michael McCandless,02/Jul/17 09:19,I'll tackle this.
LUCENE-7888, Michael McCandless,02/Jul/17 20:04,"I think these should be fixed now.  It was a tricky concurrency hazard, where an indexing thread that's resolving DV updates thinks it's done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.

Thanks Steve Rowe."
LUCENE-7888, Steve Rowe,06/Jul/17 16:28,"Mike, any reason not to backport to branch_7x and branch_7_0?  There was a recent failure on a Jenkins branch_7x job https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/9/:


Checking out Revision 758cbd98a7aa020ad67aea775028badf0be6418c (refs/remotes/origin/branch_7x)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D57106AE532F4164 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=qu-PE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.44s J2 | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f2, reader=_y(7.1.0):C435:fieldInfosGen=2:dvGen=2 expected:<7> but was:<6>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D57106AE532F4164:E38D6481D2DA2278]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   [junit4]    > 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   [junit4]    > 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
   [junit4]    > 	at java.base/java.lang.Thread.run(Thread.java:844)
   [junit4] IGNOR/A 0.00s J2 | TestMixedDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: leaving temporary files on disk at: /home/jenkins/workspace/Lucene-Solr-7.x-Linux/lucene/build/core/test/J2/temp/lucene.index.TestMixedDocValuesUpdates_D57106AE532F4164-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=qu-PE, timezone=America/Yakutat
   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 9 (64-bit)/cpus=8,threads=1,free=165322832,total=342360064"
LUCENE-7888, Steve Rowe,07/Jul/17 01:19,"More non-reproducing master failures from my Jenkins, commit shas are all after Mike's commit on this issue:


Checking out Revision 48b4960e0c093b480b8328f324992a7006054f17 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=6A2FDBE9B9C2F59C -Dtests.slow=true -Dtests.locale=ar-YE -Dtests.timezone=Asia/Beirut -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.31s J1 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=63, field=f1, reader=_l(8.0.0):c88:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([6A2FDBE9B9C2F59C:5CD3B9C638379680]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4] IGNOR/A 0.00s J1 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, f=PostingsFormat(name=LuceneVarGapDocFreqInterval), k1=PostingsFormat(name=LuceneVarGapDocFreqInterval), dvUpdateKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), k2=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), foo=PostingsFormat(name=LuceneVarGapDocFreqInterval), upd=PostingsFormat(name=Direct), updKey=FSTOrd50, id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), key=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{val=DocValuesFormat(name=Lucene70), ndv=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Asserting), sdv=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Asserting), control=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), sort=DocValuesFormat(name=Asserting), f2=DocValuesFormat(name=Direct), cf0=DocValuesFormat(name=Lucene70), f3=DocValuesFormat(name=Lucene70), f4=DocValuesFormat(name=Asserting), f5=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Asserting), bdv2=DocValuesFormat(name=Asserting), number=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Lucene70), bdv=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Lucene70), key=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1142, maxMBSortInHeap=7.285443710546513, sim=RandomSimilarity(queryNorm=false): {}, locale=ar-YE, timezone=Asia/Beirut
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=296202568,total=395313152




Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=en-ZA -Dtests.timezone=America/Inuvik -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.89s J6  | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f3, reader=_o(8.0.0):c417:fieldInfosGen=2:dvGen=2 expected:<5> but was:<4>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestBinaryDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, k1=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), f=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), dvUpdateKey=FSTOrd50, foo=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), k2=PostingsFormat(name=LuceneFixedGap), upd=Lucene50(blocksize=128), updKey=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), id=PostingsFormat(name=LuceneFixedGap), key=PostingsFormat(name=LuceneFixedGap)}, docValues:{ndv=DocValuesFormat(name=Memory), f10=DocValuesFormat(name=Asserting), f12=DocValuesFormat(name=Direct), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Asserting), f13=DocValuesFormat(name=Memory), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Direct), f1=DocValuesFormat(name=Direct), f15=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Memory), f18=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Asserting), f17=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Direct), bdv2=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Memory), number=DocValuesFormat(name=Direct), f7=DocValuesFormat(name=Asserting), f8=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Asserting), f9=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct), val=DocValuesFormat(name=Asserting), f21=DocValuesFormat(name=Asserting), f20=DocValuesFormat(name=Memory), f23=DocValuesFormat(name=Direct), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Asserting), f24=DocValuesFormat(name=Memory), sort=DocValuesFormat(name=Lucene70), cf0=DocValuesFormat(name=Asserting), cf2=DocValuesFormat(name=Direct), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Asserting), cf3=DocValuesFormat(name=Memory), cf6=DocValuesFormat(name=Direct), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Asserting), cf7=DocValuesFormat(name=Memory), cf9=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Lucene70), sdv=DocValuesFormat(name=Asserting), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Memory), cf24=DocValuesFormat(name=Asserting), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Direct), cf20=DocValuesFormat(name=Asserting), key=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Direct), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Direct), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Memory), cf17=DocValuesFormat(name=Asserting), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Direct), control=DocValuesFormat(name=Asserting), cf12=DocValuesFormat(name=Memory), cf13=DocValuesFormat(name=Asserting), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Direct), bdv=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=74, maxMBSortInHeap=7.963102974639169, sim=RandomSimilarity(queryNorm=true): {}, locale=en-ZA, timezone=America/Inuvik
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=73914040,total=520617984




Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=pt-BR -Dtests.timezone=Africa/Porto-Novo -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.17s J6  | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f1, reader=_7(8.0.0):c118:fieldInfosGen=2:dvGen=2 expected:<3> but was:<2>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestMixedDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {upd=Lucene50(blocksize=128), updKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), id=PostingsFormat(name=LuceneVarGapFixedInterval), key=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{f10=DocValuesFormat(name=Memory), f12=DocValuesFormat(name=Lucene70), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Memory), f13=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Lucene70), f15=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Asserting), f18=DocValuesFormat(name=Memory), f17=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Asserting), f7=DocValuesFormat(name=Memory), f8=DocValuesFormat(name=Lucene70), f9=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Lucene70), f21=DocValuesFormat(name=Memory), f20=DocValuesFormat(name=Asserting), f23=DocValuesFormat(name=Lucene70), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Memory), f24=DocValuesFormat(name=Asserting), cf0=DocValuesFormat(name=Memory), cf2=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Memory), cf3=DocValuesFormat(name=Asserting), cf6=DocValuesFormat(name=Lucene70), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Memory), cf7=DocValuesFormat(name=Asserting), cf9=DocValuesFormat(name=Lucene70), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Asserting), cf24=DocValuesFormat(name=Memory), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Lucene70), cf20=DocValuesFormat(name=Memory), key=DocValuesFormat(name=Lucene70), cf=DocValuesFormat(name=Lucene70), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Asserting), cf17=DocValuesFormat(name=Memory), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Lucene70), cf12=DocValuesFormat(name=Asserting), cf13=DocValuesFormat(name=Memory), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1397, maxMBSortInHeap=7.760564666966891, sim=RandomSimilarity(queryNorm=false): {}, locale=pt-BR, timezone=Africa/Porto-Novo
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=283409888,total=478150656




Checking out Revision 6c163658bbca15b1e4ff81d16b25e07df78468e8 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=85C41F1E0BBEB082 -Dtests.slow=true -Dtests.locale=ca-ES -Dtests.timezone=Kwajalein -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.71s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f2, reader=_d(8.0.0):c55:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([85C41F1E0BBEB082:B3387D318A4BD39E]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=true): {}, locale=ca-ES, timezone=Kwajalein
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=312487000,total=502792192"
LUCENE-7888, Michael McCandless,07/Jul/17 02:12,"Mike, any reason not to backport to branch_7x and branch_7_0?

Ugh, I thought my commit went in before 7.x/7.0 branched; I'll back port tomorrow, and look into the new test failures!"
LUCENE-7888, Michael McCandless,07/Jul/17 09:46,"Mike, any reason not to backport to branch_7x and branch_7_0?

OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.

I'll dig on the new failures."
LUCENE-7888, Steve Rowe,07/Jul/17 14:20,"Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.

Crap, sorry for wasting your time.  I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.  But looking now I see it on both branches' logs."
LUCENE-7888, Michael McCandless,07/Jul/17 14:57,No worries Steve Rowe!
LUCENE-7888, Michael McCandless,07/Jul/17 15:01,"Also, I think the 4 non-reproducing seeds above (https://issues.apache.org/jira/browse/LUCENE-7888?focusedCommentId=16077449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16077449) were before my last commit (7c704d5258b3be8c383ccb96bf4a30be441f091c) fixing a race ... so I'm hoping there are no more failures in this challenging test"
LUCENE-7890, Martijn van Groningen,28/Jun/17 18:03,Attached patch with fix and a test.
LUCENE-7890, Martijn van Groningen,29/Jun/17 13:05,"Pushed to master branch. I did not add an entry to CHANGES.txt, because this bug only existed in 7.0, which hasn't been released yet."
LUCENE-7892, Adrien Grand,04/Jul/17 14:39,+1 I'll fix it.
LUCENE-7894, Michael McCandless,30/Jun/17 21:49,Simple patch w/ test case showing the issue.  The problem was IW was failing to finish flushing a segment with the indexing thread that just wrote all the files for the segment.
LUCENE-7896, Simon Willnauer,04/Jul/17 13:23,here is the link to the RR issue https://github.com/randomizedtesting/randomizedtesting/issues/250
LUCENE-7896, Dawid Weiss,05/Jul/17 17:39,Thanks Simon.
LUCENE-7903, David Smiley,11/Jul/17 15:04,"I've retitled this issue and moved it to Lucene.  It's debatable if this is a bug vs improvement but I classified as minor well.  This is a long-standing known issue that affects the original Highlighter as well as the UnifiedHighlighter.  It probably affects the FVH too but I'm not sure.  I think there may be a previous issue on this matter but I'm having difficulty finding it so maybe not.

This is a hard problem that requires re-engineering a large and complicated part of the UnifiedHighlighter (PhraseHelper) – and one already on my mind but I have no time for right now.  For inspiration, we can look at the highlighter in Luwak which decomposes the query tree into separate SpanScorers.  It doesn't suffer from this problem and from some related problems to the current approach that are already filed in other issues."
LUCENE-7909, Ignacio Vera,18/Jul/17 12:25,"I think I understand, holes must be convex so the area inside is the area between the hole and the edges of the polygon. Could you confirm?"
LUCENE-7909, Karl Wright,18/Jul/17 12:33,"The comment (here from GeoPolygonFactory) is not very clear:


   * @param holes is a list of polygons representing ""holes"" in the outside polygon.  Holes describe the area outside
   *  each hole as being ""in set"".  Null == none.



The meaning is that the polygon(s) in ""holes"" describe the area OUTSIDE the hole.  So the code is right given that."
LUCENE-7914, Robert Muir,01/Aug/17 11:18,"Sorry, but prefixquery is definitely infinite. Besides this logic being wrong, I really don't like the ""trillean"" and additional ctor added to the public API."
LUCENE-7914, Jim Ferenczi,01/Aug/17 11:56,"Right sorry, a prefix query is indeed infinite. I reverted the logic in the patch. 
This is just a very small optimization and I can remove it from the patch if it makes the constructor too complex.
It's just that setting a limit on a prefix query just because it might lead to StackOverflow is error prone."
LUCENE-7914, Jim Ferenczi,01/Aug/17 12:03,"I removed the new constructor in AutomatonQuery and the optim in PrefixQuery. This is useless if we have the max recursion in place and I agree that it makes the constructor ugly. 
So here is a new patch."
LUCENE-7914, Robert Muir,01/Aug/17 12:16,"I don't understand the serialization/deserialization stuff in the tests. Lucene queries no longer support this serialization for years, is this intentional?"
LUCENE-7914, Jim Ferenczi,01/Aug/17 12:26,"It's not, it's just a copy/paste from another test in the class. I pushed another iteration that removes the serialization/deserialization from TestRegExp completely."
LUCENE-7914, Robert Muir,01/Aug/17 13:34,"ok, thanks.

As far as finite stuff, we might be able to remove the requirement that this is provided to AutomatonTermsEnum at all (see https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java#L200-L202)

It is kind of a leftover from when its intersection was less efficient (it handled infinite and finite DFAs differently). Nowadays it handles the case where its actually in a looping portion and drives that part completely by the terms dictionary. 

I'm not sure if the current ""if finite"" guards to this loop detection really save us CPU for complex finite DFAs (e.g. fuzzy, spellcheck). That would be the thing to experiment with to see if this boolean could be removed..."
LUCENE-7914, Robert Muir,01/Aug/17 13:35,"And these comments above are just for discussion and probably not something we should do on this one, we should split out a different issue probably if we want to get rid of the finite tracking."
LUCENE-7914, Adrien Grand,01/Aug/17 13:44,"+1 I like those safeguards, they are not invasive. I'd throw an IAE rather than an ISE when the recursion depth is exceeded since the problem is about the input?"
LUCENE-7914, Jim Ferenczi,01/Aug/17 14:39,"Thanks for the great feedback Adrien Grand and Robert Muir

I pushed another iteration that replaces the ISEs in IAEs and cleans up the unnecessary changes."
LUCENE-7914, Robert Muir,02/Aug/17 03:31,Patch looks good to me. I think there are some spurious imports (import org.apache.lucene.util.fst.Util) added in some of the tests that can be removed.
LUCENE-7914, Adrien Grand,02/Aug/17 06:26,+1 FYI tests seem to still expect an ISE.
LUCENE-7914, Jim Ferenczi,04/Aug/17 10:13,Thanks Robert Muir and Adrien Grand !
LUCENE-7914, Jim Ferenczi,08/Aug/17 08:19,I've merged this to master and 7.x but since it's an internal change I'd also like to merge in 7.0. Are there any objections?
LUCENE-7916, Robert Muir,01/Aug/17 17:41,"We use lots of version-dependent stuff (such as precompiled RBBI, .NRM files, etc) that can easily break across a release. Because of this, we can only support the version of ICU that we specify.

So instead i think we should hard-check that the version of ICU is the one we are supposed to have and fail if its not."
LUCENE-7916, Chris Koenig,01/Aug/17 17:43,adding patch that changes the type of CompositeBreakIterator.wordBreakers to a Map and removes the reference to UScript.CODE_LIMIT
LUCENE-7916, Robert Muir,02/Aug/17 03:22,"We should not change datastructures when all that is needed is to use UCharacter.getIntPropertyMaxValue(UProperty.SCRIPT)

See: http://icu-project.org/apiref/icu4j/com/ibm/icu/lang/UScript.html#CODE_LIMIT

But separately, this wont solve all problems. Other things will probably fail if you use a different icu4j version than the one supported."
LUCENE-7916, Chris Koenig,02/Aug/17 18:05,"Thanks for your feedback. I didn't realize how tightly coupled Lucene is to a particular ICU release.

In our case, we are using ICUTokenizer but we have modified the default ruleset of RuleBasedBreakIterator to break on emoji characters so that we can search for emoji in text. The unicode properties for emoji that our rules depend on were added to UProperty starting with ICU 57. Because we are compiling our own RBBI rules, we are not exposed to any breakage that might occur due to a binary rule encoding change on upgrade of ICU. We are not making use of the Normalizer or Folding filters so we lack exposure there as well. After thorough A/B testing, this is working well for us in production with the exception of the issue reported above, which has only occurred once so far.

The underlying issue for us is that Lucene 6.6.0 is pegged to a fairly old version of ICU. In hindsight it might have been safer for us to fork lucene-analyzers-icu temporarily to build our own internal release against ICU 59.1.

From what I've seen in JIRA and the git repo, it looks like 6.7 is targeted at ICU 59.1. Is there an ETA for the release of 6.7?"
LUCENE-7916, Robert Muir,03/Aug/17 00:27,"Attached is my patch. We need to remove the deprecated constant anyway, to prevent trouble in the future.

I added TODO's where we load data files with ICU formats. I don't have suggested logic here yet, esp since I think the stuff is considered internal, so could theoretically break with any version that isn't exactly the one we test with."
LUCENE-7916, Robert Muir,03/Aug/17 00:35,"In our case, we are using ICUTokenizer but we have modified the default ruleset of RuleBasedBreakIterator to break on emoji characters so that we can search for emoji in text.

Cool!


The underlying issue for us is that Lucene 6.6.0 is pegged to a fairly old version of ICU. In hindsight it might have been safer for us to fork lucene-analyzers-icu temporarily to build our own internal release against ICU 59.1.

Yeah, when we upgrade ICU versions we run a script the regenerates normalization and segmentation datafiles for that specific ICU jar / unicode version: ant regenerate from lucene/analyzers/icu. So at the minimum this should really be done (followed of course by ant test) so that things work correctly. 


From what I've seen in JIRA and the git repo, it looks like 6.7 is targeted at ICU 59.1. Is there an ETA for the release of 6.7?

I'm not sure, maybe ask the dev list about this? But it seems most work is towards 7.0 and onwards. 

The real problem was falling so far behind on ICU versions. You can see why if you look at the ticket: LUCENE-7540. Mainly, a bug (http://bugs.icu-project.org/trac/ticket/12873) was introduced into ICU that our test suite detected but we didn't know why. This was fixed in ICU 59.1 so we were then able to upgrade."
LUCENE-7916, Michael McCandless,04/Aug/17 13:42,+1 to Robert Muir's patch: that's what ICU docs recommend instead.
LUCENE-7916, Robert Muir,08/Aug/17 00:39,"My patch needs a minor correction when committing, we need to replace UScript.CODE_LIMIT with UCharacter.getIntPropertyMaxValue(UProperty.SCRIPT)+1, because the former is a limit (one plus the maximum value: 175) and the latter is a maximum value (174). Tests do not detect this, but that might only be happenchance due to the property values/rules/random string generation for the SYMBOLS_EMOJI script."
LUCENE-7916, Robert Muir,08/Aug/17 01:33,Thanks Chris Koenig !
LUCENE-7919, Michael McCandless,05/Aug/17 09:28,"I agree notifyAll was not necessary here; we've already replaced that with a notify in LUCENE-7868, which will be released in 7.0."
LUCENE-7919, Guoqiang Jiang,07/Aug/17 01:39,why not just remove it?
LUCENE-7919, Michael McCandless,07/Aug/17 20:50,"Oh indeed, it can be removed!

We used to have threads .wait() in the past, but we don't do that anymore except in the aborting case and we already have a .notifyAll for that.

I'll remove it; thanks Guoqiang Jiang."
LUCENE-7919, Michael McCandless,07/Aug/17 22:56,Thanks Guoqiang Jiang!
LUCENE-7919, Guoqiang Jiang,08/Aug/17 00:48,My pleasure.
LUCENE-7919, Min Zhou,27/Aug/17 21:48,"+1 LGTM!  
This patch will improve the performance much better for single doc indexing. There is another part of need to be improved as well. The getAndLock() , which in the same time only one thread could enter in.  I'd like file another jira for the improvement."
LUCENE-7920, Adrien Grand,07/Aug/17 09:22,Here is a proposal that adds an expert factory method that directly takes the binary representation of the address rather than an InetAddress instance.
LUCENE-7920, Robert Muir,07/Aug/17 21:20,"I'm concerned this is really the right thing to do: because these are in fact ipv4 addresses (just with a different representation).

the spirit of the RFC is kind of against it here: https://www.ietf.org/rfc/rfc4038.txt


   However, IPv6 applications must not be required to distinguish
   ""normal"" and ""NAT-PT translated"" addresses (or any other kind of
   special addresses, including the IPv4-mapped IPv6 addresses): This
   would be completely impractical, and if the distinction must be made,
   it must be done elsewhere (e.g., kernel, system libraries).

Also, taking raw byte[] here looks very error prone. at the very least it would need checks that the byte[] is of the correct length (32 or 128 bits only), etc etc."
LUCENE-7920, Robert Muir,08/Aug/17 00:01,"If we decide to do this we should think about the method signature too, because newPrefixQuery(String,byte[],int) is not much different than newPrefixQuery(String,InetAddress,int), just some overloading with the same name. Null is not allowed so its not too bad, but we should still avoid if there is an easy way, e.g. different method name or something."
LUCENE-7920, Robert Muir,08/Aug/17 00:03,"And i would duplicate the null checks so that exception messages are still good, e.g. IAE(""InetAddress must not be null"") vs IAE(""addressBytes must not be null""). It would just be an impl detail that one method calls the other one."
LUCENE-7920, Adrien Grand,08/Aug/17 14:26,"I think there are multiple ways that we could address the problem:
 1. Reject prefix queries on ipv6-mapped ipv4 addresses since they introduce a corner case. However we can't do it in Lucene since the ip address is already parsed, so we cannot distinguish ipv4 addresses from ipv6-mapped ipv4 addresses, so we have to assume that applications take care of this special case.
 2. Add a new method to query the binary representation of ip addresses directly (what the patch does for now, your suggestions would make it better).
 3. Change all methods to take a String instead of an InetAddress object as an ip address. This way we could know whether the prefix length assumes a length of 32 or 128 bits.

Something I like about 3 is that it moves some complexity from the user end to our end: even with option 2 users may still use the InetAddress-based prefix query factory method without knowing about its potential corner cases. I'm wondering what you think about it?"
LUCENE-7920, Robert Muir,08/Aug/17 15:37,"I agree its an interesting idea, but i am worried about accepting strings for #3 because its impossible to prevent someone from passing in hostnames unless you write your own IP parsing and validation.

I am 100% sure that we should not be doing DNS here."
LUCENE-7920, Robert Muir,08/Aug/17 15:38,"And again, i think this is all a lot of complexity that goes against the spirit of the RFC. E.g. a non-feature. I would prefer we keep things simple."
LUCENE-7920, Steve Rowe,08/Aug/17 15:47,"I agree its an interesting idea, but i am worried about accepting strings for #3 because its impossible to prevent someone from passing in hostnames unless you write your own IP parsing and validation.

FYI, UAX29URLEmailTokenizerImpl.jflex does IP parsing."
LUCENE-7920, Adrien Grand,08/Aug/17 15:51,"I am all for keeping things simple but I'd like to remove the trappiness that running InetAddressPoint.newPrefixQuery(""a"", InetAddress.getByName(""::ffff:0:0""), 24); might not do what you think it does?"
LUCENE-7920, Robert Muir,08/Aug/17 15:52,"True, but I think it only does recognition, correct? To ""parse"" in this case means going to a byte[] and involves ipv6 address decompression and other complexity.

I just still argue its a non-feature to do an ipv6 prefix query on an ipv4 address"
LUCENE-7920, Steve Rowe,08/Aug/17 15:57,"FYI, UAX29URLEmailTokenizerImpl.jflex does IP parsing.
True, but I think it only does recognition, correct? To ""parse"" in this case means going to a byte[] and involves ipv6 address decompression and other complexity.

Right."
LUCENE-7920, Adrien Grand,17/Aug/17 08:46,OK let's not do anything then.
LUCENE-7925, Adrien Grand,10/Aug/17 09:18,Here is a patch. Boosts are summed up through a double like the scorer would do.
LUCENE-7931, jin jing,28/Aug/17 05:56,Have you ever encountered the same problem? What is the solution?
LUCENE-7931, Adrien Grand,30/Aug/17 14:46,"Probably that your analyzer performs lowercasing so you should actually try to match not, not NOT."
LUCENE-7931, jin jing,11/Sep/17 10:13,i use the lowcase not .but got a same result
LUCENE-7931, Adrien Grand,11/Sep/17 14:45,Could you provide a standalone test case that reproduces the bug?
LUCENE-7931, jin jing,12/Sep/17 02:48,"public static void main(String[] args) throws IOException {  
        Directory dir = new RAMDirectory();  
        Analyzer analyzer = new StandardAnalyzer();  
        IndexWriterConfig iwc = new IndexWriterConfig(analyzer);  
        iwc.setOpenMode(OpenMode.CREATE);  
        IndexWriter writer = new IndexWriter(dir, iwc);  

        Document doc = new Document();  
        doc.add(new TextField(""text"", ""the quick brown fox jumps over the lazy dog"", Field.Store.YES));  
        writer.addDocument(doc);  

        doc = new Document();  
        doc.add(new TextField(""text"", ""the quick red fox jumps over the sleepy cat"", Field.Store.YES));  
        writer.addDocument(doc);  

        doc = new Document();  
        doc.add(new TextField(""text"", ""the quick brown fox jumps over the lazy not dog"", Field.Store.YES));  
        writer.addDocument(doc);  
        writer.close(); 
        IndexReader reader = DirectoryReader.open(dir);  
        IndexSearcher searcher = new IndexSearcher(reader);  
        String queryStringStart = ""dog"";  
        String queryStringEnd = ""quick"";  
        String excludeString = ""not"";  
        SpanQuery queryStart = new SpanTermQuery(new Term(""text"",queryStringStart));  
        SpanQuery queryEnd = new SpanTermQuery(new Term(""text"",queryStringEnd));  
        SpanQuery excludeQuery = new SpanTermQuery(new Term(""text"",excludeString));  
        SpanQuery spanNearQuery = new SpanNearQuery(  
            new SpanQuery[] 
{queryStart,queryEnd}
, 9, false, false);  

        SpanNotQuery spanNotQuery = new SpanNotQuery(spanNearQuery, excludeQuery, 4,3);  
        TopDocs results = searcher.search(spanNotQuery, null, 100);  
        ScoreDoc[] scoreDocs = results.scoreDocs;  

        for (int i = 0; i < scoreDocs.length; ++i) 
{  
            int docID = scoreDocs[i].doc;  
            Document document = searcher.doc(docID);  
            String path = document.get(""text"");  
            System.out.println(""text:"" + path);  
        }
  
    }"
LUCENE-7931, Alan Woodward,12/Sep/17 07:29,"""not"" is a stopword, and is removed by the StandardAnalyzer by default."
LUCENE-7931, jin jing,12/Sep/17 08:25,yes it is.thank you
LUCENE-7932, Rohit Balekundri,18/Aug/17 09:14,"Created new ticket LUCENE-7932 today.

Component to fix: lucene-queryparser-6.6.0.jar"
LUCENE-7932, Steve Rowe,18/Aug/17 18:24,"Rohit, there is nowhere near enough information here to figure out what the problem is.  And it looks like you're talking about configuration and/or code that isn't part of Lucene (LocationCode, Category, CollectionObjectID).  What the developers need is a simple way to reproduce the problem, i.e. data and code. 

You should start by asking questions on java-user mailing list: http://lucene.apache.org/core/discussion.html."
LUCENE-7932, Rohit Balekundri,21/Aug/17 04:55,"Hi Steve,
I updated exact relevant steps in description on how we are getting issue after making query from org.apache.lucene.search.IndexSearcher. This class is found in lucene-core-5.3.jar

Do you still suggest me to use: http://lucene.apache.org/core/discussion.html"
LUCENE-7932, Steve Rowe,21/Aug/17 13:32,"Do you still suggest me to use: http://lucene.apache.org/core/discussion.html

Yes.  You should always start there, and if you don't get a response in a day or two, or if someone requests you to do so, only then create a JIRA issue.

It's still the case that you haven't provided enough information.  You haven't provided any code, and you haven't shown what your actual queries are.  Are you using Lucene directly, or are you using some other project that depends on Lucene?"
LUCENE-7932, Rohit Balekundri,22/Aug/17 07:40,"Please find attached mail and also test scenarios and queries I used to test scenarios.
I am directly using Lucene code. I am not using any third party tool/software.
Issue found only with character 'a' and 'A' other character works fine.
If any information required please let me know.
Note: LuceneQueryTest.java  attached and Scenarios are explained in mail attached."
LUCENE-7932, Rohit Balekundri,30/Aug/17 04:21,"Hi Steve Rowe,

I still didn't get any response to my query. Hence kindly look at this issue.
Kindly suggest what else we can do to fix this issue."
LUCENE-7932, Erick Erickson,01/Sep/17 05:00,"The default StandardAnalyzer constructor uses a default English stop word set, which includes ""a"". So once the stopword is removed from the query, 
LocationCode:1 AND Category:a
just becomes
LocationCode:1

You can see this yourself by simply dumping the parsed query with:
System.out.println(""parsed query is ' "" + q.toString() + ""'"");

The third query behaves differently because the asterisk makes a* not a stopword.

Use this constructor to not have stop words:
StandardAnalyzer analyzer = new StandardAnalyzer(CharArraySet.EMPTY_SET);"
LUCENE-7933, Yonik Seeley,20/Aug/17 03:58,"This class is for supporting a contiguous (simple) bitset up to 64*2^31 bits in size.
It's not clear to me where you think the bug is, but perhaps you could provide a patch that would clear it up?"
LUCENE-7933, Won Jonghoon,20/Aug/17 05:50,"You say ""This class is for supporting a contiguous (simple) bitset up to 64*2^31 bits in size.""

but it doesn't check max size.




LongBistSet maxLongSizeLongBistSet = new LongBistSet(Long.MAX); ===> no error because no size check, numBits can have the value that is greater than 64*2^31. example  (2^63 - 1)





I think LongBitSet class is for supporting a contiguous (simple) bitset up to Long.MAX bits in size.

Well I will make new class using BitSet class  that is a java stand API.





--------- 원본 메일 ---------


보낸사람: Yonik Seeley (JIRA) <jira@apache.org>
받는사람: <k9200544@hanmail.net>
날짜: 17.08.20 12:59 GMT +0900
제목: [jira] [Commented] (LUCENE-7933) LongBistSet can't have Long size


    [ [1]https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16134299#comment-16134299 ] 

Yonik Seeley commented on LUCENE-7933:
--------------------------------------

This class is for supporting a contiguous (simple) bitset up to 64*2^31 bits in size.
It's not clear to me where you think the bug is, but perhaps you could provide a patch that would clear it up?




–
This message was sent by Atlassian JIRA
(v6.4.14#64029)




----------------------------------------------------------------------------------------
[1] https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16134299#comment-16134299
[2] https://issues.apache.org/jira/browse/LUCENE-7933
----------------------------------------------------------------------------------------
[1] https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16134299#comment-16134299
[2] https://issues.apache.org/jira/browse/LUCENE-7933"
LUCENE-7933, Michael McCandless,20/Aug/17 09:45,Let's just add a check in the ctor and throw IllegalArgumentException if the request numBits is too large?
LUCENE-7933, Won Jonghoon,20/Aug/17 11:52,"I agree it.


It just seems to add size checking logic.








--------- 원본 메일 ---------


보낸사람: Michael McCandless (JIRA) <jira@apache.org>
받는사람: <k9200544@hanmail.net>
날짜: 17.08.20 18:46 GMT +0900
제목: [jira] [Commented] (LUCENE-7933) LongBistSet can't have Long size


    [ [1]https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16134359#comment-16134359 ] 

Michael McCandless commented on LUCENE-7933:
--------------------------------------------

Let's just add a check in the ctor and throw IllegalArgumentException if the request numBits is too large?




–
This message was sent by Atlassian JIRA
(v6.4.14#64029)




----------------------------------------------------------------------------------------
[1] https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16134359#comment-16134359
[2] https://issues.apache.org/jira/browse/LUCENE-7933
----------------------------------------------------------------------------------------
[1] https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16134359#comment-16134359
[2] https://issues.apache.org/jira/browse/LUCENE-7933"
LUCENE-7933, Michael McCandless,31/Aug/17 17:58,Here's a simple patch.
LUCENE-7933, Michael McCandless,01/Sep/17 13:16,Thanks Won Jonghoon!
LUCENE-7933, Won Jonghoon,01/Sep/17 20:41,"Thanks apache LongBitset people ^^






--------- 원본 메일 ---------


보낸사람: Michael McCandless (JIRA) <jira@apache.org>
받는사람: <k9200544@hanmail.net>
날짜: 17.09.01 22:17 GMT +0900
제목: [jira] [Resolved] (LUCENE-7933) LongBistSet can't have Long size


     [ [1]https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Michael McCandless resolved LUCENE-7933.
----------------------------------------
       Resolution: Fixed
    Fix Version/s: 7.1
                   master (8.0)

Thanks Won Jonghoon!




–
This message was sent by Atlassian JIRA
(v6.4.14#64029)




----------------------------------------------------------------------------------------
[1] https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
[2] https://issues.apache.org/jira/browse/LUCENE-7933
----------------------------------------------------------------------------------------
[1] https://issues.apache.org/jira/browse/LUCENE-7933?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel
[2] https://issues.apache.org/jira/browse/LUCENE-7933"
LUCENE-7933, Hoss Man,01/Sep/17 22:18,"this new test is failing on windows jenkins builds ... looks like probably just a naive test assumption about some constant that varies based on platform?


Build: https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Windows/154/
Java: 64bit/jdk1.8.0_144 -XX:-UseCompressedOops -XX:+UseConcMarkSweepGC

7 tests failed.
FAILED:  org.apache.lucene.util.TestLongBitSet.testNegativeNumBits

Error Message:
expected:<...ust be 0 .. 13743895[2384]; got: -17> but was:<...ust be 0 .. 13743895[1872]; got: -17>

Stack Trace:
org.junit.ComparisonFailure: expected:<...ust be 0 .. 13743895[2384]; got: -17> but was:<...ust be 0 ..
13743895[1872]; got: -17>"
LUCENE-7933, Michael McCandless,02/Sep/17 09:42,"Indeed, 32 bit JVMs have a different value for ArrayUtil.MAX_ARRAY_LENGTH .. argh .. I pushed a fix."
LUCENE-7938, Ignacio Vera,21/Aug/17 15:03,Attached is a test showing the issue and a possible fix.
LUCENE-7938, Karl Wright,21/Aug/17 17:34,"Ignacio Vera, this is by design.  Bounding boxes are supposed to contain the objects within; there are indeed some rare cases where it is difficult to compute the bounding box and we return a larger box as a shortcut.

Furthermore, you should not modify the ""equals"" method to allow a match that is not a precise object match; that's a violation of the contract of ""equals"".

I'm going to close this as ""not a bug""."
LUCENE-7938, Ignacio Vera,22/Aug/17 06:18,"""there are indeed some rare cases where it is difficult to compute the bounding box and we return a larger box as a shortcut.""

What I disagree about is that computing the bounding box of a bounding box should not be difficult. Maybe there is a better implementation of getbounds() for GeoRectangle. Currently the method does the following:

  @Override
  public void getBounds(Bounds bounds) 
{
    super.getBounds(bounds);
    bounds.addHorizontalPlane(planetModel, topLat, topPlane, bottomPlane, leftPlane, rightPlane)
      .addVerticalPlane(planetModel, rightLon, rightPlane, topPlane, bottomPlane, leftPlane)
      .addHorizontalPlane(planetModel, bottomLat, bottomPlane, topPlane, leftPlane, rightPlane)
      .addVerticalPlane(planetModel, leftLon, leftPlane, topPlane, bottomPlane, rightPlane)
      .addIntersection(planetModel, leftPlane, rightPlane, topPlane, bottomPlane)
      .addPoint(ULHC).addPoint(URHC).addPoint(LLHC).addPoint(LRHC);
  }

Probably it will be enough if the method does the following:

  @Override
  public void getBounds(Bounds bounds) 
{
    super.getBounds(bounds);
    bounds.addPoint(ULHC).addPoint(URHC).addPoint(LLHC).addPoint(LRHC);
  }

As we know, the bounds of this object are the points. No need to add the planes as they introduce numerical imprecision.

Cheers,

I."
LUCENE-7938, Karl Wright,22/Aug/17 09:39,"As we know, the bounds of this object are the points. No need to add the planes as they introduce numerical imprecision.

Ignacio Vera, unfortunately that is not true.  This is a complex issue because planes that are almost but not quite coplanar define shapes that actually do have points on the opposite side of the world, within the precision limit of geo3d.  Random tests picked this up and I had to introduce addIntersection() to deal with this situation, since I could not see a better way to handle very thin rectangles using planes.  Polygons also have this issue."
LUCENE-7938, Ignacio Vera,22/Aug/17 10:44,"Ok, understood!"
LUCENE-7941, Karl Wright,25/Aug/17 12:45,"Hi Ignacio Vera, do you have a proposed solution?"
LUCENE-7941, Ignacio Vera,25/Aug/17 12:45,Attach failing test
LUCENE-7941, Ignacio Vera,25/Aug/17 12:47,"Not sure. I think the problematic methods is:

intersects(final Plane plane, ...)

If the answer is always WITHIN/CONTAINS, it means a point cannot intersects a plane and therefore the method should return always false."
LUCENE-7941, Karl Wright,25/Aug/17 14:17,"Ignacio Vera, the contract for ""intersects"" means that it must return true if there is intersection between the plane and the shape, within the bounds given.  A point lying on the plane clearly intersects it.  So that logic is right.  The error must be elsewhere.

I notice this code:


  @Override
  public boolean intersects(GeoShape geoShape) {
    return false;
  }



That seems incorrect to me.  I would think you'd want to call geoShape.within() here.  Can you explain why a GeoDegeneratePoint should never be considered to intersect?"
LUCENE-7941, Ignacio Vera,25/Aug/17 15:19,"Karl Wright, this is exactly the discussion I wanted to rise.

The contract of intersects(GeoShape geoShape) says: ""Assess whether a shape intersects with any of the edges this shape."" Therefore the current implementation is valid as there is no edges in GeoDegeneratedPoint.

Respect the other intersects method, the key lies in the word ""shape"". We are trying to make a point behave like a shape. It does not make sense that a point shape intersects a plane of another shape and the spatial relationship is WITHIN/CONTAINS. But I see that the getRelationship contract says:

""t is permissible to return OVERLAPS instead of WITHIN if the shape intersects with the area at even a single point.""

I would say that overlaps should be better than within. Therefore point shapes should return OVERLAPS if they lay in an edge. 

Make sense?"
LUCENE-7941, Karl Wright,25/Aug/17 15:37,"Ignacio Vera, a point is a shape; it just happens to be very very tiny.

I will clarify the GeoArea comments to make sure they are unambiguous.  Here is what it says for getRelationship():


   * It is permissible to return OVERLAPS instead of WITHIN if the shape
   * intersects with the area at even a single point.  So, a circle inscribed in
   * a rectangle could return either OVERLAPS or WITHIN, depending on
   * implementation.  It is not permissible to return CONTAINS or DISJOINT
   * in this circumstance, however.



In other words, if it is difficult to determine whether the shape is fully WITHIN, you may return OVERLAPS instead, as long as there is in fact some overlap.  For a GeoPoint, this is not ambiguous or difficult at all; the point is either WITHIN the shape, or it's not.  We have a method for assessing that: isWithin().  So it should be trivial to do the right thing here.

The contract for GeoShape.intersects() is as follows:


   * Assess whether a plane, within the provided bounds, intersects
   * with the shape.  Note well that this method is allowed to return ""true""
   * if there are internal edges of a composite shape which intersect the plane.
   * Doing this can cause getRelationship() for most GeoBBox shapes to return
   * OVERLAPS rather than the more correct CONTAINS, but that cannot be
   * helped for some complex shapes that are built out of overlapping parts.



Here the contract is not clear as to what should happen if the plane within its bounds are entirely contained within the shape.  But that's not needed to understand the proper behavior of a GeoDegeneratePoint – the point should be considered to intersect only if it lies on the plane within the bounds.  I will try to clarify the comment for other situations."
LUCENE-7941, Karl Wright,25/Aug/17 16:11,"Ignacio Vera, I will be away from the computer for some hours, but I think the right approach here is to adhere to the contracts.  Please let me know if this doesn't make sense."
LUCENE-7941, Ignacio Vera,25/Aug/17 16:19,"I agree and this discussion is very useful. I understand what you mean and I agree. The kind of shape I have in mind is different to what GeoDegeneratePoint is.

The only solution I find is the following: 

Change line 114 of GeoBaseAreaShape to:

if (!(geoShape instanceof GeoPoint) && intersects(geoShape))
{ 
      return  GeoArea.OVERLAPS;
}

That would do as GeoPoint do not OVERLAP.

Thanks!"
LUCENE-7941, Karl Wright,25/Aug/17 16:48,"Ignacio Vera, I think what you are trying to say is that this method in GeoDegenerateShape:


  @Override
  public boolean intersects(GeoShape geoShape) {
    return false;
  }



... cannot be properly computed because we have no general way to do it, other than implement something special for all GeoAreaShapes when they intersect with points.  I see what you are trying to do to work around this issue.

I wonder what would happen if we extend the contract for this method to allow a ""true"" return for either an intersection with an edge, but also for anything wholly within the shape, if the former is too hard to compute?  Then you could use geoShape.isWithin().  Would that yield sensible values for getRelationship()?

I'll try to think about this further while I'm offline."
LUCENE-7941, Ignacio Vera,25/Aug/17 17:13,"Nope, that will break getRelationship() as all WITHIN shapes will return OVERLAPS. The method checks if a shape goes in and out (cross and edge) of a shape, therefore OVERLAPS."
LUCENE-7941, Karl Wright,25/Aug/17 20:31,"Ignacio Vera:


Nope, that will break getRelationship() as all WITHIN shapes will return OVERLAPS. The method checks if a shape goes in and out (cross and edge) of a shape, therefore OVERLAPS.

But that is legal, although not ideal:


* It is permissible to return OVERLAPS instead of WITHIN if the shape
   * intersects with the area at even a single point.



I would opt for this in the case of GeoDegeneratePoint."
LUCENE-7941, Karl Wright,26/Aug/17 09:18,"Ignacio Vera, this is what I did.

(1) I broadened the contract for the intersects(GeoShape) method to allow it to legally return true when ""within"".  This is necessary because it's not just GeoDegeneratePoint objects that might have difficulty computing intersection with edges, and by broadening this, we still adhere to the contract of getRelationship() as it has been stated all along, which allows WITHIN situations to be reported as OVERLAPS when computational difficulty arises.  At least, I believe this to be true.  Please correct me if I am wrong.

(2) I changed the implementation of GeoDegeneratePoint.intersects(GeoShape) accordingly.

Please note that, because of this solution, asymmetrical  getRelationship() results are expected for the objects that report OVERLAPS instead of WITHIN for getRelationship(GeoShape), and for methods which report ""true"" for intersects(GeoShape) in a WITHIN situation.  So I did not commit your test; you will probably want to modify it and resubmit the patch when you are ready.

Thanks!"
LUCENE-7941, Ignacio Vera,26/Aug/17 09:39,"Hi Karl Wright,

I think actually the implementation of geoDegeneratePoint complies with the contract as it return CONTAINS/OVERLAPS (instead of WITHIN according to contract) when the geoDegeneratePoint lies in the boundary of the other shape .

On the other hand, the implementation of other shapes might  be not fully complaint as any shape that has a point in an edge of another shape will always return OVERLAPS/OVERLAPS.  Therefore a circle inscribed in a rectangle will return OVERLAPS/OVERLAPS.

If this is acceptable, then there is nothing to be done.

What it might be missing is a method in the the Plane; crossPlane(Plane lane,...) very similar to intersects but only return trues if intersects  and  has points in both sides of the other plane. With that functionaly the method getRelationship() can be more accurate according to contract.

Thanks!"
LUCENE-7941, Karl Wright,26/Aug/17 10:13,"Ignacio Vera, please see below:


I think actually the implementation of geoDegeneratePoint complies with the contract as it return CONTAINS/OVERLAPS (instead of WITHIN according to contract) when the geoDegeneratePoint lies in the boundary of the other shape .

It sounds like you believe that the currently committed code is correct, then?  It's hard to tell for sure.


On the other hand, the implementation of other shapes might be not fully complaint as any shape that has a point in an edge of another shape will always return OVERLAPS/OVERLAPS. Therefore a circle inscribed in a rectangle will return OVERLAPS/OVERLAPS.
If this is acceptable, then there is nothing to be done.

Yes, that's been the case all along.


What it might be missing is a method in the the Plane; crossPlane(Plane lane,...) very similar to intersects but only return trues if intersects and has points in both sides of the other plane. With that functionaly the method getRelationship() can be more accurate according to contract.

There is a Plane method already that does this:


  /**
   * Find the points between two planes, where one plane crosses the other, given a set of bounds.
   * Crossing is not just intersection; the planes cannot touch at just one point on the ellipsoid,
   * but must cross at two.
   *
   * @param planetModel is the planet model.
   * @param q is the plane to intersect with.
   * @param bounds are the bounds to consider to determine legal intersection points.
   * @return the set of legal crossing points, or null if the planes are numerically identical.
   */
  public GeoPoint[] findCrossings(final PlanetModel planetModel, final Plane q, final Membership... bounds) {
    if (isNumericallyIdentical(q)) {
      return null;
    }
    return findCrossings(planetModel, q, bounds, NO_BOUNDS);
  }



This code eliminates intersections that don't involve the planes actually crossing.  It's only used at the moment by GeoComplexPolygon, and has not been tested elsewhere."
LUCENE-7941, Ignacio Vera,26/Aug/17 11:00,"I think using the findCrossing is an overkill for performance. 

In the implementation you just committed the check: geoDegeneratedPoint.getRelationship(shape) always return OVERLAPS when the point is inside the shape or in the edge.

In the previous implementation: geoDegeneratedPoint.getRelationship(shape)  always return CONTAINS when the point is inside the shape or in the edge.

Note that the opposite: shape.getRelationship(geoDegeneratedPoint)  returns WITHIN when it is inside the shape except when it is in the edge that return OVERLAPS.

I think the second one is more accurate but we need to add in the contract of intersection(GeoShape geoShape):

""It is permissible to return false if the shape does not cross any edge but it is difficult to compute intersection with edges""

If you are more happy with your implementation, it is fine with me. We have those two options and I am fine with both as they agree with the contract."
LUCENE-7941, Karl Wright,26/Aug/17 11:35,"Ignacio Vera, I picked the particular contract extension that I did because I knew it was possible to compute, since by definition you can always just call shape.isWithin(this).  That's why I prefer it.  But in your situation, does it work acceptably that way?"
LUCENE-7941, Karl Wright,26/Aug/17 11:44,"Ignacio Vera, I've committed an implementation of Plane.crosses(), which is equivalent to Plane.intersects() but does not include single-point intersections.  If you want to experiment with this you are welcome to do so."
LUCENE-7941, Ignacio Vera,28/Aug/17 05:10,"Thanks for this new function Karl Wright. My comments:

It seems there is a problem with it. In line 2305 of Plane class I think the condition should read like:
if (!point1Valid) 
{
        return false;
}

If not the function is equivalent to intersects().

I try to implement get relationship() with this new information. First thing I notice is that I need to run intersects() and crosses() in the same function all the time which seems to be running very similar code. I guess it would be better to have a function similar to getRelationship() for planes that return three possibilities: DISJOINT/INTERSECTS/CROSSES.

But It seems that if we go that way we would need to change more things. There is one case, when the intersection point is equal to the edge point that fails. It means that we would need at least two edge points per shape.  

Now I am convinced that the current implementation is the most efficient under contract. I am not so keen in change it.

Thanks!"
LUCENE-7941, Karl Wright,28/Aug/17 05:46,"Ignacio Vera, line 2500 of Plane.java is the start of a comment.  But if I have the right place (line 2305) then this is my response:

(1) The difference between intersects() and crosses() is what the code does with the situation where there is only ONE solution to the line-intersecting-world computation it is performing.  The code in question does differ from intersects() to crosses().

(2) The code you point out is trying to determine whether any of the two intersection points found are within the bounds.  If neither is within the bounds then we must return false.  But at this point we already know there are two intersection points on the globe; it does not matter whether one point is in bounds and the other is outside.

The way crosses() should behave differently from intersects() is that it should exclude solutions where the line of intersection touches the world at exactly one point.  That is the only difference.  That is what the math can compute; no more, and no less.  If you can, in detail, supply an example of where this fails to detect an edge crossing, or detects a crossing where it should not, please do so, and we can analyze it further.

Note that the same logic exactly is used for GeoComplexPolygon, so we have some thoughts that it might be actually doing the right thing, provided you understand what it is actually doing.  GeoComplexPolygon also uses infitesimal planes (that is, planes that have been moved a small distance one way or another) to count crossings, so maybe you will need to do something more to come up with a fully viable approach here.

My suspicion, though, is that we really don't get to independently calculate edge crossings from intersections in the way you would like – at least not in a computationally acceptable manner."
LUCENE-7941, Ignacio Vera,28/Aug/17 09:57,"Thanks for the explanation, codes behaves as it should. For some reason I though it should only return true if both intersection were on bounds. 
I attach the test with current values."
LUCENE-7942, Mano Kovacs,29/Aug/17 13:10,"Karl Wright, there is a validation issue in the patch at GeoStandardPath.java#811:

-validate-source-patterns:
[source-patterns] tabs instead spaces: lucene/spatial3d/src/java/org/apache/lucene/spatial3d/geom/GeoStandardPath.java

BUILD FAILED



Could you please replace it? (I don't have write access)"
LUCENE-7944, Varun Thacker,29/Aug/17 05:36,"Hi Anshum,

Did you also run into the same issue which prompted the change to all the remaining branches? 

Curious how others didn't run into this previously."
LUCENE-7945, Dawid Weiss,30/Aug/17 11:33,"[junit4]   1>   leave running at Infinity MB/sec



Your computer is too fast, Adrien."
LUCENE-7945, Adrien Grand,30/Aug/17 11:40,"Haha, I wish it was the issue!"
LUCENE-7946, Uwe Schindler,31/Aug/17 06:40,"+1, looks good."
LUCENE-7946, Uwe Schindler,31/Aug/17 06:42,"In Java 9 we should use the new Objects methods for bounds checking! http://download.java.net/java/jdk9/docs/api/java/util/Objects.html#checkIndex-int-int-
Those are better optimized."
LUCENE-7946, Michael McCandless,31/Aug/17 09:19,+1!
LUCENE-7956, Adrien Grand,05/Sep/17 09:54,"Here is a patch

	Recursion has been replaced with a while loop
	It uses CharacterUtils, which is aware of supplementary chars, in order to fill the buffer. This helps avoid loading more data in memory in the case that the last char in the buffer is a surrogate."
LUCENE-7956, Robert Muir,05/Sep/17 10:28,"Looks good.

nit/style: the test was difficult to read, it seems like its ""trying too hard"" to use the lambda feature? Or is it trying to do everything on one line (i guess at this point stacktraces are already hopeless due to lambdas/streams but still...)

Instead of:

String text = IntStream.range(0, 1000000).mapToObj(i -> ""a"").collect(Collectors.joining());



Could we just do:

char text[1000000];
Arrays.fill(text, 'a');
..."
LUCENE-7956, Adrien Grand,05/Sep/17 12:18,Very good point. I'll fix when committing. Thanks for having a look!
LUCENE-7956, Uwe Schindler,05/Sep/17 12:23,"LOL, Arrays.fill() is much better. But the lambda is understandable, but this feels a bit like using a sledgehammer to crack a nut!"
LUCENE-7957, Adrien Grand,05/Sep/17 12:48,I'm curious how you found this bug? Asking because I've wanted to remove this method for a very long time so I'm wondering what your use-case is?
LUCENE-7957, Michael McCandless,05/Sep/17 14:28,"Thanks Adrien Grand; I think I hope we don't remove this API.

I'm running BooleanQuery that (roughly) looks for text matches from three possible places: 1) user-visible text fields (e.g. title), 2) a hidden keywords field and 3) in another hidden field recording what past queries led to user actions on this item.

I use a complex expression for scoring these hits, and one of the inputs to that expression was ""did this hit match only due to #2?"" or ""only due to #3?"", i.e. it wants to see which sub-clauses lead to this document being a hit, and score differently.

I realize I could also run 3 separate queries, and save away arrays to remember which clause contributed to which hits, but I suspect that's less efficient / more RAM consuming."
LUCENE-7957, Adrien Grand,05/Sep/17 15:38,"Thanks for the details.

I think I hope we don't remove this API.

I see how it could be useful, like in your case. But at the same time I see multiple issues with this API:

	With some scorers, being able to track the matching sub scorers would be additional overhead, eg. BooleanScorer today can't tell you which clauses matched a given document.
	How do you check ""did clause X or Y match?"", you have to iterate over all scorers and see whether the one you are interested in is there?
	Is it ok to perform heavy rewrites that make the scorer tree look very different from the query tree, or even make the clause you are interested in impossible to find? eg. by inlining nested disjunctions/conjunctions, rewriting a TermInSetQuery to a BooleanQuery, splitting a bbox query that crosses the dateline into 2 non-crossing bbox queries, rewriting a TermQuery to a MatchAllDocsQuery because scores are not needed and docFreq == maxDoc, etc.



I tried to think about how to address these issues but I don't have a good solution, especially about the last point: I think it would be a pity that an improvement to query execution be seen as a regression because it makes it harder to identify a matching clause.

I'd rather like this use-case to be addressed by consuming queries twice, once by Lucene so that it can build an efficient iterator, and once from a FilterScorer so that the score can be customized depending on whether a particular query matches.

I haven't removed it because there was disagreement every time I suggested that it should be removed but I don't see any way that we could support it as-is realistically. To me it's also interesting that it is the first time this bug is reported even though it has existed for almost two entire major versions (since 5.1) and affects one of our main scorers.

However we still expose this API, so +1 to fixing."
LUCENE-7957, Michael McCandless,05/Sep/17 19:47,"Thanks Adrien Grand, I agree the API is problematic and spotty at best.

I think it would be a pity that an improvement to query execution be seen as a regression because it makes it harder to identify a matching clause.

I think this API should always be ""best effort"", no guarantees, etc., and so if ever we see an improvement to query execution, we should still do it, even if it breaks this API.

and once from a FilterScorer so that the score can be customized depending on whether a particular query matches.

The challenge for me is that I'm using a large expression, and keyword_only_match is just one such variable, consulted multiple times in a big machine learned decision tree; it's not clear how I could invert that model to use a FilterScorer instead.  Maybe I'll just fall back to running N queries, one for each ""clause"" I need to identify.  This is already likely in a ""rescore top N"" context, so the amount of memory would be contained ...

To me it's also interesting that it is the first time this bug is reported even though it has existed for almost two entire major versions (since 5.1) and affects one of our main scorers.

Yeah, true."
LUCENE-7957, Michael McCandless,06/Sep/17 22:10,Simple patch + test.
LUCENE-7961, Robert Muir,06/Sep/17 22:21,"And to prevent confusion I think the exception should change slightly from:

throw new LockObtainFailedException(""Lock held elsewhere: "" + lockFile, e);



to:


throw new LockObtainFailedException(""Lock held elsewhere or inaccessible: "" + lockFile, e);



Let SimpleFSLockFactory not try to determine what happened, you can determine that from the caused-by."
LUCENE-7961, Robert Muir,09/Sep/17 06:31,Here's a patch.
LUCENE-7961, Robert Muir,09/Sep/17 06:36,"We can of course add a test like LUCENE-7959, but I think its overkill. 

The point is a filesystem can legally throw IOException(""file already there"") from this method and the lockfactory will never deliver a LockObtainedFailedException. Also its confusing today that we catch AccessDenied to handle the sporatic windows case, and deliver ""lock held elsewhere"" (even though we chain the exception). it could just be a permissions error. And it can always be the case its just a leftover file."
LUCENE-7961, Robert Muir,09/Sep/17 06:44,I fixed a bug in my comment and clarified that windows case.
LUCENE-7963, Dawid Weiss,08/Sep/17 11:22,This is Mike's commit 64b86331c and I don't see any particular reason for this line to be there. Michael McCandless?
LUCENE-7963, Uwe Schindler,08/Sep/17 11:35,"I think this was a relict from debugging! This was caused by LUCENE-7626.

About the patch: Patch is fine, thanks also for removing the import! +1"
LUCENE-7963, Daniel Mitterdorfer,08/Sep/17 11:45,"Cool, thank you for the quick feedback guys!"
LUCENE-7963, Uwe Schindler,08/Sep/17 11:58,"I will commit this later, I am just waiting for a confirmation by Michael McCandless! I set it as blocker for 7.0, I hope Anshum Gupta is fine with putting that into RC3 of Lucene 7."
LUCENE-7963, Robert Muir,08/Sep/17 12:04,"Unfortunately, it is beyond me why writing this register to main memory takes such a long time and why C2 did not eliminate this line as dead code to begin with.

The tool you are using is just buggy here. The reason it is slow is because of the unnecessary per-document hash lookup."
LUCENE-7963, Uwe Schindler,08/Sep/17 12:15,"Robert Muir: Agree, I was afraid to say this  The typical profiler bullshit."
LUCENE-7963, Daniel Mitterdorfer,08/Sep/17 12:20,"> The tool you are using is just buggy here. 

That may be possible that it does not assign the time to the correct statement. I still wonder why dead-code elimination did not kick in. For me, that line would be a classic candidate for that."
LUCENE-7963, Dawid Weiss,08/Sep/17 12:27,"If it writes to memory it's not dead code? Could be the memfence is someplace else, but is there so the memwrite can be visible to other threads. Also, depends on the address really – it can be a poison address (safepoint)... many different reasons."
LUCENE-7963, Uwe Schindler,08/Sep/17 12:58,"If it writes to memory it's not dead code?

The write here is not the write to the termAtt local variable. This one is already removed (it ignores the return value). The write comes from something inside getAttribute, as it needs to cast the attribute so it does a lot of work. In fact it is also not dead code - although the return value is ignroed, because the HashMap#get or the Class#cast may have side effects (like throwing exception,...). It might have no side effects, but that's hard to figure out by the optimizer. As far as I remeber: Hotspot was never able to remove hashmap gets easily!"
LUCENE-7963, Robert Muir,08/Sep/17 13:03,"Yeah, the method is not without side effects. I can't see how you'd eliminate the Class.cast on the result from the hashmap."
LUCENE-7963, Robert Muir,08/Sep/17 13:45,"By the way, this is almost exactly the same situation as LUCENE-7419"
LUCENE-7963, Uwe Schindler,08/Sep/17 15:43,Thanks Daniel Mitterdorfer!
LUCENE-7963, Michael McCandless,08/Sep/17 17:50,"+1 to remove, phew!!  Thanks Daniel Mitterdorfer and Uwe Schindler!"
LUCENE-7963, Daniel Mitterdorfer,11/Sep/17 05:53,Thanks Uwe Schindler for jumping on it and also Robert Muir and Dawid Weiss for the explanations. Glad it's resolved now.
LUCENE-7965, Karl Wright,08/Sep/17 21:49,"Ignacio Vera, do you agree that this is likely a test problem?"
LUCENE-7965, Ignacio Vera,09/Sep/17 07:49,"I see that was a real bug and a good catch. Around the poles the shape is always a degenerate point.

Thanks!"
LUCENE-7967, Ignacio Vera,12/Sep/17 10:11,Test attached
LUCENE-7967, Karl Wright,12/Sep/17 10:24,Ignacio Vera There is zero chance I can look at this today or tomorrow.
LUCENE-7967, Ignacio Vera,12/Sep/17 10:32,"There is no hurry so please just have a look whenever you have time. 

Thanks!"
LUCENE-7968, Robert Muir,13/Sep/17 00:30,"Here's an assert that can currently make tests fail. But I want to test that the tie-break case really works, not just that stuff is in-bounds.


diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
index 9c6a624..2fa3569 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
@@ -392,6 +392,8 @@ public class AnalyzingSuggester extends Lookup implements Accountable {
         scratchB.offset = readerB.getPosition();
         scratchA.length = a.length - scratchA.offset;
         scratchB.length = b.length - scratchB.offset;
+        assert scratchA.isValid();
+        assert scratchB.isValid();
       }
    
       return scratchA.compareTo(scratchB);




   [junit4] ERROR   0.31s J2 | AnalyzingSuggesterTest.testRandom <<<
   [junit4]    > Throwable #1: java.lang.IllegalStateException: length is negative: -1507
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D27F69F0C3A46E64:A0334CFF72C4D817]:0)
   [junit4]    > 	at org.apache.lucene.util.BytesRef.isValid(BytesRef.java:222)
   [junit4]    > 	at org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester$AnalyzingComparator.compare(AnalyzingSuggester.java:395)
   [junit4]    > 	at org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester$AnalyzingComparator.compare(AnalyzingSuggester.java:339)
   [junit4]    > 	at org.apache.lucene.util.BytesRefArray$1.comparePivot(BytesRefArray.java:157)
   [junit4]    > 	at org.apache.lucene.util.IntroSorter.quicksort(IntroSorter.java:66)
   [junit4]    > 	at org.apache.lucene.util.IntroSorter.quicksort(IntroSorter.java:82)
   [junit4]    > 	at org.apache.lucene.util.IntroSorter.sort(IntroSorter.java:36)
   [junit4]    > 	at org.apache.lucene.util.BytesRefArray.sort(BytesRefArray.java:166)
   [junit4]    > 	at org.apache.lucene.util.BytesRefArray.iterator(BytesRefArray.java:196)
   [junit4]    > 	at org.apache.lucene.util.OfflineSorter$SortPartitionTask.call(OfflineSorter.java:606)
   [junit4]    > 	at org.apache.lucene.util.OfflineSorter$SortPartitionTask.call(OfflineSorter.java:588)
   [junit4]    > 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
   [junit4]    > 	at org.apache.lucene.util.SameThreadExecutorService.execute(SameThreadExecutorService.java:34)
   [junit4]    > 	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134)
   [junit4]    > 	at org.apache.lucene.util.OfflineSorter.sort(OfflineSorter.java:285)
   [junit4]    > 	at org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester.build(AnalyzingSuggester.java:491)
   [junit4]    > 	at org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest.testRandom(AnalyzingSuggesterTest.java:787)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)"
LUCENE-7968, Robert Muir,13/Sep/17 01:23,"Patch, including a test that fails even without the assert. It just adds 50 surface forms with the same weight that all analyze to the same string, and asserts that lookup() returns them back in sorted order by surface form.

When payloads=false, the BytesRef's length should be the remaining bytes, not negative."
LUCENE-7968, Robert Muir,13/Sep/17 01:29,"From what I can tell, since the stuff going into the FST (analyzed form, costs) is still in-order in this case, nothing detected it.

The surface forms are stored in a big byte[], so by being out of order it means suggester's results just come back in a bizarre order when there are ties on both the analyzed form and costs (rather than in fact being sorted by surface form).

E.G. if you used a stemmer and added both dog (cost=2) and dogs (cost=2), suggester might sometimes return dog first, other times dogs."
LUCENE-7971, Bartosz Firyn,16/Sep/17 12:08,Pull request fixing this issue is available: https://github.com/apache/lucene-solr/pull/248
LUCENE-7977, Daniel Collins,26/Sep/17 12:09,"This is a patch against master, but it applies cleanly to branch_7x as well.
The fix is actually for the spatial3d module to make it build a test-jar."
LUCENE-7977, Daniel Collins,26/Sep/17 12:13,Looks like SOLR-11382 actually had an update to fix this.  Do we mark as a duplicate or just withdraw?
LUCENE-7977, Steve Rowe,26/Sep/17 13:50,"Looks like SOLR-11382 actually had an update to fix this.

Yeah, I committed the fix with a comment linking to SOLR-11382 without realizing it would have been more appropriately tied to LUCENE-7951.

Do we mark as a duplicate or just withdraw?

I don't think I would mark it as duplicate, since it doesn't actually duplicate (the entirety of) either of the other two issues.  But if you feel differently, I don't think it will hurt to mark it as such."
LUCENE-7977, Daniel Collins,26/Sep/17 14:57,"SOLR-11382 had a subsequent fix to correct this, both master and branch_7x are good now."
LUCENE-7978, Uwe Schindler,26/Sep/17 15:29,"Please delete your IVY cache! There seems to be a wrongly downloaded artifact causing this issue. By changing to the newer IVY version you were just requesting it to download a new version, as it was not found in the local repo!


$ rm -rf ~/.ivy2/cache/org.apache.ivy



You can also delete the whole cache, this requires the build system to download everything:


$ rm -rf ~/.ivy2/cache



The issue you are seing happens when you press Ctrl+C while Ivy is downloading or resolving dependencies. These are known bugs in IVY! 

Uwe"
LUCENE-7978, Anton R. Yuste,27/Sep/17 12:09,"Thanks for your help, Uwe Schindler. I knew my workaround was wrong but it allowed me to continue importing the project in the IDE and so on in the meantime. I was also looking for a possible duplicate of this known problem but I didn't find it in Jira.

I've added a PR to document this, I think it will help new contributors, at least, it would helped me a lot. 

https://github.com/apache/lucene-solr/pull/254

It's my first contribution, I hope it's fine but tell me if it's not, I will fix it and learn from it for the future."
LUCENE-7978, Uwe Schindler,27/Sep/17 13:29,"Hi Anton,

the documentation you added is only applicable to your specific problem, so it does not warrant to be added to the build documentation. In general the same can happen anywhere in the build, you just accidentally hit it while downloading Ivy. Cleaning up the IVY cache is a workaround for most of such issues, which is very similar to cleaning up the Maven cache when something goes wrong in Maven builds. 

The documentation text should better be rm -rf ~/.ivy2/cache as general workaround, because it nukes the whole cache.

The ""ivy-bootstrap"" part in your added documentation should be conditional, so you should add it like: 'if you see an error about Ivy missing when invoking ant, call ""ant ivy-bootstrap"" and retry.'"
LUCENE-7978, Anton R. Yuste,27/Sep/17 14:04,"Hi Uwe. Thanks for the feedback, I agree and I've updated the PR with it. Is it better now?"
LUCENE-7978, Uwe Schindler,27/Sep/17 18:35,"Yeah, much better! I will take later."
LUCENE-7978, Uwe Schindler,27/Sep/17 19:52,Reopening to apply the patch by Anton.
LUCENE-7978, Anton R. Yuste,29/Sep/17 06:27,"Thanks Uwe Schindler. If I can help with something more, please, let me know."
LUCENE-7978, Uwe Schindler,29/Sep/17 12:56,"I rephrased the text a little bit, thanks Anton! Could you please close the PR, it was merged, but Github did not notice it."
LUCENE-7978, Anton R. Yuste,29/Sep/17 13:02,The PR is already closed and the remote branch deleted. Thanks Uwe Schindler!
